% Chapter 4, Section 4 _Linear Algebra_ Jim Hefferon
%  http://joshua.smcvt.edu/linearalgebra
%  2001-Jun-12
\section{Jordan Form}
\index{Jordan form|(}
\noindent\textit{This section uses material from three optional
subsections:~Combining Subspaces, Determinants Exist, and
Laplace's Expansion.}

We began this chapter
by recalling that every linear map $\map{h}{V}{W}$ can
be represented with respect to some
bases $B\subset V$ and $D\subset W$ by a partial identity matrix.
Restated, the partial identity form is a canonical form for 
matrix equivalence. 
This chapter considers the case where
the codomain equals the domain so 
we naturally ask what is possible when the
two bases are equal, when we have $\rep{t}{B,B}$.
In short, we want a canonical form for matrix similarity.

We noted that in the $B,B$ case
a partial identity matrix is not always possible.
We therefore extended the matrix 
forms of interest to the natural generalization, 
diagonal matrices, and
showed that a transformation or square matrix can be diagonalized
if its eigenvalues are distinct.
But we also gave an example of a 
square matrix that cannot be diagonalized
because it is nilpotent,
and thus diagonal form won't suffice as the canonical form 
for matrix similarity. 

The prior section developed that example
to get a canonical form for nilpotent matrices, subdiagonal ones.

This section finishes our program by
showing that for any linear transformation there is
a basis~\( B \) 
such that the matrix representation $\rep{t}{B,B}$ is the sum of a 
diagonal matrix and a nilpotent matrix.
This is Jordan canonical form.









\subsectionoptional{Polynomials of Maps and Matrices}\index{polynomial!of map, matrix}
Recall that the set of square matrices~\( \matspace_{\nbyn{n}} \)
is a vector space under entry-by-entry addition and scalar multiplication,
and that 
%the unit matrices\Dash all entries are zero except
%for a single entry, which is one\Dash form a basis, so 
this space has dimension \( n^2 \).
Thus for any \( \nbyn{n} \) matrix $T$ the
\( n^2+1 \)-member set \( \set{I,T,T^2,\dots,T^{n^2} } \) is linearly
dependent and so there are scalars \( c_0,\dots,c_{n^2} \),
not all zero, such that
\begin{equation*}
  c_{n^2}T^{n^2}+\dots+c_1T+c_0I
\end{equation*}
is the zero matrix.
Therefore every transformation has a sort of generalized 
nilpotency\Dash the powers
of a square matrix cannot climb forever without a kind of repeat.

\begin{definition}
Let \( t \) be a linear transformation of a vector space~\( V \).
Where \( f(x)=c_nx^n+\dots+c_1x+c_0 \) is a polynomial,
\( f(t) \) is the
transformation \( c_nt^n+\dots+c_1t+c_0(\identity) \) on~\( V \).
In the same way, if \( T \) is a square matrix
then
\( f(T) \) is the matrix \( c_nT^n+\dots+c_1T+c_0I \).
\end{definition}

\noindent The polynomial of the matrix represents the polynomial of the map:~if 
\( T=\rep{t}{B,B} \) then \( f(T)=\rep{f(t)}{B,B} \).
This is because \( T^j=\rep{t^j}{B,B} \),
and \( cT=\rep{ct}{B,B} \), and \( T_1+T_2 =\rep{t_1+t_2}{B,B} \).

\begin{remark}
We shall write the matrix polynomial slightly differently than the 
map polynomial. 
For instance, if  \( f(x)=x-3 \) then 
we shall write the identity matrix, as in~\( f(T)=T-3I \),
but not write the identity map, as in~\( f(t)=t-3 \).
\end{remark}

\begin{example}  \label{ex:PolySendRotMatToZ}
Rotation of plane vectors \( \pi/3 \)~radians counterclockwise is represented
with respect to the standard basis by
\begin{equation*}
  T=
  \begin{mat}[r]
     \cos(\pi/3)  &-\sin(\pi/3)  \\
     \sin(\pi/3)   &\cos(\pi/3)
  \end{mat}
  =
  \begin{mat}[r]
     1/2        &-\sqrt{3}/2  \\
     \sqrt{3}/2  &1/2
  \end{mat}
\end{equation*}
and verifying that \( T^2-T+I=Z \) is routine.
(Geometrically, \( T^2 \) rotates by \( 2\pi/3\).
On the standard basis vector~\( \vec{e}_1 \), the action of
$T^2-T$ is to give the difference between the unit vector with angle
\( 2\pi/3\) and the unit vector with angle \( \pi/3\),
which is \( \binom{-1}{0} \).
On \( \vec{e}_2 \) it gives \( \binom{0}{-1} \).
So \( T^2-T=-I \).)
\end{example}

The space $\matspace_{\nbyn{2}}$ has dimension four so we know that for any
\( \nbyn{2} \) matrix~\( T \) there is a fourth degree polynomial 
\( f  \) such that \( f(T)=Z \). 
But in that example we exhibited a degree two polynomial that works. 
So while degree~$n^2$ always suffices, in some cases  
a smaller-degree polynomial is enough.

\begin{definition}
The \definend{minimal polynomial}\index{polynomial!minimal}%
\index{minimal polynomial}
\( m(x) \) of a transformation \( t \)\index{transformation!minimal polynomial}
or a square matrix \( T \)\index{matrix!minimal polynomial} is the
non-zero polynomial of least degree and with leading coefficient~\( 1 \)
such that \( m(t) \) is the zero map or \( m(T) \) is the zero matrix.
\end{definition}

\noindent A minimal
polynomial cannot be a constant polynomial because 
of the restriction on the leading coefficient.
So a minimal
polynomial must have degree at least one.
The zero matrix has minimal polynomial $p(x)=x$ while the 
identity matrix has minimal polynomial $\hat{p}(x)=x-1$.

\begin{lemma}
Any transformation or square matrix has a unique minimal polynomial.  
\end{lemma}

\begin{proof}
First we prove existence.
By the earlier observation 
that degree~$n^2$ suffices, there is at least one 
nonzero polynomial $p(x)=c_kx^k+\cdots+c_0$ that
takes the map or matrix to zero.
From among all such polynomials
take one with the smallest degree.
Divide this polynomial by its leading coefficient~$c_k$ to get a leading~$1$.
Hence any map or matrix has at least one minimal polynomial.

Now for uniqueness.
Suppose that 
\( m(x) \) and \( \hat{m}(x) \) both take the map or matrix to zero,
are both of 
minimal degree and are thus of equal degree, 
and both have a leading~$1$.
Consider the difference, \( m(x)-\hat{m}(x) \).
If it is not the zero polynomial then it has a nonzero leading coefficient.
Dividing through by
that leading coefficient would make it a polynomial that takes
the map or matrix to zero, has leading 
coefficient~\( 1\), and is of 
smaller degree than $m$ and $\hat{m}$ 
(because in the subtraction the leading \( 1\)'s cancel).
That would contradict the minimality of the degree
of $m$ and $\hat{m}$. 
Thus \( m(x)-\hat{m}(x) \) is the zero polynomial and
the two are equal.
\end{proof}

\begin{example}  \label{ex:MinPolyForRotMat}
One way to compute the minimal polynomial for the matrix of
\nearbyexample{ex:PolySendRotMatToZ} is to find the powers of $T$
up to $n^2=4$.
\begin{equation*}
   T^2=
   \begin{mat}[r]
      -1/2         &-\sqrt{3}/2  \\
      \sqrt{3}/2   &-1/2
   \end{mat} 
   \quad
   T^3=
   \begin{mat}[r]
      -1    &0           \\
       0    &-1
   \end{mat}
   \quad
   T^4=
   \begin{mat}[r]
      -1/2         &\sqrt{3}/2  \\
      -\sqrt{3}/2  &-1/2
   \end{mat}
\end{equation*}
Put \( c_4T^4+c_3T^3+c_2T^2+c_1T+c_0I \) equal to the zero matrix.
\begin{equation*}
  \begin{linsys}{5}
     -(1/2)c_4  
         &-  &c_3           
         &- &(1/2)c_2
         &+ &(1/2)c_1  
         &+  &c_0  &=  &0      \\
      (\sqrt{3}/2)c_4  
         &  &   
         &- &(\sqrt{3}/2)c_2
         &- &(\sqrt{3}/2)c_1  
         &   &     &=  &0        \\
     -(\sqrt{3}/2)c_4  
         &   &    
         &+ &(\sqrt{3}/2)c_2
         &+ &(\sqrt{3}/2)c_1  
        &   &            &=  &0      \\
     -(1/2)c_4  
         &-  &c_3             
         &- &(1/2)c_2
         &+ &(1/2)c_1  
         &+  &c_0  &=  &0
   \end{linsys}
\end{equation*}
Apply Gauss' Method.
\begin{equation*}
  \begin{linsys}{5}
     -(1/2)c_4  
         &- &c_3             
         &- &(1/2)c_2
         &+ &(1/2)c_1  
         &+  &c_0  &=  &0      \\
         &  &-(\sqrt{3})c_3 
         &- &\sqrt{3}c_2
         &  &    
         &+ &\sqrt{3}c_0 &=  &0
   \end{linsys} 
\end{equation*}
With an eye toward making the degree of the polynomial as small as 
possible, note that
setting \( c_4 \), \( c_3 \), and \( c_2 \) to zero forces \( c_1 \) and
\( c_0 \) to also come out as zero
so the equations won't allow a degree one minimal polynomial.
Instead, set \( c_4 \) and \( c_3 \) to zero.
The system
\begin{equation*}
  \begin{linsys}{3}           
         -(1/2)c_2
         &+ &(1/2)c_1  
         &+  &c_0  &=  &0      \\
         -\sqrt{3}c_2
         &  &    
         &+ &\sqrt{3}c_0 &=  &0
   \end{linsys} 
\end{equation*}
has the solution set \( c_1=-c_0 \) and \( c_2=c_0 \).
Taking the leading coefficient to be~\( c_2=1 \) 
gives the minimal polynomial $x^2-x+1$. 
\end{example}

% Using the method of that example to find the minimal polynomial of a
% \( \nbyn{3} \) matrix 
% would mean doing Gaussian reduction on
% a system with nine equations in ten unknowns.
That computation is ungainly.
We shall develop an alternative.
% To begin, note that we can break a polynomial of a map or a matrix into
% its components.
% (For this lemma, recall that we are using complex numbers in this chapter
% so all polynomials break completely into linear factors.)

\begin{lemma} \label{le:PolyMapsFactor}
Suppose that the polynomial \( f(x)=c_nx^n+\dots+c_1x+c_0 \) factors as
\( k(x-\lambda_1)^{q_1}\cdots(x-\lambda_z)^{q_z} \).
If \( t \) is a linear transformation then these two are equal maps. 
\begin{equation*}
  c_nt^n+\dots+c_1t+c_0
  =
  k\cdot\composed{\composed{(t-\lambda_1)^{q_1}}{\cdots}}{
      (t-\lambda_z)^{q_z}} 
\end{equation*}
Consequently, if \( T \) is a square matrix then \( f(T) \) and
\( k\cdot(T-\lambda_1I)^{q_1}\cdots(T-\lambda_z I)^{q_z} \) 
are equal matrices.
\end{lemma}

\begin{proof}
We use induction on the degree of the polynomial.
The cases where the polynomial is of
degree~zero and degree~one are clear.
The full induction argument is \nearbyexercise{exer:PolyMapsFactor}
but we will give its sense with the degree~two case.

A quadratic polynomial factors into two
linear terms \( f(x)=k(x-\lambda_1)\cdot(x-\lambda_2)
                    =k(x^2+(-\lambda_1-\lambda_2)x+\lambda_1\lambda_2) \).
% (the roots $\lambda_1$ and $\lambda_2$ could be equal).
Substituting \( t \) 
for \( x \) in the factored and
unfactored versions gives the same map.
\begin{align*}
   \bigl(k\cdot\composed{(t-\lambda_1)}{(t-\lambda_2)}\bigr)\>(\vec{v})
   &=\bigl(k\cdot(t-\lambda_1)\bigr)\,(t(\vec{v})-\lambda_2\vec{v})    \\
   &=k\cdot\bigl(t(t(\vec{v}))-t(\lambda_2\vec{v})
      -\lambda_1 t(\vec{v})-\lambda_1\lambda_2\vec{v}\bigr)    \\
   &=k\cdot \bigl(\composed{t}{t}\,(\vec{v})-(\lambda_1+\lambda_2)t(\vec{v})
          +\lambda_1\lambda_2\vec{v}\bigr)                    \\
   &=k\cdot(t^2-(\lambda_1+\lambda_2)t+\lambda_1\lambda_2)\>(\vec{v})
\end{align*}
The third equality uses linearity of \( t \) to bring $\lambda_2$ out of the
second term.
\end{proof}

% In particular, if a minimal polynomial for a transformation $t$ 
% factors as
% $m(x)=(x-\lambda_1)^{q_1}\cdots (x-\lambda_z)^{q_z}$
% then 
% \( m(t)=\composed{\composed{(t-\lambda_1)^{q_1}}{\cdots}}{
%       (t-\lambda_z)^{q_z}} \) 
% is the zero map. 
% Since \( m(t) \) sends every vector to zero, at least
% one of the maps \( t-\lambda_i \)  sends some
% nonzero vectors to zero.
% Exactly the same holds in the matrix case\Dash if $m$ is minimal for $T$ then
% \( m(T)=(T-\lambda_1I)^{q_1}\cdots (T-\lambda_z I)^{q_z} \)
% is the zero matrix and at least one of the matrices $T-\lambda_iI$
% sends some nonzero vectors to zero. 
% That is, in both cases at least some of the \( \lambda_i \) are eigenvalues.
% (\nearbyexercise{exer:SomeRootsMinPolyAreEigs} expands on this.)

The next result is that
every root of the minimal polynomial is an eigenvalue and
that every eigenvalue is a root of the minimal polynomial. 
(That is, the resule says `$1\leq q_i$' and 
not just `$0\leq q_i$'.)
For that, recall that to find eigenvalues
we solve $\deter{T-xI}=0$ and 
this determinant gives a polynomial in $x$, 
the characteristic polynomial, 
whose roots are the eigenvalues.

\begin{theorem}[Cayley-Hamilton]
\label{th:CayHam}
\index{Cayley-Hamilton theorem}
\hspace*{0em plus2em}
If the characteristic polynomial of a transformation or square matrix
factors into
\begin{equation*}
  k\cdot (x-\lambda_1)^{p_1}(x-\lambda_2)^{p_2}\cdots(x-\lambda_z)^{p_z}
\end{equation*}
then its minimal polynomial factors into
\begin{equation*}
  (x-\lambda_1)^{q_1}(x-\lambda_2)^{q_2}\cdots(x-\lambda_z)^{q_z}
\end{equation*}
where \( 1\leq q_i \leq p_i \) for each \( i \) between \( 1 \) and \( z \).
\end{theorem}

\noindent The proof takes up the next three lemmas.
We will state them in matrix terms 
because that version is convenient 
for the first proof but they apply equally
well to maps.

The first result is the key.
For the proof, observe that we can view
a matrix of polynomials as a polynomial with
matrix coefficients.
\begin{equation*}
   \begin{mat}
     2x^2+3x-1  &x^2+2    \\
     3x^2+4x+1  &4x^2+x+1
   \end{mat}
 = \begin{mat}[r]
    2  &1  \\
    3  &4
  \end{mat}x^2
 + \begin{mat}[r]
    3  &0  \\
    4  &1
  \end{mat}x
 + \begin{mat}[r]
   -1  &2  \\
    1  &1
  \end{mat}
\end{equation*}

\begin{lemma}   \label{le:MatSatItsCharPoly}
If \( T \) is a square matrix with characteristic polynomial \( c(x) \)
then \( c(T) \) is the zero matrix.
\end{lemma}

\begin{proof}
Let \( C \) be \( T-xI \),
the matrix whose determinant is the characteristic polynomial
\( c(x)=c_nx^n+\dots+c_1x+c_0 \).
\begin{equation*}
  C=\begin{mat}
    t_{1,1}-x        &t_{1,2}   &\ldots        \\
    t_{2,1}          &t_{2,2}-x               \\
    \vdots           &          &\ddots       \\
                     &          &       &t_{n,n}-x
  \end{mat}
\end{equation*}
Recall Theorem~Four.III.\ref{th:MatTimesAdjEqDiagDets},
that the product of a matrix with its adjoint equals
the determinant of the matrix times the identity.
\begin{equation*}
  c(x)\cdot I
  =\adj (C)C
  =\adj (C)(T-xI)
  =\adj (C)T- \adj(C)\cdot x
\tag*{($*$)}
\end{equation*}
The left side of~($*$) is 
$c_nIx^n+c_{n-1}Ix^{n-1}+\cdots+c_1Ix+c_0I$.
For the right side,
the entries of \( \adj (C) \) are polynomials, each of degree
at most \( n-1 \) since the minors of a matrix drop a row and column.
As suggested before the proof, rewrite it as a polynomial with
matrix coefficients:
\( \adj (C)=C_{n-1}x^{n-1}+\cdots+C_1x+C_0 \)
where each \( C_i \) is a matrix of scalars.
Now this is the right side of~($*$).
\begin{equation*}
  [(C_{n-1}T)x^{n-1}+\cdots+(C_1T)x+C_0T]  
   -[C_{n-1}x^n+C_{n-2}x^{n-1}+\cdots+C_0x]
\end{equation*}
Equate the left and right side of ($*$)'s
coefficients of \( x^n \), of $x^{n-1}$, etc.
\begin{align*}
  c_nI
  &=-C_{n-1}    \\
  c_{n-1}I
  &=-C_{n-2}+C_{n-1}T    \\
  &\vdotswithin{=}             \\
  c_{1}I
  &=-C_{0}+C_{1}T    \\
  c_{0}I
  &=C_{0}T
\end{align*}
Multiply, from the right, both sides of the first equation by \( T^n \), 
both sides of the second equation by \( T^{n-1} \), etc.
\begin{align*}
  c_nT^n
  &=-C_{n-1}T^n    \\
  c_{n-1}T^{n-1}
  &=-C_{n-2}T^{n-1}+C_{n-1}T^n    \\
  &\vdotswithin{=}             \\
  c_{1}T
  &=-C_{0}T+C_{1}T^2    \\
  c_{0}I
  &=C_{0}T
\end{align*}
Add.
The left is 
\( c_nT^n+c_{n-1}T^{n-1}+\cdots+c_0I \). 
The right telescopes;
for instance, $-C_{n-1}T^n$ from the first line combines with the 
$C_{n-1}T^n$ half of the second line. 
The total on the right is the zero matrix.
\end{proof}

We refer to that result by saying that a
matrix or map 
\definend{satisfies}\index{characteristic!polynomial!satisfied by} 
its characteristic polynomial.

\begin{lemma} \label{le:tSatisImpMinPolyDivides}
Any polynomial that is satisfied by \( T \) is divisible by
\( T \)'s minimal polynomial.
That is, for a polynomial \( f(x) \), if \( f(T) \) is the zero matrix 
then \( f(x) \) is divisible by the minimal polynomial of \( T \).
\end{lemma}

\begin{proof}
Let \( m(x) \) be minimal for \( T \).
The Division Theorem for Polynomials gives
\( f(x)=q(x)\cdot m(x)+r(x) \)
where the degree of \( r \) is strictly less than the degree of \( m \).
Because $T$ satisfies both $f$ and $m$, plugging $T$ into that equation gives
that \( r(T) \) is the zero matrix.
That contradicts the minimality of \( m \) unless \( r \)
is the zero polynomial.
\end{proof}

Combining the prior two lemmas shows that the minimal polynomial 
divides the characteristic polynomial. 
Thus
any root of the minimal polynomial is also a root of the characteristic
polynomial. 

Thus so far we have that if the minimal polynomial factors as
\( m(x)=(x-\lambda_1)^{q_1}\cdots(x-\lambda_i)^{q_i} \) then
the characteristic polynomial also has the roots \( \lambda_1\), \ldots, 
\( \lambda_i \).
But as far as what we have established to this point, 
the characteristic polynomial might have additional roots: 
\( c(x)=(x-\lambda_1)^{p_1}\cdots(x-\lambda_i)^{p_i}
     (x-\lambda_{i+1})^{p_{i+1}}\cdots(x-\lambda_z)^{p_z} \), where
\( 1\leq q_j\leq p_j \) for \( 1\leq j \leq i \).
We finish the proof of the Cayley-Hamilton Theorem by showing that 
the characteristic polynomial has no additional roots so that 
there are no $\lambda_{i+1}$, $\lambda_{i+2}$, etc.

\begin{lemma}
Each linear factor of the characteristic polynomial of a square matrix
is also a linear factor of the minimal polynomial.
\end{lemma}

\begin{proof}
Let \( T \) be a square matrix with minimal polynomial \( m(x) \) 
of degree~\( n \) and 
assume that \( x-\lambda \) is a factor of the characteristic polynomial of 
\( T \), so that \( \lambda \) is an eigenvalue of \( T \).
We will show that $m(\lambda)=0$, i.e., that $x-\lambda$ is a factor of $m$.

Assume that $\lambda$ is associated with the eigenvector~$\vec{v}$ and
consider the powers $T^2(\vec{v})$, \ldots, \( T^n(\vec{v}) \).
We have
$T^2(\vec{v})
=T\cdot\lambda\vec{v}=\lambda\cdot T\vec{v}=\lambda^2\vec{v}$.
The same happens for all of the powers: 
$T^i\vec{v}=\lambda^i\vec{v}\,$ for \( 1\leq i \leq n \).
Thus
for any polynomial function \( p(x) \), application of the matrix \( p(T) \)
to \( \vec{v} \) equals the result of multiplying \( \vec{v} \) by the scalar
\( p(\lambda) \).
\begin{multline*}
  p(T)\,\vec{v}
  =(c_kT^k+\dots+c_1T+c_0I)\,\vec{v}
  =c_kT^k\vec{v}+\dots+c_1T\vec{v}+c_0\vec{v}  \\
  =c_k\lambda^k\vec{v}+\dots+c_1\lambda\vec{v}+c_0\vec{v}
  =p(\lambda)\cdot\vec{v}
\end{multline*}
Since \( m(T) \) is the zero matrix,
\( \zero=m(T)(\vec{v})=m(\lambda)\cdot\vec{v} \) and
hence \( m(\lambda)=0 \), as \( \vec{v}\neq\zero \) 
because it is an eigenvector.
\end{proof}

That concludes the proof of the Cayley-Hamilton Theorem.

\begin{example} \label{ex:MinPolyUsingCH}
We can use the Cayley-Hamilton Theorem to find the minimal polynomial of
this matrix.
\begin{equation*}
   T=
   \begin{mat}[r]
      2  &0  &0  &1  \\
      1  &2  &0  &2  \\
      0  &0  &2  &-1 \\
      0  &0  &0  &1
   \end{mat}
\end{equation*}
First we find its characteristic polynomial \( c(x)=(x-1)(x-2)^3 \)
with the usual determinant.
Now, the Cayley-Hamilton Theorem says that 
\( T \)'s minimal polynomial is either
\( (x-1)(x-2) \) or
\( (x-1)(x-2)^2 \) or
\( (x-1)(x-2)^3 \).
We can decide among the choices just by computing
\begin{equation*}
   (T-1I)(T-2I)=\!
   \begin{mat}[r]
      1  &0  &0  &1  \\
      1  &1  &0  &2  \\
      0  &0  &1  &-1 \\
      0  &0  &0  &0
   \end{mat}
   \begin{mat}[r]
      0  &0  &0  &1  \\
      1  &0  &0  &2  \\
      0  &0  &0  &-1 \\
      0  &0  &0  &-1
   \end{mat}
   =
   \begin{mat}[r]
      0  &0  &0  &0  \\
      1  &0  &0  &1  \\
      0  &0  &0  &0  \\
      0  &0  &0  &0
   \end{mat}
\end{equation*}
and
\begin{equation*}
   (T-1I)(T-2I)^2=
   \begin{mat}[r]
      0  &0  &0  &0  \\
      1  &0  &0  &1  \\
      0  &0  &0  &0  \\
      0  &0  &0  &0
   \end{mat}
   \begin{mat}[r]
      0  &0  &0  &1  \\
      1  &0  &0  &2  \\
      0  &0  &0  &-1 \\
      0  &0  &0  &-1
   \end{mat}
   =
   \begin{mat}[r]
      0  &0  &0  &0  \\
      0  &0  &0  &0  \\
      0  &0  &0  &0  \\
      0  &0  &0  &0
   \end{mat}
\end{equation*}
and so \( m(x)=(x-1)(x-2)^2 \).
\end{example}



\begin{exercises}
  \recommended \item 
    What are the possible minimal polynomials if a matrix has
    the given characteristic polynomial?
    \begin{exparts*}
      \partsitem $(x-3)^4$
      \partsitem $(x+1)^3(x-4)$
      \partsitem $(x-2)^2(x-5)^2$
      \partsitem  \( (x+3)^2(x-1)(x-2)^2 \)
    \end{exparts*}
    What is the degree of each possibility?
    \begin{answer} 
      The Cayley-Hamilton Theorem  \nearbytheorem{th:CayHam} says that
      the minimal polynomial must contain the same linear factors
      as the characteristic polynomial, although possibly of lower degree
      but not of zero degree.
      \begin{exparts}
        \partsitem The possibilities are 
          $m_1(x)=x-3$, $m_2(x)=(x-3)^2$, $m_3(x)=(x-3)^3$,
          and $m_4(x)=(x-3)^4$.
          The first is a degree one polynomial, the second is degree two,
          the third is degree three, and the fourth is degree four.
        \partsitem The possibilities are $m_1(x)=(x+1)(x-4)$,
          $m_2(x)=(x+1)^2(x-4)$, and $m_3(x)=(x+1)^3(x-4)$.
          The first is a quadratic polynomial, that is, it has degree two.
          The second has degree three, and the third has degree four.
        \partsitem We have $m_1(x)=(x-2)(x-5)$, $m_2(x)=(x-2)^2(x-5)$,
          $m_3(x)=(x-2)(x-5)^2$, and $m_4(x)=(x-2)^2(x-5)^2$.
          They are polynomials of degree two, three, three, and four.
        \partsitem The possibilities are \( m_1(x)=(x+3)(x-1)(x-2) \),
          \( m_2(x)=(x+3)^2(x-1)(x-2) \),
          \( m_3(x)=(x+3)(x-1)(x-2)^2 \),
          and \( m_4(x)=(x+3)^2(x-1)(x-2)^2 \).
          The degree of $m_1$ is three, the degree of $m_2$ is four,
          the degree of $m_3$ is four, and the degree of $m_4$ is five.
      \end{exparts}
    \end{answer}
  \recommended \item 
    Find the minimal polynomial of each matrix.
    \begin{exparts*}
       \partsitem \( \begin{mat}[r]
                   3  &0  &0  \\
                   1  &3  &0  \\
                   0  &0  &4
                \end{mat} \)
       \partsitem \( \begin{mat}[r]
                   3  &0  &0  \\
                   1  &3  &0  \\
                   0  &0  &3
                \end{mat} \)
       \partsitem \( \begin{mat}[r]
                   3  &0  &0  \\
                   1  &3  &0  \\
                   0  &1  &3
                \end{mat} \)
       \partsitem \( \begin{mat}[r]
                   2  &0  &1  \\
                   0  &6  &2  \\
                   0  &0  &2
                \end{mat} \)
       \partsitem \( \begin{mat}[r]
                   2  &2  &1  \\
                   0  &6  &2  \\
                   0  &0  &2
                \end{mat} \)
       \partsitem \( \begin{mat}[r]
                   -1 &4  &0  &0  &0  \\
                    0 &3  &0  &0  &0  \\
                    0 &-4 &-1 &0  &0  \\
                    3 &-9 &-4 &2  &-1 \\
                    1 &5  &4  &1  &4
                \end{mat} \)
    \end{exparts*}
    \begin{answer}
      In each case we will use the method of \nearbyexample{ex:MinPolyUsingCH}.
      \begin{exparts}
       \partsitem Because $T$ is triangular, $T-xI$ is also triangular
         \begin{equation*}
           T-xI=
           \begin{mat}
             3-x  &0    &0   \\
             1    &3-x  &0   \\
             0    &0    &4-x
           \end{mat}
         \end{equation*}
         the characteristic polynomial is
         easy $c(x)=\deter{T-xI}=(3-x)^2(4-x)=-1\cdot (x-3)^2(x-4)$.
         There are only two possibilities for the minimal polynomial,
         $m_1(x)=(x-3)(x-4)$ and $m_2(x)=(x-3)^2(x-4)$.
         (Note that the characteristic polynomial has a negative sign
         but the minimal polynomial does not since it must
         have a leading coefficient of one).
         Because $m_1(T)$ is not the zero matrix
         \begin{equation*}
           (T-3I)(T-4I)
           =
           \begin{mat}[r]
             0  &0  &0  \\
             1  &0  &0  \\
             0  &0  &1
           \end{mat}
           \begin{mat}[r]
             -1  &0  &0  \\
              1  &-1 &0  \\
              0  &0  &0
           \end{mat}
           =
           \begin{mat}[r]
             0  &0  &0  \\
            -1  &0  &0  \\
             0  &0  &0
           \end{mat}
         \end{equation*}
         the minimal polynomial is $m(x)=m_2(x)$.
         \begin{multline*}
           (T-3I)^2(T-4I)
           =(T-3I)\cdot\bigl((T-3I)(T-4I)\bigr)              \\
           =
           \begin{mat}
             0  &0  &0  \\
             1  &0  &0  \\
             0  &0  &1
           \end{mat}
           \begin{mat}[r]
              0  &0  &0  \\
             -1  &0  &0  \\
              0  &0  &0
           \end{mat}
           =
           \begin{mat}[r]
             0  &0  &0  \\
             0  &0  &0  \\
             0  &0  &0
           \end{mat}
         \end{multline*}
       \partsitem As in the prior item, the fact that the matrix is 
        triangular makes computation of the characteristic polynomial
        easy.
        \begin{equation*}
          c(x)=\deter{T-xI}
              =
              \begin{vmat}
                3-x  &0   &0   \\
                1    &3-x &0   \\
                0    &0   &3-x
              \end{vmat}
              =(3-x)^3=-1\cdot (x-3)^3
        \end{equation*}
        There are three possibilities for the minimal polynomial
        $m_1(x)=(x-3)$, $m_2(x)=(x-3)^2$, and $m_3(x)=(x-3)^3$.
        We settle the question by computing $m_1(T)$
        \begin{equation*}
          T-3I=
          \begin{mat}[r]
            0  &0  &0  \\
            1  &0  &0  \\
            0  &0  &0
          \end{mat}
        \end{equation*}
        and $m_2(T)$.
        \begin{equation*}
          (T-3I)^2=
          \begin{mat}[r]
            0  &0  &0  \\
            1  &0  &0  \\
            0  &0  &0
          \end{mat}
          \begin{mat}[r]
            0  &0  &0  \\
            1  &0  &0  \\
            0  &0  &0
          \end{mat}
          =          
          \begin{mat}[r]
            0  &0  &0  \\
            0  &0  &0  \\
            0  &0  &0
          \end{mat}
        \end{equation*}
        Because $m_2(T)$ is the zero matrix, $m_2(x)$ is the minimal
        polynomial.
       \partsitem Again, the matrix is triangular.
        \begin{equation*}
          c(x)=\deter{T-xI}
              =
              \begin{vmat}
                3-x  &0   &0   \\
                1    &3-x &0   \\
                0    &1   &3-x
              \end{vmat}
              =(3-x)^3=-1\cdot (x-3)^3
        \end{equation*}
        Again, there are three possibilities for the minimal polynomial
        $m_1(x)=(x-3)$, $m_2(x)=(x-3)^2$, and $m_3(x)=(x-3)^3$.
        We compute $m_1(T)$
        \begin{equation*}
          T-3I=
          \begin{mat}[r]
            0  &0  &0  \\
            1  &0  &0  \\
            0  &1  &0
          \end{mat}
        \end{equation*}
        and $m_2(T)$
        \begin{equation*}
          (T-3I)^2=
          \begin{mat}[r]
            0  &0  &0  \\
            1  &0  &0  \\
            0  &1  &0
          \end{mat}
          \begin{mat}[r]
            0  &0  &0  \\
            1  &0  &0  \\
            0  &1  &0
          \end{mat}
          =          
          \begin{mat}[r]
            0  &0  &0  \\
            0  &0  &0  \\
            1  &0  &0
          \end{mat}
        \end{equation*}
        and $m_3(T)$.
        \begin{equation*}
          (T-3I)^3
          =(T-3I)^2(T-3I)
          =
          \begin{mat}[r]
            0  &0  &0  \\
            0  &0  &0  \\
            1  &0  &0
          \end{mat}
          \begin{mat}[r]
            0  &0  &0  \\
            1  &0  &0  \\
            0  &1  &0
          \end{mat}
          =          
          \begin{mat}[r]
            0  &0  &0  \\
            0  &0  &0  \\
            0  &0  &0
          \end{mat}
        \end{equation*}
        Therefore, the minimal polynomial is $m(x)=m_3(x)=(x-3)^3$.
       \partsitem This case is also triangular, here upper triangular.
         \begin{equation*}
           c(x)=\deter{T-xI}=
           \begin{vmat}
             2-x  &0   &1     \\
             0    &6-x &2     \\
             0    &0   &2-x
           \end{vmat}
           =(2-x)^2(6-x)=-(x-2)^2(x-6)
         \end{equation*}
         There are two possibilities for the minimal polynomial,
         $m_1(x)=(x-2)(x-6)$ and $m_2(x)=(x-2)^2(x-6)$.
         Computation shows that the minimal polynomial isn't $m_1(x)$.
         \begin{equation*}
           (T-2I)(T-6I)=
           \begin{mat}[r]
             0  &0  &1  \\
             0  &4  &2  \\
             0  &0  &0  
           \end{mat}
           \begin{mat}[r]
             -4  &0  &1  \\
              0  &0  &2  \\
              0  &0  &-4
           \end{mat}
           =
           \begin{mat}[r]
             0  &0  &-4  \\
             0  &0  &0   \\
             0  &0  &0
           \end{mat}
         \end{equation*}
         It therefore must be that $m(x)=m_2(x)=(x-2)^2(x-6)$. 
         Here is a verification.
         \begin{multline*}
           (T-2I)^2(T-6I)=(T-2I)\cdot\bigl((T-2I)(T-6I)\bigr)             \\
           =
           \begin{mat}[r]
             0  &0  &1  \\
             0  &4  &2  \\
             0  &0  &0  
           \end{mat}
           \begin{mat}[r]
              0  &0  &-4   \\
              0  &0  &0   \\
              0  &0  &0
           \end{mat}
           =
           \begin{mat}[r]
             0  &0  &0  \\
             0  &0  &0   \\
             0  &0  &0
           \end{mat}
         \end{multline*}
       \partsitem The characteristic polynomial is 
         \begin{equation*}
           c(x)=\deter{T-xI}=
           \begin{vmat}
             2-x  &2   &1     \\
             0    &6-x &2     \\
             0    &0   &2-x
           \end{vmat}
           =(2-x)^2(6-x)=-(x-2)^2(x-6)
         \end{equation*}
         and there are two possibilities for the minimal polynomial,
         $m_1(x)=(x-2)(x-6)$ and $m_2(x)=(x-2)^2(x-6)$.
         Checking the first one
         \begin{equation*}
           (T-2I)(T-6I)=
           \begin{mat}[r]
             0  &2  &1  \\
             0  &4  &2  \\
             0  &0  &0  
           \end{mat}
           \begin{mat}[r]
             -4  &2  &1  \\
              0  &0  &2  \\
              0  &0  &-4
           \end{mat}
           =
           \begin{mat}[r]
             0  &0  &0  \\
             0  &0  &0   \\
             0  &0  &0
           \end{mat}
         \end{equation*}
         shows that the minimal polynomial is
         $m(x)=m_1(x)=(x-2)(x-6)$.
       \partsitem The characteristic polynomial is this.
         \begin{equation*}
           c(x)=\deter{T-xI}=
           \begin{vmat} 
              -1-x &4    &0    &0    &0    \\
               0   &3-x  &0    &0    &0    \\
               0   &-4   &-1-x &0    &0    \\
               3   &-9   &-4   &2-x  &-1   \\
               1   &5    &4    &1    &4-x
           \end{vmat}     
           =(x-3)^3(x+1)^2
         \end{equation*}
         Here are the possibilities for the minimal polynomial,
         listed here by ascending degree:
         $m_1(x)=(x-3)(x+1)$, $m_1(x)=(x-3)^2(x+1)$, $m_1(x)=(x-3)(x+1)^2$, 
         $m_1(x)=(x-3)^3(x+1)$, $m_1(x)=(x-3)^2(x+1)^2$, 
         and $m_1(x)=(x-3)^3(x+1)^2$. 
         The first one doesn't pan out
         \begin{align*}
           (T-3I)(T+1I)
           &=
           \begin{mat}[r] 
              -4   &4    &0    &0    &0    \\
               0   &0    &0    &0    &0    \\
               0   &-4   &-4   &0    &0    \\
               3   &-9   &-4   &-1   &-1   \\
               1   &5    &4    &1    &1  
           \end{mat}     
           \begin{mat}[r] 
               0   &4    &0    &0    &0    \\
               0   &4    &0    &0    &0    \\
               0   &-4   &0    &0    &0    \\
               3   &-9   &-4   &3    &-1   \\
               1   &5    &4    &1    &5  
           \end{mat}                           \\     
           &=
           \begin{mat}[r] 
               0   &0    &0    &0    &0    \\
               0   &0    &0    &0    &0    \\
               0   &0    &0    &0    &0    \\
              -4   &-4   &0    &-4   &-4   \\
               4   &4    &0    &4    &4  
           \end{mat}           
         \end{align*}
         but the second one does.
         \begin{multline*}
           (T-3I)^2(T+1I)=(T-3I)\bigl((T-3I)(T+1I)\bigr) \\
           \begin{aligned}
           &=
           \begin{mat}[r] 
              -4   &4    &0    &0    &0    \\
               0   &0    &0    &0    &0    \\
               0   &-4   &-4   &0    &0    \\
               3   &-9   &-4   &-1   &-1   \\
               1   &5    &4    &1    &1  
           \end{mat}     
           \begin{mat}[r] 
               0   &0    &0    &0    &0    \\
               0   &0    &0    &0    &0    \\
               0   &0    &0    &0    &0    \\
              -4   &-4   &0    &-4   &-4   \\
               4   &4    &0    &4    &4  
           \end{mat}                          \\          
           &=
           \begin{mat}[r] 
               0   &0    &0    &0    &0    \\
               0   &0    &0    &0    &0    \\
               0   &0    &0    &0    &0    \\
               0   &0    &0    &0    &0    \\
               0   &0    &0    &0    &0  
           \end{mat}  
           \end{aligned}         
         \end{multline*}
         The minimal polynomial is \( m(x)=(x-3)^2(x+1) \).
      \end{exparts} 
    \end{answer}
   \item 
     Find the minimal polynomial of this matrix.
     \begin{equation*}
        \begin{mat}[r]
           0  &1  &0  \\
           0  &0  &1  \\
           1  &0  &0
        \end{mat}
     \end{equation*}
     \begin{answer}
       Its characteristic polynomial has complex roots.
       \begin{equation*}
          \begin{vmat}
                   -x  &1  &0  \\
                    0  &-x &1  \\
                    1  &0  &-x
          \end{vmat}
          =(1-x)\cdot (x-(-\frac{1}{2}+\frac{\sqrt{3}}{2}i))
                \cdot (x-(-\frac{1}{2}-\frac{\sqrt{3}}{2}i))
       \end{equation*}
       As the roots are distinct, the characteristic polynomial equals the
       minimal polynomial. 
     \end{answer}
  \recommended \item 
     What is the minimal polynomial of the differentiation
     operator $d/dx$ on \( \polyspace_n \)?
     \begin{answer}
       We know that $\polyspace_n$ is a dimension $n+1$ space and that
       the differentiation operator is
       nilpotent of index~$n+1$ (for instance, taking $n=3$, 
       $\polyspace_3=\set{c_3x^3+c_2x^2+c_1x+c_0\suchthat c_3,\ldots,c_0\in\C}$
       and the fourth derivative of a cubic is the zero polynomial).  
       Represent this operator using the canonical 
       form for nilpotent transformations.
       \begin{equation*}
         \begin{mat}
           0  &0  &0  &\ldots &  &0  \\
           1  &0  &0  &       &  &0  \\
           0  &1  &0  &       &  &   \\
              &   &\ddots            \\
           0  &0  &0  &       &1 &0 
         \end{mat}
       \end{equation*}
       This is an $\nbyn{(n+1)}$ matrix with an easy 
       characteristic polynomial,
       $c(x)=x^{n+1}$.
       (\textit{Remark:} this matrix is $\rep{d/dx}{B,B}$ where
        $B=\sequence{x^n,nx^{n-1},n(n-1)x^{n-2},\ldots,n!}$.)
       To find the minimal polynomial as in \nearbyexample{ex:MinPolyUsingCH}
       we consider the powers of $T-0I=T$.
       But, of course, the first power of $T$ that is the zero matrix is 
       the power $n+1$.
       So the minimal polynomial is also \( x^{n+1} \).
     \end{answer}
  \recommended \item 
    Find the minimal polynomial of matrices of this form
    \begin{equation*}
      \begin{mat}
        \lambda  &0        &0          &\ldots  &        &0  \\
        1        &\lambda  &0          &        &        &0  \\
        0        &1        &\lambda                          \\
                 &         &           &\ddots                \\
                 &         &           &        &\lambda &0   \\
        0        &0        &\ldots     &        &1       &\lambda
      \end{mat}
    \end{equation*}
    where the scalar $\lambda$ is fixed (i.e., is not a variable).
    \begin{answer}
      Call the matrix $T$ and suppose that it is \( \nbyn{n} \).
      Because $T$ is triangular, and so $T-xI$ is triangular,
      the characteristic polynomial is $c(x)=(x-\lambda)^n$.
      To see that the minimal polynomial is the same, consider
      $T-\lambda I$.
      \begin{equation*}
        \begin{mat}
          0        &0        &0          &\ldots  &0  \\
          1        &0        &0          &\ldots  &0  \\
          0        &1        &0                       \\
                   &         &\ddots                  \\
          0        &0        &\ldots     &1       &0      
        \end{mat}
      \end{equation*}
      Recognize it as the canonical form for a transformation that is 
      nilpotent of degree~$n$; the power $(T-\lambda I)^j$ is zero first
      when $j$ is $n$.
    \end{answer}
  \item 
    What is the minimal polynomial of the transformation of
    \( \polyspace_n \) that sends \( p(x) \) to \( p(x+1) \)?
    \begin{answer}
      The $n=3$ case provides a hint.
      A natural basis for $\polyspace_3$ is  
      $B=\sequence{1,x,x^2,x^3}$.
      The action of the transformation is
      \begin{equation*}
        1\mapsto 1
        \quad
        x\mapsto x+1
        \quad
        x^2\mapsto x^2+2x+1
        \quad
        x^3\mapsto x^3+3x^2+3x+1
      \end{equation*}
      and so the representation $\rep{t}{B,B}$ is this upper triangular matrix.
      \begin{equation*}
        \begin{mat}[r]
          1  &1  &1  &1  \\
          0  &1  &2  &3  \\
          0  &0  &1  &3  \\
          0  &0  &0  &1
        \end{mat}
      \end{equation*}
      Because it is triangular, the fact that the characteristic polynomial is
      $c(x)=(x-1)^4$ is clear.
      For the minimal polynomial, the candidates are $m_1(x)=(x-1)$,
      \begin{equation*}
        T-1I=
        \begin{mat}[r]
          0  &1  &1  &1  \\
          0  &0  &2  &3  \\
          0  &0  &0  &3  \\
          0  &0  &0  &0
        \end{mat}
      \end{equation*}
      $m_2(x)=(x-1)^2$, 
      \begin{equation*}
        (T-1I)^2=
        \begin{mat}[r]
          0  &0  &2  &6  \\
          0  &0  &0  &6  \\
          0  &0  &0  &0  \\
          0  &0  &0  &0
        \end{mat}
      \end{equation*}
      $m_3(x)=(x-1)^3$,
      \begin{equation*}
        (T-1I)^3=
        \begin{mat}[r]
          0  &0  &0  &6  \\
          0  &0  &0  &0  \\
          0  &0  &0  &0  \\
          0  &0  &0  &0
        \end{mat}
      \end{equation*}
      and $m_4(x)=(x-1)^4$.
      Because $m_1$, $m_2$, and $m_3$ are not right, $m_4$ must be right,
      as is easily verified.
      
      In the case of a general $n$, the representation is an upper
      triangular matrix with ones on the diagonal.
      Thus the characteristic polynomial is $c(x)=(x-1)^{n+1}$.
      One way to verify that the minimal polynomial equals the 
      characteristic polynomial is argue something like this:
      say that an upper triangular matrix is $0$-upper triangular if
      there are nonzero entries on the diagonal, that it is $1$-upper 
      triangular if the diagonal contains only zeroes and there are nonzero
      entries just above the diagonal, etc.
      As the above example illustrates, an induction argument will 
      show that, where $T$ has only nonnegative entries, 
      $T^j$ is $j$-upper triangular.
      % We leave that argument to the reader.
    \end{answer}
   \item 
     What is the minimal polynomial of
     the map \( \map{\pi}{\C^3}{\C^3} \)
     projecting onto the first two coordinates?
      \begin{answer}
        The map twice is the same as the map once:~$\composed{\pi}{\pi}=\pi$,
        that is, $\pi^2=\pi$ and so the minimal polynomial is of degree
        at most two since \( m(x)=x^2-x \) will do.
        The fact that no linear polynomial will do follows from applying
        the maps on the left and right side of 
        $c_1\cdot \pi+c_0\cdot \identity=z$ (where $z$ is the zero map)
        to these two vectors.
        \begin{equation*}
          \colvec[r]{0 \\ 0 \\ 1}
          \qquad
          \colvec[r]{1 \\ 0 \\ 0}
        \end{equation*}
        Thus the minimal polynomial is $m$.
      \end{answer}
   \item 
     Find a \( \nbyn{3} \) matrix whose minimal
     polynomial is \( x^2 \).
     \begin{answer}
        This is one answer.
        \begin{equation*}
            \begin{mat}[r]
              0  &0  &0  \\
              1  &0  &0  \\
              0  &0  &0
            \end{mat}
        \end{equation*} 
      \end{answer}
  \item 
     What is wrong with this claimed proof of
     \nearbylemma{le:MatSatItsCharPoly}:
      ``if \( c(x)=\deter{T-xI} \) then \( c(T)=\deter{T-TI}=0 \)''?
     \cite{Cullen}
     \begin{answer}
       The \( x \) must be a scalar, not a matrix.
     \end{answer}
  \item 
    Verify \nearbylemma{le:MatSatItsCharPoly} for \( \nbyn{2} \)
    matrices by direct calculation.
    \begin{answer}
      The characteristic polynomial of
      \begin{equation*}
         T=\begin{mat}
              a  &b  \\
              c  &d
           \end{mat}
      \end{equation*}
      is \( (a-x)(d-x)-bc=x^2-(a+d)x+(ad-bc) \).
      Substitute
      \begin{multline*}
         \begin{mat}
              a  &b  \\
              c  &d
         \end{mat}^2
         -
         (a+d)\begin{mat}
            a  &b  \\
            c  &d
         \end{mat}
         +
         (ad-bc)\begin{mat}[r]
            1  &0  \\
            0  &1
         \end{mat}                            \\
         =                     
         \begin{mat}
            a^2+bc  &ab+bd  \\
            ac+cd   &bc+d^2
         \end{mat}
         -
         \begin{mat}
            a^2+ad  &ab+bd   \\
            ac+cd   &ad+d^2
         \end{mat}
         +
         \begin{mat}
            ad-bc  &0      \\
            0      &ad-bc
         \end{mat}
      \end{multline*}
      and just check each entry sum to see that the result is the zero matrix.
    \end{answer}
  \recommended \item
    Prove that the minimal polynomial of an \( \nbyn{n} \) matrix has
    degree at most \( n \) (not \( n^2 \) as a person might guess from this
    subsection's opening).
    Verify that this maximum, \( n \), can happen.
    \begin{answer}
      By the Cayley-Hamilton theorem the degree of the minimal polynomial is
      less than or equal to the degree of the characteristic polynomial,
      \( n \).
      \nearbyexample{ex:MinPolyForRotMat} shows that \( n \) can happen.
    \end{answer}
   \recommended \item 
     Show that, on a nontrivial vector space, a linear transformation is 
     nilpotent if and only if its only eigenvalue is zero.
     \begin{answer}
       Let the linear transformation be $\map{t}{V}{V}$.
       If $t$ is nilpotent then there is an $n$ such that $t^n$ is the zero map,
       so $t$ satisfies the polynomial $p(x)=x^n=(x-0)^n$.
       By \nearbylemma{le:tSatisImpMinPolyDivides} the minimal polynomial of 
       $t$ divides $p$, so the minimal polynomial has only zero for a root.
       By Cayley-Hamilton, \nearbytheorem{th:CayHam},
       the characteristic polynomial has only zero for a root.
       Thus the only eigenvalue of $t$ is zero.

       Conversely, if a transformation \( t \) on an
       \( n \)-dimensional space has only the single eigenvalue of zero 
       then its characteristic polynomial is \( x^n \). 
       The Cayley-Hamilton Theorem says that a map satisfies its
       characteristic polynomial so \( t^n \) is the zero map.
       Thus $t$ is nilpotent.
     \end{answer}
   \item 
       What is the minimal polynomial of a zero map or matrix?
       Of an identity map or matrix?
       \begin{answer}
         A minimal polynomial must have leading coefficient $1$, 
         and so if the minimal polynomial of a map or matrix were to 
         be a degree zero polynomial then it would be $m(x)=1$.
         But the identity map or matrix equals the zero map or matrix
         only on a trivial vector space.

         So in the nontrivial case the minimal polynomial must be of degree
         at least one.
         A zero map or matrix has minimal polynomial \( m(x)=x \), and an
         identity map or matrix has minimal polynomial \( m(x)=x-1 \). 
       \end{answer}
  \recommended \item 
     Interpret the minimal polynomial of 
     \nearbyexample{ex:PolySendRotMatToZ} geometrically.
     \begin{answer}
       We can interpret the polynomial can geometrically as, ``a \( \degs{60} \)
       rotation minus two rotations of \( \degs{30} \) equals the
       identity.''
     \end{answer}
   \item 
     What is the minimal polynomial of a diagonal matrix?
     \begin{answer}
       For a diagonal matrix
       \begin{equation*}
          T=
          \begin{mat}
             t_{1,1}   &0        \\
             0         &t_{2,2}  \\
                       &        &\ddots  \\
                       &        &      &t_{n,n}
          \end{mat}
       \end{equation*}
       the characteristic polynomial is 
       $(t_{1,1}-x)(t_{2,2}-x)\cdots (t_{n,n}-x)$.     
       Of course, some of those factors may be repeated, e.g., the matrix might
       have $t_{1,1}=t_{2,2}$.
       For instance, the characteristic polynomial of
       \begin{equation*}
          D=
          \begin{mat}[r]
             3 &0 &0  \\
             0 &3 &0  \\
             0 &0 &1
          \end{mat}
       \end{equation*}
       is \( (3-x)^2(1-x)=-1\cdot (x-3)^2(x-1) \). 

       To form the minimal polynomial, 
       take the terms \( x-t_{i,i} \), throw out repeats, 
       and multiply them together.
       For instance, the minimal polynomial of $D$
       is \( (x-3)(x-1) \).
       To check this, note first that \nearbytheorem{th:CayHam}, 
       the Cayley-Hamilton theorem, requires that each linear factor in the
       characteristic polynomial appears at least once in the minimal
       polynomial.
       One way to check the other direction\Dash that in the case of
       a diagonal matrix, 
       each linear factor need appear at most once\Dash is to
       use a matrix argument.
       A diagonal matrix, multiplying from the left, rescales rows by
       the entry on the diagonal.
       But in a product $(T-t_{1,1}I)\cdots\hbox{}$, even without any repeat
       factors, every row is zero in at least one of the factors. 

       For instance, in the product 
       \begin{equation*}
         (D-3I)(D-1I)=(D-3I)(D-1I)I=
         \begin{mat}[r]
           0  &0  &0  \\
           0  &0  &0  \\
           0  &0  &-2        
         \end{mat}
         \begin{mat}[r]
           2  &0  &0  \\
           0  &2  &0  \\
           0  &0  &0
         \end{mat}
         \begin{mat}[r]
           1  &0  &0  \\
           0  &1  &0  \\
           0  &0  &1
         \end{mat}
       \end{equation*}
       because the first and second rows of the first matrix $D-3I$ are
       zero, the entire product will have a first row and second
       row that are zero.
       And because the third row of the middle matrix $D-1I$ is zero,
       the entire product has a third row of zero.
    \end{answer}
  \recommended \item 
    A \definend{projection}\index{projection}%
    \index{transformation!projection} 
    is any transformation \( t \) such that \( t^2=t \).
    (For instance, consider the transformation of the plane $\Re^2$ projecting
    each vector onto its first coordinate.
    If we project twice then we get the same result as if we project just once.)
    What is the minimal polynomial of a projection?
    \begin{answer}
      This subsection starts with the observation that the powers of 
      a linear transformation cannot climb forever without a ``repeat'',
      that is, that for some power~$n$ there is a linear relationship
      $c_n\cdot t^n+\dots+c_1\cdot t+c_0\cdot \identity=z$ where $z$ is the
      zero transformation.
      The definition of projection is that for such a map
      one linear relationship is quadratic, $t^2-t=z$.
      To finish, we need only consider whether this relationship might not
      be minimal, that is, are there projections for which the 
      minimal polynomial is constant or linear?

      For the minimal polynomial to be constant, the map would have to
      satisfy that $c_0\cdot\identity=z$, where $c_0=1$ since the leading
      coefficient of a minimal polynomial is $1$.
      This is only satisfied by the zero transformation on a trivial space.
      This is a projection, but not an interesting one.

      For the minimal polynomial of a transformation to be linear would give 
      $c_1\cdot t+c_0\cdot\identity=z$ where $c_1=1$.
      This equation gives $t=-c_0\cdot \identity$.
      Coupling it with the requirement that $t^2=t$ gives
      $t^2=(-c_0)^2\cdot\identity=-c_0\cdot\identity$, which gives that
      $c_0=0$ and $t$ is the zero transformation or that $c_0=1$ and
      $t$ is the identity.       

      Thus, except in the cases where the projection is a zero map or an
      identity map, the minimal polynomial is $m(x)=x^2-x$. 
    \end{answer}
  \item \label{exer:SomeRootsMinPolyAreEigs}
    \textit{The first two items of this question are review.}
    \begin{exparts}
      \partsitem Prove that the composition of one-to-one maps is
        one-to-one.
      \partsitem Prove that if a linear map is not one-to-one then
        at least one nonzero vector from the domain maps to the 
        zero vector in the codomain.
      \partsitem Verify the statement, excerpted here, that
         precedes \nearbytheorem{th:CayHam}.
         \begin{quotation}
           \noindent \ldots{} 
           if a minimal polynomial $m(x)$ for a transformation $t$ 
           factors as
           $m(x)=(x-\lambda_1)^{q_1}\cdots (x-\lambda_z)^{q_z}$
           then 
           \( m(t)=\composed{\composed{(t-\lambda_1)^{q_1}}{\cdots}}{
                                                        (t-\lambda_z)^{q_z}} \) 
          is the zero map. 
          Since \( m(t) \) sends every vector to zero, at least
          one of the maps \( t-\lambda_i \)  sends some
          nonzero vectors to zero.  \ldots{}
          That is, \ldots{} 
          at least some of the \( \lambda_i \) are eigenvalues.
        \end{quotation}
    \end{exparts}
    \begin{answer}
      \begin{exparts}
       \partsitem \textit{This is a property of functions in general,
          not just of linear functions.}
          Suppose that $f$ and $g$ are one-to-one functions such that
          $\composed{f}{g}$ is defined.
          Let $\composed{f}{g}(x_1)=\composed{f}{g}(x_2)$, so that
          $f(g(x_1))=f(g(x_2))$.
          Because $f$ is one-to-one this implies that $g(x_1)=g(x_2)$.
          Because $g$ is also one-to-one, this in turn implies that
          $x_1=x_2$.
          Thus, in summary, $\composed{f}{g}(x_1)=\composed{f}{g}(x_2)$
          implies that $x_1=x_2$ and so $\composed{f}{g}$ is one-to-one.
        \partsitem If the linear map $h$ 
          is not one-to-one then there are unequal
          vectors $\vec{v}_1$, $\vec{v}_2$ that map to the same value
          $h(\vec{v}_1)=h(\vec{v}_2)$.
          Because $h$ is linear, we have
          $\zero=h(\vec{v}_1)-h(\vec{v}_2)=h(\vec{v}_1-\vec{v}_2)$
          and so $\vec{v}_1-\vec{v}_2$ is a nonzero vector from the domain
          that $h$ maps to the zero vector of the codomain  
          ($\vec{v}_1-\vec{v}_2$
          does not equal the zero vector of the domain because $\vec{v}_1$
          does not equal $\vec{v}_2$).
        \partsitem The minimal polynomial 
          $m(t)$ sends every vector in the domain to 
          zero and so it is not one-to-one (except in a trivial space, which 
          we ignore).
          By the first item of this question, 
          since the composition $m(t)$ is not one-to-one, 
          at least one of the components $t-\lambda_i$ is not one-to-one.
          By the second item, $t-\lambda_i$ has a nontrivial null space.
          Because $(t-\lambda_i)(\vec{v})=\zero$ holds if and only if
          $t(\vec{v})=\lambda_i\cdot\vec{v}$, the prior sentence gives that
          $\lambda_i$ is an eigenvalue (recall that the definition of
          eigenvalue requires that the relationship hold for at least one
          nonzero $\vec{v}$).
      \end{exparts}
    \end{answer}
  \item 
    True or false:~for a transformation on an
    \( n \) dimensional space, if the minimal polynomial has degree \( n \) 
    then the map is diagonalizable.
    \begin{answer}
      This is false.
      The natural example of a non-diagonalizable transformation works here.
      Consider the transformation of $\C^2$ represented with respect to
      the standard basis by this matrix.
      \begin{equation*}
        N=
        \begin{mat}[r]
          0  &1  \\
          0  &0
        \end{mat}
      \end{equation*}
      The characteristic polynomial is $c(x)=x^2$. 
      Thus the minimal polynomial is either $m_1(x)=x$ or $m_2(x)=x^2$.
      The first is not right since $N-0\cdot I$ is not the zero matrix,
      thus in this example the minimal polynomial has degree equal to the 
      dimension of the underlying space, and, as mentioned,
      we know this matrix is not diagonalizable because it is nilpotent. 
    \end{answer}
   \item 
     Let $f(x)$ be a polynomial.
     Prove that if $A$ and $B$ are similar matrices then $f(A)$ is 
     similar to $f(B)$.
     \begin{exparts}
       \partsitem Now show that similar matrices have the same characteristic
         polynomial.
       \partsitem Show that similar matrices have the same minimal polynomial.
       \partsitem Decide if these are similar.
          \begin{equation*}
            \begin{mat}[r]
              1  &3  \\
              2  &3
            \end{mat}
            \qquad
            \begin{mat}[r]
              4  &-1 \\
              1  &1
            \end{mat}
          \end{equation*}
     \end{exparts}
     \begin{answer}
       Let \( A \) and \( B \) be similar \( A=PBP^{-1} \).
       From the facts that 
       \begin{multline*}
           A^n=(PBP^{-1})^n=(PBP^{-1})(PBP^{-1})\cdots(PBP^{-1})   \\
                           =PB(P^{-1}P)B(P^{-1}P)\cdots (P^{-1}P)BP^{-1}
                           =PB^nP^{-1}
       \end{multline*}
       and $c\cdot A=c\cdot(PBP^{-1})=P(c\cdot B)P^{-1}$ follows 
       the required fact that for any polynomial function $f$ we have 
       \( f(A)=P\,f(B)\,P^{-1} \).
       For instance, if $f(x)=x^2+2x+3$ then
       \begin{multline*}
         A^2+2A+3I=(PBP^{-1})^2+2\cdot PBP^{-1}+3\cdot I           \\
                  =(PBP^{-1})(PBP^{-1})+P(2B)P^{-1}+3\cdot PP^{-1}
                  =P(B^2+2B+3I)P^{-1}
       \end{multline*}
       shows that $f(A)$ is similar to $f(B)$.
       \begin{exparts}
         \partsitem Taking $f$ to be a linear polynomial we have that
           $A-xI$ is similar to $B-xI$.
           Similar matrices have equal determinants (since 
           $\deter{A}=\deter{PBP^{-1}}
               =\deter{P}\cdot\deter{B}\cdot\deter{P^{-1}}
               =1\cdot\deter{B}\cdot 1=\deter{B}$).
           Thus the characteristic polynomials are equal. 
         \partsitem 
           As \( P \) and \( P^{-1} \) are invertible, \( f(A) \) is the
           zero matrix when, and only when, \( f(B) \) is the zero matrix.
         \partsitem They cannot be similar since they don't have the same
           characteristic polynomial.
           The characteristic polynomial of the first one is 
           $x^2-4x-3$ while the characteristic polynomial of the 
           second is $x^2-5x+5$.
       \end{exparts}  
     \end{answer}
   \item 
    \begin{exparts}
     \partsitem Show that a matrix is invertible if and only if 
       the constant term
       in its minimal polynomial is not $0$.
     \partsitem Show that if a square matrix \( T \) is not invertible     
       then there is
       a nonzero matrix \( S \) such that \( ST \) and \( TS \) both equal the
       zero matrix.
     \end{exparts}
     \begin{answer}
      Suppose that \( m(x)=x^n+m_{n-1}x^{n-1}+\dots+m_1x+m_0 \)
      is minimal for \( T \).
      \begin{exparts}
       \partsitem 
         For the `if' argument, 
         because \( T^n+\dots+m_1T+m_0I \) is the zero matrix we
         have that \( I=(T^n+\dots+m_1T)/(-m_0)=
         T\cdot (T^{n-1}+\dots+m_1I)/(-m_0) \) and so 
         the matrix $(-1/m_0)\cdot (T^{n-1}+\dots+m_1I)$ is the inverse of $T$.
         For `only if', suppose that \( m_0=0 \) 
         (we put the \( n=1 \) case aside but it is easy) so that
         \( T^n+\dots+m_1T=(T^{n-1}+\dots+m_1I)T \) is the zero matrix.
         Note that \( T^{n-1}+\dots+m_1I \) is not the zero matrix because
         the degree of the minimal polynomial is \( n \).
         If \( T^{-1} \) exists then multiplying both
         \( (T^{n-1}+\dots+m_1I)T \) and the zero matrix from the right 
         by $T^{-1}$ gives a contradiction.
       \partsitem If \( T \) is not invertible then the constant term in its
         minimal polynomial is zero.
         Thus,
         \begin{equation*}
            T^n+\dots+m_1T=(T^{n-1}+\dots+m_1I)T=T(T^{n-1}+\dots+m_1I)
         \end{equation*}
         is the zero matrix.
       \end{exparts} 
     \end{answer}
   \recommended \item \label{exer:PolyMapsFactor}
      \begin{exparts}
        \partsitem Finish the proof of \nearbylemma{le:PolyMapsFactor}.
        \partsitem Give an example to show that the result does not hold
          if $t$ is not linear.
      \end{exparts}
      \begin{answer}
        \begin{exparts}
          \partsitem
            For the inductive step, assume that \nearbylemma{le:PolyMapsFactor}
            is true for polynomials
            of degree \( i,\ldots,k-1 \) and consider a polynomial \( f(x) \)
            of degree \( k \). 
            Factor $f(x)=k(x-\lambda_1)^{q_1}\cdots(x-\lambda_z)^{q_z}$
            and let
            \( k(x-\lambda_1)^{q_1-1}\cdots(x-\lambda_z)^{q_z} \)
            be \( c_{n-1}x^{n-1}+\cdots+c_1x+c_0 \).
            Substitute:
            \begin{align*}
               k\composed{\composed{(t-\lambda_1)^{q_1}}{\cdots}}{
                                            (t-\lambda_z)^{q_z}}(\vec{v})
               &=
               \composed{(t-\lambda_1)}{
                  \composed{\composed{(t-\lambda_1)^{q_1}}{\cdots}}{
                  (t-\lambda_z)^{q_z}} }
                          (\vec{v})    \\
               &=
               (t-\lambda_1)\,(c_{n-1}t^{n-1}(\vec{v})+\cdots+c_0\vec{v}) \\
               &=
               f(t)(\vec{v})
            \end{align*}
           (the second equality follows from the inductive hypothesis and 
           the third from the linearity of \( t \)).
         \partsitem One example is to consider the squaring map
            $\map{s}{\Re}{\Re}$ given by $s(x)=x^2$.
            It is nonlinear.
            The action defined by the polynomial $f(t)=t^2-1$ 
            changes $s$ to $f(s)=s^2-1$, which is this map.
            \begin{equation*} 
              x\mapsunder{s^2-1} \composed{s}{s}(x)-1=x^4-1
            \end{equation*}
            Observe that this map differs from the map
            $\composed{(s-1)}{(s+1)}$; for instance, the first map takes
            $x=5$ to $624$ while the second one takes $x=5$ to $675$.
       \end{exparts} 
     \end{answer}
%  \item
%    Give an example of two \( \nbyn{4} \) matrices that have the 
%    same characteristic polynomial and the same minimal polynomial but
%    nonetheless are not similar.
%    \begin{answer}
%      These two have the same characteristic polynomial,
%      $c_S(x)=c_T(x)=(x-2)^4$.
%      \begin{equation*}
%        S=
%        \begin{mat}[r]
%          2  &0  &0  &0  \\
%          1  &2  &0  &0  \\
%          0  &0  &2  &0  \\
%          0  &0  &0  &2
%        \end{mat}
%        \qquad
%        T=
%        \begin{mat}[r]
%          2  &0  &0  &0  \\
%          1  &2  &0  &0  \\
%          0  &0  &2  &0  \\
%          0  &0  &1  &2
%        \end{mat}
%      \end{equation*}
%      For each of the two
%      the candidates for the minimal polynomial are 
%      $m_1(x)=(x-2)$, $m_2(x)=(x-2)^2$, $m_3(x)=(x-2)^3$, and 
%      $m_4(x)=(x-2)^4$.
%      We have 
%      \begin{equation*}
%        S-2I=
%        \begin{mat}[r]
%          0  &0  &0  &0  \\
%          1  &0  &0  &0  \\
%          0  &0  &0  &0  \\
%          0  &0  &0  &0
%        \end{mat}
%        \qquad
%        T-2I=
%        \begin{mat}[r]
%          0  &0  &0  &0  \\
%          1  &0  &0  &0  \\
%          0  &0  &0  &0  \\
%          0  &0  &1  &0
%        \end{mat}
%      \end{equation*}
%      and
%      \begin{equation*}
%        (S-2I)^2=
%        \begin{mat}[r]
%          0  &0  &0  &0  \\
%          0  &0  &0  &0  \\
%          0  &0  &0  &0  \\
%          0  &0  &0  &0
%        \end{mat}
%        =
%        (T-2I)^2
%      \end{equation*}
%      and so the minimal polynomial for each is $m_2$.
%    \end{answer}
  \item
    Any transformation or square matrix has a minimal polynomial.
    Does the converse hold?
    \begin{answer}
      Yes.
      Expand down the last column to check that
      \( x^n+m_{n-1}x^{n-1}+\dots+m_1x+m_0 \) is plus or minus the
      determinant of this.
      \begin{equation*}
         \begin{mat}
            -x  &0  &0  &      &    &m_0    \\
             0  &1-x&0  &      &    &m_1    \\
             0  &0  &1-x&      &    &m_2    \\
                &   &   &\ddots             \\
                &   &   &      &1-x &m_{n-1}
         \end{mat}
      \end{equation*} 
     \end{answer}
\end{exercises}












\subsectionoptional{Jordan Canonical Form}
We are looking for a canonical form for matrix similarity.
This subsection completes this program by moving
from the canonical form for the classes 
of nilpotent matrices to
the canonical form for all classes.

\begin{lemma} \label{le:NilIffOnlyEigenZero}
A linear transformation on a nontrivial vector space 
is nilpotent if and only if its only eigenvalue is zero.
\end{lemma}

\begin{proof}
Let the linear transformation be $\map{t}{V}{V}$.
If $t$ is nilpotent then there is an $n$ such that $t^n$ is the zero map,
so $t$ satisfies the polynomial $p(x)=x^n=(x-0)^n$.
By \nearbylemma{le:tSatisImpMinPolyDivides} the minimal polynomial of 
$t$ divides $p$, so the minimal polynomial has only zero for a root.
By Cayley-Hamilton, \nearbytheorem{th:CayHam},
the characteristic polynomial has only zero for a root.
Thus the only eigenvalue of $t$ is zero.

Conversely, if a transformation \( t \) on an
\( n \)-dimensional space has only the single eigenvalue of zero 
then its characteristic polynomial is \( x^n \). 
\nearbylemma{le:MatSatItsCharPoly} says that a map satisfies its
characteristic polynomial so \( t^n \) is the zero map.
Thus $t$ is nilpotent.
\end{proof}

\noindent The `nontrivial vector space' is in the statement of 
that lemma because on a trivial
space $\set{\zero}$ the only transformation is the zero map, which has
no eigenvalues because there are no associated nonzero eigenvectors.

\begin{corollary} \label{cor:tMinLambdaNilpotent}
The transformation $t-\lambda$ is nilpotent if and only if 
$t$'s only eigenvalue is $\lambda$.   
\end{corollary}

\begin{proof}
The transformation \( t-\lambda \) is nilpotent if and only if
$t-\lambda$'s only eigenvalue is \( 0 \).
That holds if and only if $t$'s only eigenvalue is $\lambda$, because
\( t(\vec{v})=\lambda\vec{v} \) if and 
only if \( (t-\lambda)\,(\vec{v})=0\cdot\vec{v} \).
\end{proof}

\begin{lemma}   \label{le:SimRespAddScalar}
If the matrices \( T-\lambda I \) and \( N \) are similar 
then \( T \) and \( N+\lambda I \) are also similar,
via the same change of basis matrices.
\end{lemma}

\begin{proof}
With \( N=P(T-\lambda I)P^{-1}=PTP^{-1}-P(\lambda I)P^{-1} \)
we have $N=PTP^{-1}-PP^{-1}(\lambda I)$
since the diagonal matrix \( \lambda I \) commutes with anything, 
and so \( N=PTP^{-1}-\lambda I \).
Therefore \( N+\lambda I=PTP^{-1} \).
\end{proof}

We already have the canonical form for 
the case of nilpotent matrices, 
that is, the case of matrices whose only eigenvalue is zero.
By Corollary~III.\ref{cor:NilpotentMatCanonForm}, each 
such matrix is similar to one that is all
zeroes except for blocks of subdiagonal ones.
% (To make this representation unique we can fix some arrangement of
% the blocks, perhaps from longest to shortest.)
The prior results let us extend this to all
matrices that have a single eigenvalue~$\lambda$.
In the new form,
besides blocks of subdiagonal ones, all of the entries on the diagonal 
are~$\lambda$.
(See \nearbydefinition{def:JordanBlock} below.)

\begin{example}   \label{ex:SingJordBlock}
The characteristic polynomial of
\begin{equation*}
  T=\begin{mat}[r]
      2  &-1  \\
      1  &4
    \end{mat}
\end{equation*}
is \( (x-3)^2 \) and so \( T \) has a single eigenvalue, \( 3 \).
% We can find a matrix similar to $T$ that is simple in 
% the way that our canonical form for nilpotent matrices is simple.
% The form is described in \nearbydefinition{def:JordanBlock} below but
% as a preview here the new matrix.
% \begin{equation*}
%   \begin{mat}[r]
%       3  &0  \\
%       1  &3
%     \end{mat}
% \end{equation*}

Because $3$ is the only eigenvalue, 
$T-3I$ has the single eigenvalue of \( 0 \) and is therefore nilpotent.
To get the canonical form of~$T$, start by
finding a string basis for $T-3I$
as in the prior section, computing the powers of this matrix 
and the null spaces of those powers.
\begin{center}
  \begin{tabular}{c|cc}
    \multicolumn{1}{c}{\textit{power} \( p \)}  &\( (T-3I)^p \)  &\( \nullspace{(T-3I)^p}  \)   \\  
    \hline
    \( 1 \)
    &\(  
       \matrixvenlarge{\begin{mat}[r]
         -1  &-1 \\
         1   &1   \\
       \end{mat}}  \)
    &\( \set{\matrixvenlarge{\colvec{-y \\ y}}\suchthat
                               y\in\C}  \)   \\
    \( 2 \)
    &\(  
       \matrixvenlarge{\begin{mat}[r]
         0  &0  \\
         0  &0   \\
       \end{mat}}  \)
    &\( \set{\matrixvenlarge{\colvec{x \\ y}}\suchthat
                               x,y\in\C}  \)   \\
  \end{tabular}
\end{center}
The null space of $t-3$ has dimension one
and the null space of $(t-3)^2$ has dimension two.
So this is the canonical representation matrix and the form of the
associated string basis~$B$ for $t-3$.
\begin{equation*}
  \rep{t-3}{B,B}
  =N=
  \begin{mat}[r]
      0  &0   \\
      1  &0
  \end{mat}
  \qquad
  \begin{array}{r}
    \vec{\beta}_1\xmapsto{t-3}\vec{\beta}_2\xmapsto{t-3}\zero  \\
  \end{array}  
\end{equation*}
Find example elements for such a basis
by taking a $\vec{\beta}_2$ that is mapped to~$\zero$
by $t-3$ (that is, pick an element of 
the null space $\nullspace{t-3}$ given in the table).
Then choose a $\vec{\beta}_1$ that is mapped to~$\zero$ by $(t-3)^2$.
The only restriction on the choice, besides membership in the null space,
is that the two vectors need to form a linearly independent set.
Here is a choice of two such vectors.
\begin{equation*}
  \vec{\beta}_1=\colvec[r]{1 \\ 1}
  \quad
  \vec{\beta}_2=\colvec[r]{-2 \\ 2}
\end{equation*}
With that, \nearbylemma{le:SimRespAddScalar} says that \( T \) is similar to
this matrix.
\begin{equation*}
  \rep{t}{B,B}=
  N+3I=
  \begin{mat}[r]
     3  &0  \\
     1  &3
  \end{mat}
\end{equation*}

We can exhibit the similarity computation.
Take 
$T$ to represent a transformation $\map{t}{\C^2}{\C^2}$ with respect to
the standard basis (as we shall do for the rest of the chapter). 
% Recall how to find the change of
% basis matrices $P$ and $P^{-1}$ to express \( N \) as \( P(T-3I)P^{-1} \).
The similarity diagram
\begin{equation*}
  \begin{CD}
    \C^2_{\wrt{\stdbasis_2}}      @>t-3>T-3I>      \C^2_{\wrt{\stdbasis_2}}     \\
    @V\scriptstyle\identity V\scriptstyle PV  
                                 @V\scriptstyle\identity V\scriptstyle PV \\
    \C^2_{\wrt{B}}                 @>t-3>N>         \C^2_{\wrt{B}}
  \end{CD}
\end{equation*}
describes that to move from the lower left to the upper left we multiply by
\begin{equation*}
  P^{-1}=\bigl(\rep{\identity}{\stdbasis_2,B}\bigr)^{-1}
    =\rep{\identity}{B,\stdbasis_2}
    =\begin{mat}[r]
        1  &-2  \\
        1  &2
     \end{mat}
\end{equation*}
and to move from the upper right to the lower right we multiply by
this matrix.
\begin{equation*}
  P=\begin{mat}[r]
      1  &-2  \\
      1  &2
     \end{mat}^{-1}\!\!
   =\begin{mat}[r]
      1/2  &1/2  \\
      -1/4 &1/4
   \end{mat}
\end{equation*}
So this equation shows the similarity.
\begin{equation*}
  \begin{mat}[r]
      1/2  &1/2  \\
      -1/4 &1/4
   \end{mat}
   \begin{mat}[r]
      2  &-1  \\
      1  &4
    \end{mat}
    \begin{mat}[r]
        1  &-2  \\
        1  &2
     \end{mat}
  =
  \begin{mat}[r]
     3  &0  \\
     1  &3
  \end{mat}
\end{equation*}
\end{example}

\begin{example}  \label{ex:FullNilpotentCase}
This matrix has characteristic polynomial \( (x-4)^4 \) 
\begin{equation*}
  T=
  \begin{mat}[r]
    4  &1  &0  &-1  \\
    0  &3  &0  &1   \\
    0  &0  &4  &0   \\
    1  &0  &0  &5
   \end{mat}
\end{equation*}
and so has the single eigenvalue $4$.
\begin{center}
  \begin{tabular}{c|cc}
    \multicolumn{1}{c}{\textit{power} \( p \)}  &\( (T-4I)^p \)  &\( \nullspace{(T-4I)^p}  \)   \\  
    \hline
    \( 1 \)
    &\(  
       \matrixvenlarge{\begin{mat}[r]
         0  &1  &0  &-1  \\
         0  &-1 &0  &1   \\
         0  &0  &0  &0   \\
         1  &0  &0  &1
       \end{mat}}  \)
    &\( \set{\matrixvenlarge{\colvec{-w \\ w \\ z \\ w}}\suchthat
                               z,w\in\C}  \)   \\
    \( 2 \)
    &\(  
       \matrixvenlarge{\begin{mat}[r]
         -1  &-1 &0  &0  \\
          1  &1  &0  &0   \\
          0  &0  &0  &0   \\
          1  &1  &0  &0
       \end{mat}}  \)
    &\( \set{\matrixvenlarge{\colvec{-y \\ y \\ z \\ w}}\suchthat
                               y,z,w\in\C}  \)   \\
    \( 3 \)
    &\(  
       \matrixvenlarge{\begin{mat}[r]
          0  &0   &0  &0  \\
          0  &0  &0  &0   \\
          0  &0  &0  &0   \\
          0  &0  &0  &0   \\
       \end{mat}}  \)
    &\( \set{\matrixvenlarge{\colvec{x \\ y \\ z \\ w}}\suchthat
                               x,y,z,w\in\C}  \)   \\
  \end{tabular}
% sage: T=matrix(QQ, [[4,1,0,-1], [0,3,0,1], [0,0,4,0], [1,0,0,5]])
% sage: S=T-4*identity_matrix(4)
% sage: S^2
% [-1 -1  0  0]
% [ 1  1  0  0]
% [ 0  0  0  0]
% [ 1  1  0  0]
% sage: S^3
% [0 0 0 0]
% [0 0 0 0]
% [0 0 0 0]
% [0 0 0 0]
% sage: S
% [ 0  1  0 -1]
% [ 0 -1  0  1]
% [ 0  0  0  0]
% [ 1  0  0  1]
% sage: S*vector(QQ, [-1,1, 0, 1])
% (0, 0, 0, 0)
\end{center}
The null space of $t-4$ has dimension two, the null space of $(t-4)^2$
has dimension three, and the null space of $(t-4)^3$ has dimension four.
This gives the canonical form for $t-4$.
\begin{equation*}
  N=\rep{t-4}{B,B}=
  \begin{mat}[r]
    0  &0  &0  &0   \\
    1  &0  &0  &0   \\
    0  &1  &0  &0   \\
    0  &0  &0  &0
   \end{mat}
   \qquad
  \begin{array}{r}
    \vec{\beta}_{1}\xmapsto{t-4}\vec{\beta}_{2}
       \xmapsto{t-4}\vec{\beta}_{3}\xmapsto{t-4}\zero \\
   \vec{\beta}_{4}\xmapsto{t-4}\zero \\
  \end{array}  
\end{equation*}
For the four basis elements, choose a
$\vec{\beta}_3$ and~$\vec{\beta}_4$ that are mapped to~$\zero$
by~$t-4$.
Also choose a~$\vec{\beta}_2$ that is mapped to~$\zero$ by~$(t-4)^2$ 
and a $\vec{\beta}_1$ is mapped to~$\zero$ by~$(t-4)^3$.
\begin{equation*}
  \vec{\beta}_1=\colvec{1 \\ 0 \\ 0 \\ 0}
  \quad
  \vec{\beta}_2=\colvec{-1 \\ 1 \\ 0 \\ 0}
  \quad
  \vec{\beta}_3=\colvec{-1 \\ 1 \\ 0 \\ 1}
  \quad
  \vec{\beta}_4=\colvec{0 \\ 0 \\ 1 \\ 0}
\end{equation*}
Of course, we choose these four to not have any linear dependences.
That gives the canonical form matrix similar to~$T$.
\begin{equation*}
  \rep{t}{B,B}
  =N+4I=
  \begin{mat}[r]
    4  &0  &0  &0   \\
    1  &4  &0  &0   \\
    0  &1  &4  &0   \\
    0  &0  &0  &4
   \end{mat}
\end{equation*}
\end{example}

\begin{definition}  \label{def:JordanBlock}
A \definend{Jordan block}\index{Jordan block}
is a square matrix, or a square block of entries inside of a matrix,
that is all zeroes except that 
there is a number $\lambda\in\C$ such that 
every diagonal entry is $\lambda$,
and that every the subdiagonal entries is~$1$.
(If the block is $\nbyn{1}$ then it has no subdiagonal~$1$'s.)
\end{definition}

The strings in the associated basis are 
\definend{Jordan strings}\index{Jordan string}\index{basis!Jordan string} 
or \definend{Jordan chains}.\index{Jordan chain}\index{basis!Jordan chain}

The above examples illustrate that Jordan block matrices are
canonical representatives of the similarity classes of single-eigenvalue
matrices.
When we construct canonical representatives of single-eigenvalue matrices,
to make them unique we arrange that the blocks of subdiagonal ones go from
longest to shortest (reading left to right).

\begin{example}
The \( \nbyn{3} \) matrices whose only eigenvalue is \( 1/2 \) separate into
three similarity classes.
The three classes have these canonical representatives.
\begin{equation*}
  \begin{mat}[r]
     1/2  &0    &0  \\
     0    &1/2  &0  \\
     0    &0    &1/2
   \end{mat}
   \qquad 
   \begin{mat}[r]
     1/2  &0    &0  \\
     1    &1/2  &0  \\
     0    &0    &1/2
   \end{mat}
   \qquad 
   \begin{mat}[r]
     1/2  &0    &0  \\
     1    &1/2  &0  \\
     0    &1    &1/2
   \end{mat}
\end{equation*}
In particular, this matrix
\begin{equation*}
   \begin{mat}[r]
     1/2  &0    &0    \\
     0    &1/2  &0    \\
     0    &1    &1/2
   \end{mat}
\end{equation*}
belongs to the similarity class represented by the middle one, because of 
the convention of ordering the blocks of subdiagonal ones.
\end{example}

We are now set up to finish the program of this chapter.
First we review of what we so far know about 
transformations $\map{t}{\C^n}{\C^n}$.

The diagonalizable case is where
$t$ has~$n$ distinct eigenvalues 
$\lambda_1$, \ldots $\lambda_n$, that is, where the number of eigenvalues
equals the dimension of the space. 
In this case there is a basis
$\sequence{\vec{\beta}_1,\ldots,\vec{\beta}_n}$ where 
$\vec{\beta}_i$ is an eigenvector associated with the eigenvalue~$\lambda_i$.
The example below has 
$n=3$ and the three eigenvalues are $\lambda_1=1$, $\lambda_2=3$,
and $\lambda_3=-1$.
It shows the canonical representative matrix and the associated basis.
\begin{equation*}
  T_1=\begin{mat}
    1  &0  &0  \\
    0  &3  &0  \\
    0  &0  &-1
  \end{mat}
  \qquad
    \begin{array}{r}
    \vec{\beta}_1\mapsunder{t-1}\zero  \\
    \vec{\beta}_2\mapsunder{t-3}\zero  \\
    \vec{\beta}_3\mapsunder{t+1}\zero
    \end{array}  
\end{equation*}
We saw a diagonalization example in 
Five.II.\ref{ex:SummaryOfDiagonalizable}.

The case where $t$ has a single eigenvalue leverages 
the results on nilpotency to get a basis of
associated eigenvectors that form disjoint strings.
This example has $n=10$ and the single eigenvalue~$\lambda=2$.
\begin{equation*}
  T_2=\begin{mat}
    2  &0  &0  &0 &0 &0 &0 &0 &0 &0 \\
    1  &2  &0  &0 &0 &0 &0 &0 &0 &0 \\
    0  &1  &2  &0 &0 &0 &0 &0 &0 &0 \\
    0  &0  &0  &2 &0 &0 &0 &0 &0 &0 \\
    0  &0  &0  &1 &2 &0 &0 &0 &0 &0 \\
    0  &0  &0  &0 &1 &2 &0 &0 &0 &0 \\
    0  &0  &0  &0 &0 &0 &2 &0 &0 &0 \\
    0  &0  &0  &0 &0 &0 &1 &2 &0 &0 \\
    0  &0  &0  &0 &0 &0 &0 &0 &2 &0 \\
    0  &0  &0  &0 &0 &0 &0 &0 &0 &2 \\
  \end{mat}
  \qquad
  {\renewcommand{\arraystretch}{1.75}
    \begin{array}{r}
    \vec{\beta}_1\mapsunder{t-2}\vec{\beta}_2\mapsunder{t-2}\vec{\beta}_3\mapsunder{t-2}\zero  \\
    \vec{\beta}_4\mapsunder{t-2}\vec{\beta}_5\mapsunder{t-2}\vec{\beta}_6\mapsunder{t-2}\zero  \\
    \vec{\beta}_7\mapsunder{t-2}\vec{\beta}_8\mapsunder{t-2}\zero  \\
    \vec{\beta}_9\mapsunder{t-2}\zero  \\
    \vec{\beta}_{10}\mapsunder{t-2}\zero
    \end{array}}  
\end{equation*}
We saw a full example as 
\nearbyexample{ex:FullNilpotentCase}.

\nearbytheorem{th:JordanForm} extends to arbitrary
$\map{t}{\C^n}{\C^n}$.
The canonical form consists of
Jordan blocks containing the eigenvalues. 
This illustrates such a matrix for $n=6$ with 
eigenvalues $3$, $2$, and~$-1$
(examples with full compuations are after the theorem). 
\begin{equation*}
  T_3=\begin{mat}
    3  &0  &0  &0 &0 &0 \\
    1  &3  &0  &0 &0 &0  \\
    0  &0  &3  &0 &0 &0  \\
    0  &0  &0  &2 &0 &0  \\
    0  &0  &0  &1 &2 &0  \\
    0  &0  &0  &0 &0 &-1  \\
  \end{mat}
  \qquad
  {\renewcommand{\arraystretch}{1.5}
    \begin{array}{r}
    \vec{\beta}_1\mapsunder{t-3}\vec{\beta}_2\mapsunder{t-3}\zero  \\
    \vec{\beta}_3\mapsunder{t-3}\zero  \\
    \vec{\beta}_4\mapsunder{t-2}\vec{\beta}_5\mapsunder{t-2}\zero  \\
    \vec{\beta}_6\mapsunder{t+1}\zero  \\
    \end{array}}  
\end{equation*}
It has four blocks, two of them
associated with the eigenvalue~$3$.

As in the nilpotent cases earlier in this section,
we will determine the structure of the basis 
associated with~$T_3$ by applying
the maps $t-3$ and~$(t-3)^2$, as well as $t-2$ and~$(t-2)^2$, and 
also~$t+1$.
For instance,
the basis has three elements that are all mapped to~$\zero$
if we apply $t-3$ twice.
It has two elements
that are sent to~$\zero$ on two applications of~$t-2$.
And it has an element that is sent to~$\zero$ by
one application of~$t+1$.
That is, we will find the elements of this basis in  
the generalized null spaces $\gennullspace{t-3}$, $\gennullspace{t-2}$,
and~$\gennullspace{t+1}$.

The idea of the theorem's proof is that applying the
map $t-3$ will send two of the basis vectors, 
$\vec{\beta}_2$ and~$\vec{\beta}_3$, to~$\zero$.
That is, these two are eigenvectors associated with the 
eigenvalue~$\lambda_1=3$.
This suggests using induction,
since the dimension of the range space~$\rangespace{t-3}$
is less than that of the starting space.

Before the theorem, we first will show that applying $t-3$ 
won't do something awkward to the basis elements not in 
$\gennullspace{t-3}$, the vectors~$\vec{\beta}_4$, 
$\vec{\beta}_5$, and~$\vec{\beta}_6$. 
Here, awkward means that
where \( \map{t}{V}{V} \) is a linear transformation,
the restriction\appendrefs{restrictions of functions}\spacefactor=1000  %
of \( t \) to a subspace \( M\subseteq V \) 
need not be a linear transformation on \( M \):~there
may be an \( \vec{m}\in M \)
with \( t(\vec{m})\not\in M \). 
An example is where $t$ is the transformation 
that rotates the plane by
a quarter turn and $M$ is the $x=y$ line; the vector $\binom{1}{1}$
is rotated away from the subspace.
We next show this does not happen in the special case of 
eigenvalues;~if $j\neq i$ then 
application of \( t-\lambda_j \) leaves 
\( t-\lambda_i \)'s part unchanged.

\begin{definition} \label{def:invariant}
Let \( \map{t}{V}{V} \) be a transformation.
A subspace \( M\subseteq V \) is \definend{$t$ invariant}%
\index{invariant subspace}\index{subspace!invariant}
if whenever \( \vec{m}\in M \) then \( t(\vec{m})\in M \)
(shorter: \( t(M)\subseteq M \)).
\end{definition}
 
\begin{theorem}  \label{th:JordanForm}
\index{similar!canonical form}%
\index{canonical form!for similarity}\index{transformation!Jordan form for}
Any transformation $\map{t}{\C^n}{\C^n}$ can be represented in 
\definend{Jordan form},\index{Jordan form!represents similarity classes}\index{Jordan form!definition}
where each $J_{\lambda}$ is a Jordan block.\index{Jordan block}
\begin{equation*}
  \begin{mat}
    J_{\lambda_1}  &            &\text{\textit{--zeroes--}}                 \\
               &J_{\lambda_2}                                              \\
               &     &\ddots                                     \\
                &     &                           &J_{\lambda_{k-1}} &     \\
               &     &\text{\textit{--zeroes--}} &            &J_{\lambda_{k}}
  \end{mat}
\end{equation*}
Restated, any $\nbyn{n}$ matrix is similar to one in this form. 
\end{theorem}
 
\begin{proof}
This proof is closely related to that of
Five.III.\ref{th:NilMapHasStrBas}, that any nilpotent transformation
has a basis of disjoint $t$-strings.

We will do induction on~$n$.
The $n=1$ base case is trivial since any $\nbyn{1}$ matrix is in Jordan form.
For the inductive step assume that 
every transformation of $\C^1$, \ldots, $\C^{n-1}$ has a Jordan form
representation,
and fix $\map{t}{\C^n}{\C^n}$.

This transformation has at least one eigenvalue~$\lambda_1$ because
every transformation has a minimal polynomial and every minimal polynomial
has at least one root over the complex numbers.
Since $\lambda_1$ is an eigenvalue, there is an associated 
eigenvector~$\vec{v}\neq\zero$ where $(t-\lambda_1)(\vec{v})=\zero$
and so the null space of $t-\lambda_1$ is nontrivial.
Thus the dimension of $\nullspace{t-\lambda_1}$, the nullity,
is greater than zero.
Since the rank plus the nullity equals the dimension of the domain,
the rank
is strictly less than~$n$.
Write $W$ for $\rangespace{t-\lambda_1}$ and write
$r$ for the rank, the dimension of $W$.

The space $W$ is $t-\lambda_1$~invariant because if 
$\vec{w}\in W=\rangespace{t-\lambda_1}$ then $\vec{w}\in\C^n$ and so
$(t-\lambda_1)(\vec{w})\in\rangespace{t-\lambda_1}$.
Thus the restriction of $t-\lambda_1$ to~$w$ induces a transformation of~$W$, 
which we shall also call $t-\lambda_1$.
By the prior paragraph the dimension of~$W$ is less than~$n$ 
so the inductive hypothesis applies and
we get a basis for~$W$ consisting of disjoint strings.

These strings are associated with the eigenvalues of $t-\lambda_1$ 
on~$W$.
For any $\lambda$ that is an eigenvalue of~$t-\lambda_1$,
the transformation of the basis elements will be 
$(t-\lambda_1)-\lambda=t-(\lambda_1-\lambda)$,
which we will write as $t-\lambda_s$.
The following diagram covers the case of strings for $t-\lambda_1$, that is
where $\lambda=0$;
the argument remains valid if there are no such strings.
\begin{equation*}
\begin{strings}{*{10}{l}}
  \vec{w}_{1,n_1} &\xmapsto{t-\lambda_1} &\cdots 
     &\xmapsto{t-\lambda_1} &\vec{w}_{1,1} &\xmapsto{t-\lambda_1} &\zero \\
     \vdotswithin{\xmapsto{t-\lambda_1}}                   \\
  \vec{w}_{q,n_q} &\xmapsto{t-\lambda_1} &\cdots 
     &\xmapsto{t-\lambda_1} &\vec{w}_{q,1} &\xmapsto{t-\lambda_1} &\zero \\
  \vec{w}_{q+1,n_{q+1}} &\xmapsto{t-\lambda_2} &\cdots 
     &\xmapsto{t-\lambda_2} &\vec{w}_{q+1,1} &\xmapsto{t-\lambda_2} &\zero \\
     \vdotswithin{\xmapsto{t-\lambda_1}}                              \\
  \vec{w}_{k,n_k} &\xmapsto{t-\lambda_i} &\cdots 
     &\xmapsto{t-\lambda_i} &\vec{w}_{k,1} &\xmapsto{t-\lambda_i} &\zero \\
\end{strings}
\end{equation*}
(These strings may have differing lengths; 
for instance, perhaps $n_1\neq n_k$.)

The rightmost non-$\zero$ vector in each string 
is in the null space of the associated 
map:~$(t-\lambda_1)(\vec{w}_{1,1})=\zero$, \ldots, 
$(t-\lambda_i)(\vec{w}_{k,1})=\zero$.
Thus the number of strings associated with $t-\lambda_1$,
denoted $q$ in the above diagram, is
equal to the dimension of $W\cap\nullspace{t-\lambda_1}$.

The string basis above is for $W=\rangespace{t-\lambda_1}$.
We now expand it to make it a basis for all of~$\C_n$.
First, because 
each of the $q$-many vectors $\vec{w}_{1,n_1}$, or \ldots,~$\vec{w}_{q,n_q}$
is an element of~$\rangespace{t-\lambda_1}$, each is the image of 
a vector~$\vec{x}_1$, or \ldots, $\vec{x}_{q}$ from~$\C^n$.
Add these as prefixes to the strings.
Second, the dimension of $W\cap\nullspace{t-\lambda_1}$ is~$q$,
and the dimension of~$W$ is~$r$, 
so the bookkeeping requires that there be a $r-q$-dimensional subspace~$Y$
of $\nullspace{t-\lambda_1}$ whose intersection with $W$ is~$\set{\zero}$.
Consequently, 
add $r-q$-many linearly independent
vectors $\vec{y}_1$, \ldots, $\vec{y}_{r-q}$, all
from $\nullspace{t-\lambda_1}$.
\begin{equation*}
\begin{strings}{*{10}{c}}
  \vec{x}_1  &\xmapsto{t-\lambda_1} 
     &\vec{w}_{1,n_1} &\xmapsto{t-\lambda_1} &\cdots 
     &\xmapsto{t-\lambda_1} &\vec{w}_{1,1} &\xmapsto{t-\lambda_1} &\zero \\
     \vdotswithin{\xmapsto{t-\lambda_1}}                   \\
  \vec{x}_q  &\xmapsto{t-\lambda_1}  
     &\vec{w}_{q,n_q} &\xmapsto{t-\lambda_1} &\cdots 
     &\xmapsto{t-\lambda_1} &\vec{w}_{q,1} &\xmapsto{t-\lambda_1} &\zero \\
  \vec{w}_{q+1,n_{q+1}} &\xmapsto{t-\lambda_2} &\cdots 
     &\xmapsto{t-\lambda_2} &\vec{w}_{q+1,1} &\xmapsto{t-\lambda_2} &\zero \\
     \vdotswithin{\xmapsto{t-\lambda_1}}                              \\
  \vec{w}_{k,n_k} &\xmapsto{t-\lambda_i} &\cdots 
     &\xmapsto{t-\lambda_i} &\vec{w}_{k,1} &\xmapsto{t-\lambda_i} &\zero \\
     \vec{y}_1  &\xmapsto{t-\lambda_1} &\zero \\
     \vdotswithin{\xmapsto{t-\lambda_1}}                              \\
     \vec{y}_{r-q}  &\xmapsto{t-\lambda_1} &\zero \\
\end{strings}
\end{equation*}
We will show that this is the desired basis for~$\C^n$.

Because the number of vectors in the set
is equal to the dimension of the space, 
we will be done if we verify that its elements are linearly independent.
Assume that this is equal to~$\zero$
\begin{equation*}
  a_1\vec{x}_1+\cdots+a_q\vec{x}_q   
    +b_{1,n_1}\vec{w}_{1,n_1}+\cdots+b_{k,1}\vec{w}_{k,1} 
    +c_1\vec{y}_1+\cdots+c_{r-q}\vec{y}_{r-q}
  \tag{$*$}
\end{equation*}
We first show that this assumption implies that
all of the $a$'s are zero.
Apply $t-\lambda_1$ to~($*$).
The vectors $\vec{y}_1$, \ldots, $\vec{y}_{r-q}$ 
go to~$\zero$.
Each of the vectors $\vec{x}_i$ is sent to 
$\vec{w}_{i,n_i}$.
As to the $\vec{w}$'s, they have the property that
$(t-\lambda_k)\vec{w}_{i,j}=\vec{w}_{i,j-1}$ 
for some eigenvalue~$\lambda_k$.
Write $\hat{\lambda}$ for $\lambda_1-\lambda_k$ to get this.
\begin{equation*}
  (t-\lambda_1)(\vec{w}_{i,j})
    =(t-(\lambda_1-\hat{\lambda})+\hat{\lambda})(\vec{w}_{i,j})
    =\vec{w}_{i,j-1}+\hat{\lambda}\cdot\vec{w}_{i,j} 
  \tag{$**$}
\end{equation*}
Thus, applying $t-\lambda_1$ to~($*$) ends with a 
linear combination of $\vec{w}$'s.
These vectors are linearly independent because
they form a basis for~$W$ and so in this linear combination 
the coefficients are~$0$.
We will be done showing that all the $a$'s are zero if
we verify that 
the coefficient of $\vec{w}_{i,n_i}$ is $a_i$ for $i=1,\ldots,q$.
But that's true because for $i=1,\ldots,q$, 
equation~($**$) has $\hat{\lambda}=0$
and so $\vec{w}_{i,n_i}$ is the image of $x_{i}$ alone.
Thus $a_1$, \ldots, $a_q$ are zero. 

What remains of~($*$) is this equation.
\begin{equation*}
  \zero
  =
    b_{1,n_1}\vec{w}_{1,n_1}+\cdots+b_{k,1}\vec{w}_{k,1}
    +c_1\vec{y}_1+\cdots+c_{r-q}\vec{y}_{r-q}
\end{equation*}
The $\vec{w}$'s are from~$W$ while the $\vec{y}$'s are from~$Y$, and the
two spaces have only a trivial intersection.
Consequently the $b$'s and $c$'s are all~$0$.
\end{proof}




% Recall that Lemma~III.\ref{le:RangeAndNullChains} shows that for any
% transformation~$t$ on an $n$~dimensional space
% the range spaces of iterates are stable 
% \begin{equation*}
%   \rangespace{t^n}=\rangespace{t^{n+1}}=\cdots=\genrangespace{t}
% \end{equation*} 
% as are the null spaces. 
% \begin{equation*}
%   \nullspace{t^n}=\nullspace{t^{n+1}}=\cdots=\gennullspace{t}
% \end{equation*} 
% Thus,
% the generalized null space $\gennullspace{t}$ and the generalized range space
% $\genrangespace{t}$ are $t$~invariant.
% % For the generalized null space, if $\vec{v}\in\gennullspace{t}$ then
% % $t^n(\vec{v})=\zero$ where $n$ is the dimension of the underlying space
% % and so $t(\vec{v})\in\gennullspace{t}$ because
% % $t^n(\,t(\vec{v})\,)$ is zero also.
% % For the generalized rangespace, if $\vec{v}\in\genrangespace{t}$ then
% % $\vec{v}=t^n(\vec{w})$ for some $\vec{w}$ and then 
% % $t(\vec{v})=t^{n+1}(\vec{w})=t^n(\,t(\vec{w})\,)$ 
% % shows that $t(\vec{v})$ is also a member of $\genrangespace{t}$.
% Also, 
% $\gennullspace{t-\lambda_i}$ and $\genrangespace{t-\lambda_i}$
% are $t-\lambda_i$~invariant.

% The action of the transformation $t-\lambda_i$ on $\gennullspace{t-\lambda_i}$
% is especially easy to understand.
% Observe that any transformation~$t$ is nilpotent on $\gennullspace{t}$, 
% because if $\vec{v}\in\gennullspace{t}$ then 
% by definition $t^n(\vec{v})=\zero$. 
% Thus $t-\lambda_i$ is nilpotent on 
% $\gennullspace{t-\lambda_i}$.

% % Thus, the generalized null space $\gennullspace{t-\lambda_i}$ is a part of
% % the space on which the action of $t-\lambda_i$ is easy to understand.

% We shall take three steps to prove this section's major result. 
% The next result is the first.
% % It establishes that if $j=neq i$ then \( t-\lambda_j \) leaves
% % \( t-\lambda_i \)'s part unchanged.

% \begin{lemma} \label{le:tInvIfftMinLambdaInv}
% A subspace is \( t \) invariant if and only if 
% it is \( t-\lambda \) invariant for all scalars \( \lambda \).
% In particular, 
% if \( \lambda_i \) is an eigenvalue of  a linear transformation
% \( t \) then for any other eigenvalue $\lambda_j$
% the spaces \( \gennullspace{t-\lambda_i} \) 
% and \( \genrangespace{t-\lambda_i} \)
% are \( t-\lambda_j \) invariant.
% \end{lemma}

% \begin{proof}
% For the first sentence we check the two implications separately.
% The `if' half is easy: if the subspace is $t-\lambda$ invariant for 
% all scalars $\lambda$ then using $\lambda=0$ shows that it is $t$ invariant.
% For `only if' suppose that the subspace is $t$ invariant,
% so that if $\vec{m}\in M$ then $t(\vec{m})\in M$, and let $\lambda$ be 
% a scalar.
% The subspace $M$ is closed under linear combinations and so if 
% $t(\vec{m})\in M$ then $t(\vec{m})-\lambda\vec{m}\in M$.
% Thus if $\vec{m}\in M$ then $(t-\lambda)\,(\vec{m})\in M$.

% The lemma's second sentence follows from its first.
% The
% two spaces are $t-\lambda_i$~invariant so they are \( t \)~invariant.
% Apply the first sentence again to
% conclude that they are also \( t-\lambda_j \) invariant.
% \end{proof}

% The second step of the three that we will take to prove this
% section's major result makes use of an additional property of 
% \( \gennullspace{t-\lambda_i} \) and
% \( \genrangespace{t-\lambda_i} \), that they are complementary.
% Recall that if a space is the direct sum of two others 
% \( V=\mathscr{N}\directsum \mathscr{R} \) 
% then any vector \( \vec{v} \) in the space breaks into
% two parts \( \vec{v}=\vec{n}+\vec{r} \) where \( \vec{n}\in \mathscr{N} \) and
% \( \vec{r}\in \mathscr{R} \), and recall also 
% that if \( B_{\mathscr{N}} \) and \( B_{\mathscr{R}} \) are bases for
% \( \mathscr{N} \) and \( \mathscr{R} \) then the concatenation
% \( \cat{B_{\mathscr{N}}}{B_{\mathscr{R}}} \) is linearly independent.
% The next result says that for any subspaces
% \( \mathscr{N} \) and \( \mathscr{R} \) that are complementary 
% as well as \( t \)~invariant,
% the action
% of \( t \) on \( \vec{v} \) breaks into the actions of
% \( t \) on \( \vec{n} \) and on \( \vec{r} \).

% \begin{lemma} \label{le:InvCompSubspSplitTrans}
% Let \( \map{t}{V}{V} \) be a transformation and let \( \mathscr{N} \) and 
% \( \mathscr{R} \) be
% \( t \) invariant complementary subspaces of \( V \).
% Then we can represent \( t \) by a matrix with
% blocks of square submatrices $T_1$ and $T_2$
% \begin{equation*} \renewcommand{\arraystretch}{1.2}
%   \begin{pmat}{c|c}
%       T_1   &Z_2  \\  \cline{1-2}
%       Z_1 &T_2
%    \end{pmat}
%    \begin{array}{@{}l}
%      \} \text{\ $\dim(\mathscr{N})$-many rows}  \\
%      \} \text{\ $\dim(\mathscr{R})$-many rows}
%    \end{array}
% \end{equation*}
% where \( Z_1 \) and \( Z_2 \) are blocks of zeroes.
% \end{lemma}

% \begin{proof}
% Since the two subspaces are complementary, the concatenation of a basis
% for \( \mathscr{N} \) with a basis for \( \mathscr{R} \) makes a basis
% \( B=\sequence{\vec{\nu}_1,\dots,\vec{\nu}_p,
%         \vec{\mu}_1,\ldots,\vec{\mu}_q}  \)
% for \( V \).
% We shall show that the matrix
% \begin{equation*}
%   \rep{t}{B,B}=
%   \begin{pmat}{c@{\hspace*{1em}}c@{\hspace*{1em}}c}
%      \vdots                   &        &\vdots     \\
%      \rep{t(\vec{\nu}_1)}{B}  &\cdots  &\rep{t(\vec{\mu}_q)}{B}  \\
%      \vdots                   &        &\vdots     \\
%   \end{pmat}
% \end{equation*}
% has the desired form.

% Any vector \( \vec{v}\in V \) is a member of \( \mathscr{N} \) 
% if and only if when it is represented with respect to \( B \)
% the final \( q \)
% coefficients are zero.
% As \( \mathscr{N} \) is \( t \)~invariant, each of the vectors
% \( \rep{t(\vec{\nu}_1)}{B} \),
% \ldots, \( \rep{t(\vec{\nu}_p)}{B} \) has this form.
% Hence the lower left of \( \rep{t}{B,B} \) is all zeroes.
% The argument for the upper right is similar.
% \end{proof}

% To see that we have decomposed \( t \) into its action on the parts, 
% let $B_{\mathscr{N}}=\sequence{\vec{\nu}_1,\dots,\vec{\nu}_p}$ and 
% $B_{\mathscr{R}}=\sequence{\vec{\mu}_1,\ldots,\vec{\mu}_q}$.
% The restrictions of \( t \) to the subspaces \( \mathscr{N} \) 
% and~\( \mathscr{R} \) 
% are represented
% with respect to the bases $B_{\mathscr{N}},B_{\mathscr{N}}$
% and $B_{\mathscr{R}},B_{\mathscr{R}}$
% by the matrices \( T_1 \) and \( T_2 \).
% So with subspaces that are invariant and complementary 
% we can split the problem of examining
% a linear transformation into two lower-dimensional subproblems.
% The next result illustrates this decomposition into blocks.

% \begin{lemma} \label{le:DetIsProdOfSubDets}
% If $T$ is a matrix with square submatrices $T_1$ and $T_2$
% \begin{equation*} \renewcommand{\arraystretch}{1.2}
%   T=
%   \begin{pmat}{c|c}
%       T_1   &Z_2  \\  \cline{1-2}
%       Z_1   &T_2
%    \end{pmat}
% \end{equation*}
% where the \( Z \)'s are blocks of zeroes,
% then \( \deter{T}=\deter{T_1}\cdot\deter{T_2} \).
% \end{lemma}

% \begin{proof}
% Suppose that \( T \) is \( \nbyn{n} \),
% that \( T_1 \) is \( \nbyn{p} \),
% and that \( T_2 \) is \( \nbyn{q} \).
% In the permutation formula for the determinant
% \begin{equation*}
%   \deter{T}=
%   \sum_{\text{\scriptsize permutations\ }\phi}
%           t_{1,\phi(1)}t_{2,\phi(2)}\cdots t_{n,\phi(n)}\sgn(\phi)
% \end{equation*}
% each term comes from a rearrangement of the column numbers
% \( 1,\dots,n \) into a new order \( \phi(1),\dots,\phi(n) \).
% The upper right block $Z_2$ is all zeroes, so if a
% \( \phi \) has at least one of \( p+1,\dots,n \) among its first
% \( p \) column numbers \( \phi(1),\dots,\phi(p) \) then the term arising
% from \( \phi \) does not contribute to the sum because it is zero,
% e.g., if \( \phi(1)=n \) then
% \( t_{1,\phi(1)}t_{2,\phi(2)}\dots t_{n,\phi(n)}
%    =0\cdot t_{2,\phi(2)}\dots t_{n,\phi(n)}=0 \).

% So the above formula reduces to a sum over all permutations with 
% two halves:~any contributing $\phi$ is the composition of a $\phi_1$ that
% rearranges only \( 1,\dots,p \) 
% and a $\phi_2$ that rearranges only \( p+1,\dots,p+q \).
% Now, the distributive law 
% and the fact that the signum of a composition is the product
% of the signums gives that this
% \begin{multline*}
%    \deter{T_1}\cdot\deter{T_2}=
%    \bigg(\sum_{\begin{subarray}{c}
%                 \text{\scriptsize perms\ }\phi_1 \\
%                 \text{\scriptsize of\ } 1,\dots,p
%                \end{subarray}}
%        \!\!\! t_{1,\phi_1(1)}\cdots t_{p,\phi_1(p)}\sgn(\phi_1) \bigg)  \\
%    \cdot
%    \bigg(\sum_{\begin{subarray}{c}
%                 \text{\scriptsize perms\ }\phi_2 \\
%                 \text{\scriptsize of\ } p+1,\dots,p+q
%                \end{subarray}}
%        \!\!\! t_{p+1,\phi_2(p+1)}\cdots t_{p+q,\phi_2(p+q)}\sgn(\phi_2) 
%         \bigg)
% \end{multline*}
% equals
%   $\deter{T}=
%   \sum_{\text{\scriptsize contributing\ }\phi}
%           t_{1,\phi(1)}t_{2,\phi(2)}\cdots t_{n,\phi(n)}\sgn(\phi)$.
% \end{proof}

% \begin{example}
% \begin{equation*}
%     \begin{vmat}[r]
%        2  &0  &0  &0  \\
%        1  &2  &0  &0  \\
%        0  &0  &3  &0  \\
%        0  &0  &0  &3
%     \end{vmat}
%    =\begin{vmat}[r]
%        2  &0  \\
%        1  &2
%     \end{vmat}
%     \cdot
%     \begin{vmat}[r]
%        3  &0  \\
%        0  &3
%     \end{vmat}
%    =36
% \end{equation*}
% \end{example}

% From \nearbylemma{le:DetIsProdOfSubDets} we conclude that
% if two subspaces 
% are complementary and \( t \)~invariant then
% \( t \) is one-to-one if and only if its 
% restriction %\appendrefs{restrictions}
% to each subspace is nonsingular.

% Now for the promised third, and final, step to the main result.

% \begin{lemma}
% If a linear transformation \( \map{t}{V}{V} \) has the 
% characteristic polynomial
% \( (x-\lambda_1)^{p_1}\dots(x-\lambda_k)^{p_k} \) then
% (1)~\( V=\gennullspace{t-\lambda_1}\directsum\cdots
%              \directsum\gennullspace{t-\lambda_k} \) 
% and
% (2)~\( \dim(\gennullspace{t-\lambda_i})=p_i  \).
% \end{lemma}

% \begin{proof}
% This argument consists of proving
% two preliminary claims, followed by proofs of clauses~(1) and~(2).

% The first claim is that
% \( \gennullspace{t-\lambda_i}\intersection\gennullspace{t-\lambda_j}=\set{\zero} \)
% when \( i\neq j \).
% By \nearbylemma{le:tInvIfftMinLambdaInv}
% both \( \gennullspace{t-\lambda_i} \) and 
% \( \gennullspace{t-\lambda_j} \) are \( t \)~invariant.
% The intersection of \( t \) invariant subspaces is \( t \)
% invariant and so the restriction of \( t \) to
% \( \gennullspace{t-\lambda_i}\intersection\gennullspace{t-\lambda_j} \)
% is a linear transformation.
% Now,
% $t-\lambda_i$ is nilpotent on \( \gennullspace{t-\lambda_i} \)
% and  
% $t-\lambda_j$ is nilpotent on \( \gennullspace{t-\lambda_j} \),
% so both \( t-\lambda_i \) and \( t-\lambda_j \) are 
% nilpotent on the intersection.
% Therefore by \nearbylemma{le:NilIffOnlyEigenZero} and the observation
% following it, if \( t \) has any eigenvalues on the intersection 
% then the ``only'' eigenvalue is both
% \( \lambda_i \) and \( \lambda_j \).
% This cannot be, so the restriction has no eigenvalues:
% \( \gennullspace{t-\lambda_i}\intersection\gennullspace{t-\lambda_j} \)
% is the trivial space
% (\nearbylemma{le:MapNonTrivSpHasEigen} shows that 
% the only transformation that is without any eigenvalues is 
% the transformation on the trivial space).

% The second claim is that  when \( i\neq j \) then
% \( \gennullspace{t-\lambda_i} \subseteq \genrangespace{t-\lambda_j} \).
% To verify it we will show that \( t-\lambda_j \) is one-to-one on
% \( \gennullspace{t-\lambda_i} \).
% This will imply, 
% since \( \gennullspace{t-\lambda_i} \) is \( t-\lambda_j \) invariant
% by \nearbylemma{le:tInvIfftMinLambdaInv},
% that the map \( t-\lambda_j \) is an automorphism of the subspace
% \( \gennullspace{t-\lambda_i} \),
% and therefore that 
% \( \gennullspace{t-\lambda_i} \)  is a subset of each
% \( \rangespace{t-\lambda_j} \), \( \rangespace{(t-\lambda_j)^2} \), etc.
% To verify that the map is one-to-one,
% suppose that \( \vec{v}\in\gennullspace{t-\lambda_i} \) is in the 
% null space of \( t-\lambda_j \), aiming to show that \( \vec{v}=\zero \).
% Consider the map 
% \( [(t-\lambda_i)-(t-\lambda_j)]^n \).
% On the one hand, the only
% vector that \( (t-\lambda_i)-(t-\lambda_j)=\lambda_i-\lambda_j \) maps to 
% zero is the zero vector itself.
% On the other hand, as in the proof of 
% \nearbylemma{le:PolyMapsFactor}
% the binomial expansion gives this.
% \begin{equation*}
%   (t-\lambda_i)^n(\vec{v})
%     +\binom{n}{1}(t-\lambda_i)^{n-1}(t-\lambda_j)^1(\vec{v})
%     +\binom{n}{2}(t-\lambda_i)^{n-2}(t-\lambda_j)^2(\vec{v})
%     +\cdots 
% \end{equation*}
% The first term is the zero vector 
% because \( \vec{v}\in\gennullspace{t-\lambda_i} \).
% The remaining terms are the zero vector because
% \( \vec{v} \) is in the null space of \( t-\lambda_j \).
% Therefore \( \vec{v}=\zero \). 

% With those two preliminary claims done 
% we can prove clause~(1), that the space is the direct sum of the
% generalized null spaces.
% By Corollary~III.\ref{GenRngNullDirSumToSp} the space is the direct sum
% \( V=\gennullspace{t-\lambda_1}\directsum\genrangespace{t-\lambda_1} \).
% By the second claim
% \( \gennullspace{t-\lambda_2}\subseteq\genrangespace{t-\lambda_1} \)
% and so we can get a basis for \( \genrangespace{t-\lambda_1} \) by
% starting with a basis for \( \gennullspace{t-\lambda_2} \) and adding
% extra basis elements taken from
% \( \genrangespace{t-\lambda_1}\intersection\genrangespace{t-\lambda_2} \). 
% Thus \( V=\gennullspace{t-\lambda_1}\directsum\gennullspace{t-\lambda_2}
%           \directsum 
%        (\genrangespace{t-\lambda_1}\intersection\genrangespace{t-\lambda_2}) \).
% Continuing in this way we get this.
% \begin{equation*}
%   V=\gennullspace{t-\lambda_1}\directsum\cdots\directsum\gennullspace{t-\lambda_k}
%           \directsum 
%        (\genrangespace{t-\lambda_1}\intersection\cdots\intersection\genrangespace{t-\lambda_k})
% \end{equation*}
% The first claim above shows that the final space is trivial. 

% We finish by verifying clause~(2).
% Decompose \( V \) as 
% \( \gennullspace{t-\lambda_i}\directsum\genrangespace{t-\lambda_i} \)
% %with basis $B=\cat{B_{\mathscr{N}}}{B_{\mathscr{R}}}$
% and apply \nearbylemma{le:InvCompSubspSplitTrans}.
% \begin{equation*}   \renewcommand{\arraystretch}{1.2}
%   T=
% %  \rep{t}{B,B}=
%   \begin{pmat}{c|c}
%       T_1   &Z_2  \\  \cline{1-2}
%       Z_1   &T_2
%    \end{pmat}
%    \begin{array}{@{}l}
%      \} \text{\ $\dim(\,\gennullspace{t-\lambda_i}\,)$-many rows}  \\
%      \} \text{\ $\dim(\,\genrangespace{t-\lambda_i}\,)$-many rows}
%    \end{array}
% \end{equation*}
% \nearbylemma{le:DetIsProdOfSubDets} says that
% \( \deter{T-xI}=\deter{T_1-xI}\cdot\deter{T_2-xI} \).
% By the uniqueness clause of the Fundamental Theorem of Algebra, 
% Theorem~I.\ref{th:FundThmAlg},
% the determinants of the blocks have the same factors as the
% characteristic polynomial
% \( \deter{T_1-xI}=(x-\lambda_1)^{q_1}\cdots(x-\lambda_z)^{q_k} \)
% and
% \( \deter{T_2-xI}=(x-\lambda_1)^{r_1}\cdots(x-\lambda_z)^{r_k} \),
% where
% \( q_1+r_1=p_1 \), \dots, \( q_k+r_k=p_k \).
% We will finish by establishing that (i)~$q_j=0$ for all $j\neq i$,
% and (ii)~$q_i=p_i$.
% Together these prove clause~(2) because they show that the degree of 
% the polynomial $\deter{T_1-xI}$ is $q_i$ and
% the degree of that polynomial equals 
% the dimension of the
% generalized null space $\gennullspace{t-\lambda_i}$. 

% For (i),
% because the restriction of \( t-\lambda_i \) to \( \gennullspace{t-\lambda_i} \)
% is nilpotent on that space,
% $t$'s only eigenvalue on that space is \( \lambda_i \),
% by \nearbylemma{cor:tMinLambdaNilpotent}.
% So $q_j=0$ for $j\neq i$.

% For (ii),
% consider the restriction of \( t \) to \( \genrangespace{t-\lambda_i} \).
% By Lemma~III.\ref{lem:RestONeToOne}, the map
% \( t-\lambda_i \) is one-to-one on
% \( \genrangespace{t-\lambda_i} \) and so \( \lambda_i \) is not an
% eigenvalue of \( t \) on that subspace.
% Therefore \( x-\lambda_i \) is not a factor of \( \deter{T_2-xI} \),
% so $r_i=0$, and so \( q_i=p_i \).
% \end{proof}

% Recall the goal of this chapter, to give a canonical form for matrix similarity.
% That result is next.
% It 
% translates the above steps into matrix terms.

% \begin{theorem}
% \index{Jordan form!represents similarity classes}\index{similar!canonical form}
% \index{canonical form!for similarity}\index{transformation!Jordan form for}
% Any square matrix is similar to one in \definend{Jordan form}
% \begin{equation*}
%   \begin{mat}
%     J_{\lambda_1}  &            &\text{\textit{--zeroes--}}                 \\
%                &J_{\lambda_2}                                              \\
%                &     &\ddots                                     \\
%                 &     &                           &J_{\lambda_{k-1}} &     \\
%                &     &\text{\textit{--zeroes--}} &            &J_{\lambda_{k}}
%   \end{mat}
% \end{equation*}
% where each \( J_{\lambda} \) is the Jordan block associated with an
% eigenvalue $\lambda$ of the original matrix (that is, each \( J_{\lambda} \)
% is all zeroes except for
% \( \lambda \)'s down the diagonal and some subdiagonal ones).
% \end{theorem}

% \begin{proof}
% Given an \( \nbyn{n} \) matrix \( T \), consider the linear map
% \( \map{t}{\C^n}{\C^n} \) that it represents
% with respect to the standard bases.
% Use the prior lemma to write
% \( \C^n=\gennullspace{t-\lambda_1}\directsum\cdots
%         \directsum\gennullspace{t-\lambda_k} \)
% where \( \lambda_1 \), \ldots, \(\lambda_k \) are the eigenvalues of \( t \).
% Because each \( \gennullspace{t-\lambda_i} \)  is \( t \) invariant,
% \nearbylemma{le:InvCompSubspSplitTrans} and the prior lemma show
% that \( t \) is represented by a matrix that is all zeroes except for square
% blocks along the diagonal.
% To make those blocks into Jordan blocks, pick each \( B_{\lambda_i} \)
% to be a string basis for the action of \( t-\lambda_i \) on
% \( \gennullspace{t-\lambda_i} \). 
% \end{proof}

% \begin{corollary}
% Every square matrix is similar to the sum of a diagonal matrix and a nilpotent
% matrix.
% \end{corollary}

\begin{remark}
For Jordan form to be a canonical form for 
matrix similarity,\index{representative!of similarity classes}
strictly speaking it must be unique.
That is, for any square matrix there needs to be one and only one 
Jordan form matrix
similar to it.
As stated, the theorem allows us to rearrange the Jordan blocks.
We could make this form unique, 
by arranging the Jordan blocks so the eigenvalues are in 
some specified order and then arranging
the blocks of subdiagonal ones from 
longest to shortest.
Below, we won't bother with that.
\end{remark}

The examples below we start with a matrix and calculate 
a Jordan form matrix similar to it.
For instance in the first example we find the eigenvalues are $2$ 
and~$6$ and to pick basis elements we find the null spaces of $t-2$ and~$t-6$. 
We want that the calculation for each map does not affect the calculation
for the other.

So before those examples, 
suppose that $\map{t}{\C^n}{\C^n}$ is a transformation and 
that $\lambda\in\C$ is an eigenvalue.
Note that the generalized null space~$\gennullspace{t-\lambda}$ 
and range space~$\genrangespace{t-\lambda}$ are $t-\lambda$
invariant spaces.
For the generalized null space, 
if $\vec{v}\in\gennullspace{t-\lambda}$ then 
$(t-\lambda)^n(\vec{v})=\zero$ and it follows that
$(t-\lambda)^n(\,(t-\lambda)(\vec{v})\,)=(t-\lambda)^{n+1}(\vec{v})=\zero$ 
also. 
As to the generalized range space,
Lemma Five.III.\ref{le:RangeAndNullChains} and its proof show that
$\genrangespace{t-\lambda}
  =\rangespace{(t-\lambda)^n}
  =\rangespace{(t-\lambda)^{n+1}}$ 
and so $\vec{v}\in\genrangespace{t-\lambda}$ implies that
$(t-\lambda)(\vec{v})\in\genrangespace{t-\lambda}$ also.

\begin{lemma} \label{le:tInvIfftMinLambdaInv}
A subspace is \( t \) invariant if and only if 
it is \( t-\lambda \) invariant for all scalars \( \lambda \).
In particular, 
if \( \lambda_i,\lambda_j\in\C \) are eigenvalues of
\( t \) then
the spaces \( \gennullspace{t-\lambda_i} \) 
and \( \genrangespace{t-\lambda_i} \)
are \( t-\lambda_j \) invariant.
\end{lemma}

\begin{proof}
The right-to-left half of the first sentence is easy:~if 
the subspace is $t-\lambda$ invariant for 
all scalars $\lambda$ then taking $\lambda=0$ 
shows that it is $t$ invariant.
For the other half suppose that the subspace is $t$~invariant,
so that if $\vec{m}\in M$ then $t(\vec{m})\in M$, and let $\lambda$ be 
a scalar.
The subspace $M$ is closed under linear combinations and so if 
$t(\vec{m})\in M$ then $t(\vec{m})-\lambda\vec{m}\in M$.
Thus if $\vec{m}\in M$ then $(t-\lambda)\,(\vec{m})\in M$.

The lemma's second sentence follows from its first.
The
two spaces are $t-\lambda_i$~invariant, so they are \( t \)~invariant,
and applying the first sentence again gives that 
they are also \( t-\lambda_j \) invariant.
\end{proof}

\begin{example} \label{ex:FirstJordForm}
This matrix 
\begin{equation*}
   T=
   \begin{mat}[r]
     2  &0  &1  \\
     0  &6  &2  \\
     0  &0  &2
   \end{mat}
\end{equation*}
has the characteristic polynomial \( (x-2)^2(x-6) \).
\begin{equation*}
   |T-xI|=
   \begin{vmat}
     2-x  &0    &1  \\
     0    &6-x  &2  \\
     0    &0    &2-x
   \end{vmat}
   =(x-2)^2(x-6)
\end{equation*}
First we do the eigenvalue~$2$.
Computation of the powers of $T-2I$, and of the null spaces and nullities, 
is routine.
(Recall from \nearbyexample{ex:SingJordBlock} our convention
of taking $T$ to represent a transformation $\map{t}{\C^3}{\C^3}$
with respect to the standard basis.)
\begin{center}
  \renewcommand{\arraystretch}{1.25}
  \begin{tabular}{r|ccc} 
    \multicolumn{1}{c}{\( p \)}  
         &\( (T-2I)^p \) &\( \nullspace{(t-2)^p}  \) 
         &\textit{nullity}                                            \\  
    \hline
    \( 1 \)
    &\matrixvenlarge{\begin{mat}[r]
          0  &0  &1  \\
          0  &4  &2  \\
          0  &0  &0
        \end{mat}}
    &\( \set{\matrixvenlarge{\colvec{x \\ 0 \\ 0}}
         \suchthat x\in\C}  \)  
    &$1$                                                   \\
    \( 2 \)
    &\matrixvenlarge{\begin{mat}[r]
          0  &0  &0  \\
          0  &16 &8  \\
          0  &0  &0
        \end{mat}}
    &\( \set{\matrixvenlarge{\colvec{x \\ -z/2 \\  z}}
               \suchthat x,z\in\C}  \) 
    &$2$                                                   \\
    \( 3 \)
    &\matrixvenlarge{\begin{mat}[r]
          0  &0  &0  \\
          0  &64 &32 \\
          0  &0  &0
        \end{mat}}
    &\textit{--same--}
    &\textit{--same--}
  \end{tabular}
\end{center}
So the generalized null space $\gennullspace{t-2}$ has dimension two.
We know that the restriction of $t-2$ is nilpotent on this subspace.
From the way that the nullities grow we know that the action
of $t-2$ on a string basis is
$\vec{\beta}_1\mapsto\vec{\beta}_2\mapsto\zero$.  
Thus we can represent the restriction in the canonical form 
\begin{equation*}
  N_2=
  \begin{mat}[r]
    0  &0  \\
    1  &0  
  \end{mat}
  =\rep{t-2}{B,B}
  \qquad
   B_2=\sequence{\colvec[r]{1 \\ 1 \\ -2},
                 \colvec[r]{-2 \\ 0 \\ 0}}  
\end{equation*}
(other choices of basis are possible).
Consequently, the action of the restriction of $t$ to 
$\gennullspace{t-2}$ is represented by this matrix.
\begin{equation*}
  J_2=N_2+2I=\rep{t}{B_2,B_2}=
  \begin{mat}[r]
    2  &0  \\
    1  &2
  \end{mat}
\end{equation*}

The second eigenvalue is $6$.
Its computations are easier.
Because the power of $x-6$ in the characteristic polynomial is one,
the restriction of $t-6$ to $\gennullspace{t-6}$
must be nilpotent, of index one
(it can't be of index less than one and since $x-6$ is a 
factor of the characteristic polynomial with the exponent one it can't
be of index more than one either). 
Its action on a string basis must be $\vec{\beta}_3\mapsto\zero$ and
since it is the zero map, its canonical form $N_6$ 
is the $\nbyn{1}$ zero matrix.
Consequently, the canonical form $J_6$ for the action of $t$ on 
$\gennullspace{t-6}$ is the $\nbyn{1}$ matrix with the single entry $6$.
For the basis we can use any nonzero vector from the generalized null space.  
\begin{equation*}
   B_6=\sequence{\colvec[r]{0 \\ 1 \\ 0}}
\end{equation*}

Taken together, these two give that
the Jordan form of \( T \) is
\begin{equation*}
   \rep{t}{B,B}=
   \begin{mat}[r]
      2  &0  &0  \\
      1  &2  &0  \\
      0  &0  &6
   \end{mat}
\end{equation*}
where \( B \) is the concatenation of $B_2$ and $B_6$.
(If we want to be careful about getting a unique Jordan form then
we could, for instance, rearrange the basis elements to have the eigenvalues
in descending order.
But, ascending order is fine also.)
\end{example}

\begin{example}  \label{SecJordanForm}
As a contrast, this matrix
has the same characteristic polynomial 
as the prior example, \( (x-2)^2(x-6) \).
\begin{equation*}
   T=
   \begin{mat}[r]
     2  &2  &1  \\
     0  &6  &2  \\
     0  &0  &2
   \end{mat}
\end{equation*}
But here 
the action of $t-2$ is stable after only one application\Dash the 
restriction
of $t-2$ to $\gennullspace{t-2}$ is nilpotent of index one. 
\begin{center}
  % \renewcommand{\arraystretch}{1.25}
  \begin{tabular}{r|ccc}
    \multicolumn{1}{c}{\( p \)}  &\( (T-2I)^p \)  &\( \nullspace{(t-2)^p}  \)  
       &\textit{nullity} \\                                     \hline
   \( 1 \)
   &\matrixvenlarge{\begin{mat}[r]
         0  &2  &1  \\
         0  &4  &2  \\
         0  &0  &0
       \end{mat}}
   &\( \set{\matrixvenlarge{\colvec{x \\ (-1/2)z \\ z}}\suchthat x,z\in\C}  \) 
   &$2$                                                     \\
   \( 2 \)
   &\matrixvenlarge{\begin{mat}[r]
         0  &8  &4 \\
         0  &16 &8 \\
         0  &0  &0
       \end{mat}}
   &\textit{--same--}
   &\textit{--same--}
 \end{tabular}
\end{center}
So the restriction of $t-2$ to the generalized null space acts on a string
basis via the two strings $\vec{\beta}_1\mapsto\zero$ 
and $\vec{\beta}_2\mapsto\zero$.
We have this Jordan block associated with the eigenvalue~$2$.
\begin{equation*}
  J_2=
  \begin{mat}[r]
    2  &0  \\
    0  &2  
  \end{mat}
\end{equation*}

Thus, the contrast with the prior example is that while 
the characteristic polynomial tells us to look at the 
action of $t-2$ on its generalized null space, the characteristic
polynomial does not completely describe $t-2$'s action. 
We must do some computations to find that  
the minimal polynomial is \( (x-2)(x-6) \).

For the eigenvalue $6$, the arguments for the second eigenvalue of
the prior example apply again:~because the power of $x-6$ in the 
characteristic polynomial is one,
the restriction of $t-6$ to $\gennullspace{t-6}$
must be nilpotent of index one.
Alternatively, we can just compute it.
\begin{center}
  \begin{tabular}{r|ccc}
    \multicolumn{1}{c}{\( p \)}  &\( (T-6I)^p \)  &\( \nullspace{(t-6)^p}  \)  
       &\textit{nullity} \\                                     \hline
   \( 1 \)
   &\matrixvenlarge{\begin{mat}[r]
        -4  &2  &1  \\
         0  &0  &2  \\
         0  &0  &-4
       \end{mat}}
   &\( \set{\matrixvenlarge{\colvec{x \\ 2x \\ 0}}\suchthat x\in\C}  \) 
   &$1$                                                     \\
   \( 2 \)
   &\matrixvenlarge{\begin{mat}[r]
        16  &-8 &-4 \\
         0  &0  &-8 \\
         0  &0  &16
       \end{mat}}
   &\textit{--same--}
   &\textit{--same--}
 \end{tabular}
\end{center}
Either way, the restriction of $t-6$ to $\gennullspace{t-6}$ is nilpotent of 
index one.
Thus $t-6$'s canonical form $N_6$ is the $\nbyn{1}$ zero matrix,
and the associated Jordan block $J_6$ is the $\nbyn{1}$ matrix with entry $6$.
 
Therefore the Jordan form for $T$ is a diagonal matrix, so this is a 
diagonalizable transformation.
\begin{equation*}
  \rep{t}{B,B}=
  \begin{mat}[r]
    2  &0  &0  \\
    0  &2  &0  \\
    0  &0  &6
  \end{mat}
  \qquad
  B=\cat{B_2}{B_6}
   =\sequence{\colvec[r]{1 \\ 0 \\ 0},
              \colvec[r]{0 \\ 1 \\ -2},
              \colvec[r]{1 \\ 2 \\ 0}}
\end{equation*}
Of the three basis vectors, the first two come from the nullspace of
$t-2$ and the third is from the nullspace of~$t-6$. 
\end{example}

\begin{example} \label{ThirdJordanForm}
A bit of computing with
\begin{equation*}
   T=
   \begin{mat}[r]
     -1  &4  &0  &0  &0  \\
      0  &3  &0  &0  &0  \\
      0  &-4 &-1 &0  &0  \\
      3  &-9 &-4 &2  &-1 \\
      1  &5  &4  &1  &4
   \end{mat}
\end{equation*}
shows that its characteristic polynomial
is \( (x-3)^3(x+1)^2 \).
This table
\begin{center}
  % \renewcommand{\arraystretch}{1.25}
  \begin{tabular}{@{}r|c@{}c@{}c@{}}
    \multicolumn{1}{c}{\( p \)}  
            &\( (T-3I)^p \)  &\( \nullspace{(t-3)^p}  \)
             &\textit{nullity}   
    \\  \hline
    \( 1 \)
    &\matrixvenlarge{\begin{mat}[r]
         -4  &4  &0  &0  &0  \\
          0  &0  &0  &0  &0  \\
          0  &-4 &-4 &0  &0  \\
          3  &-9 &-4 &-1 &-1 \\
          1  &5  &4  &1  &1
       \end{mat}}
    &\( \set{\matrixvenlarge{\colvec{-(u+v)/2 \\
                     -(u+v)/2 \\
                      (u+v)/2 \\
                       u      \\
                       v}}\suchthat u,v\in\C}  \) 
    &$2$                                            \\
    \( 2 \)
    &\matrixvenlarge{\begin{mat}[r]
         16  &-16&0  &0  &0  \\
          0  &0  &0  &0  &0  \\
          0  &16 &16 &0  &0  \\
        -16  &32 &16 &0  &0  \\
          0  &-16&-16&0  &0
       \end{mat}}
    &\( \set{\matrixvenlarge{\colvec{ -z      \\
                      -z      \\
                       z      \\
                       u      \\
                       v}}\suchthat z,u,v\in\C}  \) 
    &$3$                                              \\
    \( 3 \)
    &\matrixvenlarge{\begin{mat}[r]
        -64  &64   &0   &0   &0  \\
          0  &0    &0   &0   &0  \\
          0  &-64  &-64 &0   &0  \\
         64  &-128 &-64 &0   &0  \\
          0  &64   &64  &0   &0
       \end{mat}}
    &\textit{--same--}
    &\textit{--same--}
  \end{tabular}
\end{center}
shows that the restriction of $t-3$ to $\gennullspace{t-3}$ acts on a 
string basis via the two strings
$\vec{\beta}_1\mapsto\vec{\beta}_2\mapsto\zero$
and
$\vec{\beta}_3\mapsto\zero$.

A similar calculation for the other eigenvalue
\begin{center}
  \renewcommand{\arraystretch}{1.25}
  \begin{tabular}{r|ccc}
    \multicolumn{1}{c}{\( p \)}  
         &\( (T+1I)^p \)  &\( \nullspace{(t+1)^p}  \) 
         &\textit{nullity}  \\  
    \hline
    \( 1 \)
    &\matrixvenlarge{\begin{mat}[r]
          0  &4  &0  &0  &0  \\
          0  &4  &0  &0  &0  \\
          0  &-4 &0  &0  &0  \\
          3  &-9 &-4 &3  &-1 \\
          1  &5  &4  &1  &5
       \end{mat}}
    &\( \set{\matrixvenlarge{\colvec{-(u+v)   \\
                       0      \\
                      -v      \\
                       u      \\
                       v}}\suchthat u,v\in\C}  \)  
    &$2$                                              \\
    \( 2 \)
    &\matrixvenlarge{\begin{mat}[r]
          0  &16 &0  &0  &0  \\
          0  &16 &0  &0  &0  \\
          0  &-16&0  &0  &0  \\
          8  &-40&-16&8  &-8 \\
          8  &24 &16 &8  &24
       \end{mat}}
    &\textit{--same--}
    &\textit{--same--}
  \end{tabular}
\end{center}
gives that the restriction of $t+1$ to its generalized null space
acts on a string basis via the two separate strings
$\vec{\beta}_4\mapsto\zero$ and $\vec{\beta}_5\mapsto\zero$.

Therefore
\( T \) is similar to this Jordan form matrix.
\begin{equation*}
   \begin{mat}[r]
     -1  &0  &0  &0  &0  \\
      0  &-1 &0  &0  &0  \\    
      0  &0  &3  &0  &0  \\
      0  &0  &1  &3  &0  \\
      0  &0  &0  &0  &3  
   \end{mat}
\end{equation*}
\end{example}





\begin{exercises}
  \item 
    Do the check for \nearbyexample{ex:SingJordBlock}.
    \begin{answer}
      We must check that
      \begin{equation*}
         \begin{mat}[r]
           3  &0  \\
           1  &3
         \end{mat}
         =
         N+3I=PTP^{-1}
         =
         \begin{mat}[r]
           1/2  &1/2  \\
          -1/4 &1/4
         \end{mat}
         \begin{mat}[r]
           2  &-1  \\
           1  &4
         \end{mat}
         \begin{mat}[r]
           1  &-2  \\
           1  &2
         \end{mat}
      \end{equation*}
      That calculation is easy.
    \end{answer}
  \item 
    Each matrix is in Jordan form.
    State its characteristic polynomial and its minimal polynomial.
    \begin{exparts*}
      \partsitem 
        $\begin{mat}[r]
           3  &0  \\
           1  &3        
         \end{mat}$
      \partsitem 
        $\begin{mat}[r]
           -1  &0  \\
            0  &-1 
         \end{mat}$
      \partsitem 
        $\begin{mat}[r]
           2  &0  &0  \\
           1  &2  &0  \\
           0  &0  &-1/2
         \end{mat}$
      \partsitem 
        $\begin{mat}[r]
           3  &0  &0  \\
           1  &3  &0  \\
           0  &1  &3  \\ 
         \end{mat}$
      \partsitem 
        $\begin{mat}[r]
           3  &0  &0  &0  \\
           1  &3  &0  &0  \\
           0  &0  &3  &0  \\
           0  &0  &1  &3 
         \end{mat}$
      \partsitem 
        $\begin{mat}[r]
           4  &0  &0  &0  \\
           1  &4  &0  &0  \\
           0  &0  &-4 &0  \\
           0  &0  &1  &-4
         \end{mat}$
      \partsitem 
        $\begin{mat}[r]
           5  &0  &0  \\
           0  &2  &0  \\
           0  &0  &3  
         \end{mat}$
      \partsitem 
        $\begin{mat}[r]
           5  &0  &0  &0  \\
           0  &2  &0  &0  \\
           0  &0  &2  &0  \\
           0  &0  &0  &3 
         \end{mat}$
      \partsitem 
        $\begin{mat}[r]
           5  &0  &0  &0  \\
           0  &2  &0  &0  \\
           0  &1  &2  &0  \\
           0  &0  &0  &3 
         \end{mat}$
    \end{exparts*}
    \begin{answer}
      \begin{exparts}
        \partsitem The characteristic polynomial is $c(x)=(x-3)^2$ and
          the minimal polynomial is the same.
        \partsitem The characteristic polynomial is $c(x)=(x+1)^2$.
          The minimal polynomial is $m(x)=x+1$.
        \partsitem The characteristic polynomial is 
          $c(x)=(x+(1/2))(x-2)^2$ and
          the minimal polynomial is the same.
        \partsitem The characteristic polynomial is $c(x)=(x-3)^3$
          The minimal polynomial is the same.
        \partsitem The characteristic polynomial is $c(x)=(x-3)^4$.
          The minimal polynomial is $m(x)=(x-3)^2$.
        \partsitem The characteristic polynomial is $c(x)=(x+4)^2(x-4)^2$ and
          the minimal polynomial is the same.
        \partsitem The characteristic polynomial is 
          $c(x)=(x-2)^2(x-3)(x-5)$ and
          the minimal polynomial is $m(x)=(x-2)(x-3)(x-5)$.
        \partsitem The characteristic polynomial is 
          $c(x)=(x-2)^2(x-3)(x-5)$ and
          the minimal polynomial is the same.
      \end{exparts}
    \end{answer}
  \recommended \item
    Find the Jordan form from the given data.
    \begin{exparts}
       \partsitem The matrix 
         \( T \) is \( \nbyn{5} \) with the single eigenvalue $3$.
         The nullities of the powers are:
         \( T-3I \) has nullity two, \( (T-3I)^2 \) has nullity three,
         \( (T-3I)^3 \) has nullity four, and \( (T-3I)^4 \) has nullity
         five.
       \partsitem The matrix \( S \) is \( \nbyn{5} \) with two eigenvalues.
         For the eigenvalue $2$ the nullities are:
         \( S-2I \) has nullity two, and \( (S-2I)^2 \) has nullity four.
         For the eigenvalue $-1$ the nullities are:
         \( S+1I \) has nullity one.
    \end{exparts}
    \begin{answer}
      \begin{exparts}
         \partsitem The transformation $t-3$ is nilpotent 
          (that is, $\gennullspace{t-3}$ is the entire space)
          and it acts on a string basis via two strings, 
          $\vec{\beta}_1\mapsto\vec{\beta}_2\mapsto\vec{\beta}_3
            \mapsto\vec{\beta}_4\mapsto\zero$
          and $\vec{\beta}_5\mapsto\zero$.
          Consequently, $t-3$ can be represented in this canonical form.
          \begin{equation*}
            N_3=
            \begin{mat}[r]
               0  &0  &0  &0  &0  \\
               1  &0  &0  &0  &0  \\
               0  &1  &0  &0  &0  \\
               0  &0  &1  &0  &0  \\
               0  &0  &0  &0  &0
             \end{mat}
          \end{equation*}
          and therefore $T$ is similar to this canonical form matrix.
          \begin{equation*}
           J_3=N_3+3I=
           \begin{mat}[r]
               3  &0  &0  &0  &0  \\
               1  &3  &0  &0  &0  \\
               0  &1  &3  &0  &0  \\
               0  &0  &1  &3  &0  \\
               0  &0  &0  &0  &3
             \end{mat}
         \end{equation*} 
         \partsitem The restriction of the transformation $s+1$ is nilpotent
          on the subspace $\gennullspace{s+1}$, and the action on a 
          string basis is  $\vec{\beta}_1\mapsto\zero$.  
          The restriction of the transformation $s-2$ is nilpotent
          on the subspace $\gennullspace{s-2}$, having the action on a 
          string basis of $\vec{\beta}_2\mapsto\vec{\beta}_3\mapsto\zero$
          and $\vec{\beta}_4\mapsto\vec{\beta}_5\mapsto\zero$.        
          Consequently the Jordan form is this.
          \begin{equation*}
            \begin{mat}[r]
              -1  &0  &0  &0  &0  \\
               0  &2  &0  &0  &0  \\
               0  &1  &2  &0  &0  \\
               0  &0  &0  &2  &0  \\
               0  &0  &0  &1  &2
             \end{mat} 
          \end{equation*}
      \end{exparts}  
    \end{answer}
  \item 
    Find the change of basis matrices for each example. 
    \begin{exparts*}
      \partsitem \nearbyexample{ex:FirstJordForm}
      \partsitem \nearbyexample{SecJordanForm}
      \partsitem \nearbyexample{ThirdJordanForm}
    \end{exparts*}
    \begin{answer}
      For each, because many choices of basis are possible, many other 
      answers are possible.
      Of course, the calculation to check if an answer gives that $PTP^{-1}$
      is in Jordan form is the arbiter of what's correct.
      \begin{exparts}
        \partsitem Here is the arrow diagram.
          \begin{equation*}
            \begin{CD}
              \C^3_{\wrt{\stdbasis_3}}      @>t>T>    \C^3_{\wrt{\stdbasis_3}}   \\
                @V\scriptstyle\identity V\scriptstyle PV  
                                    @V\scriptstyle\identity V\scriptstyle PV \\
              \C^3_{\wrt{B}}                 @>t>J>         \C^3_{\wrt{B}}
            \end{CD}
          \end{equation*}
          The matrix to move from the lower left to the upper left is this. 
          \begin{equation*}
            P^{-1}=\bigl(\rep{\identity}{\stdbasis_3,B}\bigr)^{-1}
                  =\rep{\identity}{B,\stdbasis_3}   
                  =\begin{mat}[r]
                     1  &-2   &0 \\
                     1  &0   &1 \\
                    -2  &0   &0
                   \end{mat}
          \end{equation*}
          The matrix $P$ to move from the upper right to the lower
          right is the inverse of $P^{-1}$.
        \partsitem We want this matrix and its inverse.
          \begin{equation*}
            P^{-1}=
            \begin{mat}[r]
              1  &0  &3  \\
              0  &1  &4  \\
              0  &-2 &0
            \end{mat}
          \end{equation*}
        \partsitem The concatenation of these bases for the 
          generalized null spaces will do for the basis for the
          entire space.
          \begin{equation*}
             B_{-1}=\sequence{\colvec[r]{-1\\ 0 \\  0 \\ 1 \\ 0},
                 \colvec[r]{-1\\ 0 \\ -1 \\ 0 \\ 1}}
            \qquad
             B_3=\sequence{\colvec[r]{1 \\ 1 \\ -1 \\ 0 \\ 0},
                 \colvec[r]{0 \\ 0 \\  0 \\-2 \\ 2},
                 \colvec[r]{-1\\-1 \\  1 \\ 2 \\ 0}}
          \end{equation*}
          The change of basis matrices are this one and its inverse.
          \begin{equation*}
            P^{-1}=
            \begin{mat}[r]
              -1  &-1  &1  &0  &-1  \\
              0   &0   &1  &0  &-1  \\
              0   &-1  &-1 &0  &1   \\
              1   &0   &0  &-2 &2   \\
              0   &1   &0  &2  &0   \\
            \end{mat}
          \end{equation*}
      \end{exparts}
    \end{answer}
  \recommended \item 
    Find the Jordan form and a Jordan basis for each matrix.
    \begin{exparts*}
      \partsitem 
        \(
        \begin{mat}[r]
          -10  &4  \\
          -25  &10
        \end{mat} \)
      \partsitem 
        \(
        \begin{mat}[r]
           5   &-4 \\
           9   &-7
        \end{mat} \)
      \partsitem 
        \(
        \begin{mat}[r]
           4   &0    &0  \\
           2   &1    &3  \\
           5   &0    &4
        \end{mat} \)
      \partsitem 
        \(
        \begin{mat}[r]
           5   &4    &3  \\
          -1   &0    &-3 \\
           1   &-2   &1
        \end{mat} \)
      \partsitem
        \(
        \begin{mat}[r]
           9   &7    &3  \\
          -9   &-7   &-4 \\
           4   &4    &4
        \end{mat} \)
      \partsitem 
        \(
        \begin{mat}[r]
           2   &2    &-1 \\
          -1   &-1   &1  \\
          -1   &-2   &2
        \end{mat} \)
      \partsitem 
        \(
        \begin{mat}[r]
           7   &1    &2   &2 \\
           1   &4    &-1  &-1\\
          -2   &1    &5   &-1\\
           1   &1    &2   &8
        \end{mat} \)
    \end{exparts*}
    \begin{answer}
      The general procedure is to factor the characteristic polynomial 
      $c(x)=(x-\lambda_1)^{p_1}(x-\lambda_2)^{p_2}\cdots $ 
      to get the eigenvalues $\lambda_1$, $\lambda_2$, etc. 
      Then, for each $\lambda_i$ we find a 
      string basis for the action of the transformation $t-\lambda_i$
      when restricted to $\gennullspace{t-\lambda_i}$,
      by computing the powers of the matrix $T-\lambda_iI$ and finding
      the associated null spaces, until these null spaces settle down
      (do not change), at which point we have the generalized null space.
      The dimensions of those null spaces (the nullities) tell us the
      action of $t-\lambda_i$ on a string basis for the generalized
      null space, and so we can write the pattern of subdiagonal ones
      to have $N_{\lambda_i}$.
      From this matrix, the Jordan block $J_{\lambda_i}$ associated
      with $\lambda_i$ is immediate $J_{\lambda_i}=N_{\lambda_i}+\lambda_iI$.
      Finally, after we have done this for each eigenvalue, we put them
      together into the canonical form.
      \begin{exparts}
        \partsitem The characteristic polynomial of this matrix
           is $c(x)=(-10-x)(10-x)+100=x^2$, 
           so it has only the single eigenvalue $\lambda=0$.
           \begin{center}
             \renewcommand{\arraystretch}{1.25}
             \begin{tabular}{r|ccc}
                \multicolumn{1}{c}{\textit{power}~$p$} 
                    &$(T+0\cdot I)^p$ &$\nullspace{(t-0)^p}$
                    &\textit{nullity}                   \\ 
                \hline
                $1$  
                &\matrixvenlarge{\begin{mat}[r]
                  -10  &4  \\ 
                  -25  &10
                \end{mat}}
                &$\set{\matrixvenlarge{\colvec{2y/5 \\ y}}
                                     \suchthat
                                     y\in\C}$
                &$1$                                       \\
                $2$  
                &\matrixvenlarge{\begin{mat}[r]
                    0  &0  \\ 
                    0  &0
                \end{mat}}
                &$\C^2$
                &$2$
             \end{tabular}
           \end{center}
           (Thus, this transformation is nilpotent: 
           $\gennullspace{t-0}$ is the entire space).
           From the nullities we know that $t$'s
           action on a string basis is 
           $\vec{\beta}_1\mapsto\vec{\beta}_2\mapsto\zero$.
           This is the canonical form matrix for the action of $t-0$ on
           $\gennullspace{t-0}=\C^2$
           \begin{equation*}
             N_0=
             \begin{mat}[r]
               0  &0  \\
               1  &0
            \end{mat}
           \end{equation*}
           and this is the Jordan form of the matrix.
           \begin{equation*}
             J_0=N_0+0\cdot I=
             \begin{mat}[r]
               0  &0  \\
               1  &0
            \end{mat}
           \end{equation*}
           Note that if a matrix is nilpotent then its canonical form
           equals its Jordan form.

           We can find such a string basis using the techniques of the 
           prior section.
           \begin{equation*}
                 B=\sequence{\colvec[r]{1 \\ 0},
                             \colvec[r]{-10 \\ -25}}
           \end{equation*}
           We took the first basis vector so that it is in
           the null space of $t^2$ but is not in the null space of $t$.
           The second basis vector is the image of the first under $t$.
        \partsitem The characteristic polynomial of this matrix
           is \( c(x)=(x+1)^2 \), so it is a single-eigenvalue matrix.
           (That is, the generalized null space of $t+1$ is the entire
           space.) 
           We have
           \begin{equation*}
             \nullspace{t+1}=\set{\colvec{2y/3 \\ y}\suchthat
                                     y\in\C} 
             \qquad
             \nullspace{(t+1)^2}=\C^2 
           \end{equation*}
           and so the action of $t+1$ on
           an associated string basis is 
           $\vec{\beta}_1\mapsto\vec{\beta}_2\mapsto\zero$.
           Thus, 
           \begin{equation*}
             N_{-1}
             =
             \begin{mat}[r]
                0  &0  \\
                1  &0 
             \end{mat}
           \end{equation*}
           the Jordan form of T is
           \begin{equation*}
             J_{-1}=N_{-1}+-1\cdot I
             =
             \begin{mat}[r]
               -1  &0  \\
                1  &-1
             \end{mat}
           \end{equation*}
           and choosing vectors from the above null spaces gives
           this string basis (other choices are possible).
           \begin{equation*}
             B=\sequence{\colvec[r]{1 \\ 0},
                         \colvec[r]{6 \\ 9}}
           \end{equation*}
        \partsitem The characteristic polynomial 
            \( c(x)=(1-x)(4-x)^2=-1\cdot (x-1)(x-4)^2 \) has two roots
            and they are the eigenvalues $\lambda_1=1$ and $\lambda_2=4$.

            We handle the two eigenvalues separately.
            For $\lambda_1$, the calculation of the powers of $T-1I$
            yields
            \begin{equation*}
              \nullspace{t-1}=\set{\colvec{0 \\ y \\ 0}
                                      \suchthat y\in\C}
            \end{equation*}
            and the null space of $(t-1)^2$ is the same.
            Thus this set is the generalized null space 
            $\gennullspace{t-1}$.
            The nullities show that the action of the restriction of $t-1$ 
            to the generalized null space on a string basis
            is  $\vec{\beta}_1\mapsto\zero$.

            A similar calculation for $\lambda_2=4$ gives these null spaces.
            \begin{equation*}
              \nullspace{t-4}=\set{\colvec{0 \\ z \\ z}
                                      \suchthat z\in\C}
              \qquad
              \nullspace{(t-4)^2}=\set{\colvec{y-z \\ y \\ z}
                                          \suchthat y,z\in\C}
            \end{equation*}
            (The null space of $(t-4)^3$ is the same, as it must be because
            the power of the term associated with $\lambda_2=4$ in the
            characteristic polynomial is two, and so the restriction of
            $t-2$ to the generalized null space $\gennullspace{t-2}$
            is nilpotent of index at most two\Dash it takes at most
            two applications of $t-2$ for the null space to settle down.)
            The pattern of how the nullities rise tells us that
             the action of $t-4$ on an associated string basis 
            for $\gennullspace{t-4}$ is 
            $\vec{\beta}_2\mapsto\vec{\beta}_3\mapsto\zero$.

            Putting the information for the two eigenvalues 
            together gives the Jordan form of the transformation $t$.
            \begin{equation*}
              \begin{mat}[r]
                1  &0  &0  \\
                0  &4  &0  \\
                0  &1  &4
              \end{mat}
            \end{equation*}
            We can take elements of the null spaces to get an appropriate
            basis.
            \begin{equation*}
              B=\cat{B_{1}}{B_4}=
               \sequence{\colvec[r]{0 \\ 1 \\ 0},
                          \colvec[r]{1 \\ 0 \\ 1},
                          \colvec[r]{0 \\ 5 \\ 5}}
            \end{equation*}
        \partsitem The characteristic polynomial is 
            \( c(x)=(-2-x)(4-x)^2=-1\cdot (x+2)(x-4)^2 \).

            For the eigenvalue $\lambda_{-2}$, calculation of the
            powers of $T+2I$ yields this.
            \begin{equation*}
              \nullspace{t+2}=\set{\colvec{z \\ z \\ z}
                                      \suchthat z\in\C}
            \end{equation*}
            The null space of $(t+2)^2$ is the same, and so 
            this is the generalized null space $\gennullspace{t+2}$.
            Thus the action of the restriction of $t+2$ to 
            $\gennullspace{t+2}$ on an associated
            string basis is $\vec{\beta}_1\mapsto\zero$.

            For $\lambda_2=4$, 
            computing the powers of $T-4I$ yields 
            \begin{equation*}
              \nullspace{t-4}=\set{\colvec{z \\ -z \\ z}
                                      \suchthat z\in\C} 
              \qquad
              \nullspace{(t-4)^2}=\set{\colvec{x \\ -z \\ z}
                                           \suchthat x,z\in\C}
            \end{equation*}
            and so the action of $t-4$ on a string basis for 
            $\gennullspace{t-4}$ is
            $\vec{\beta}_2\mapsto\vec{\beta}_3\mapsto\zero$.

            Therefore the Jordan form is  
            \begin{equation*}
              \begin{mat}[r]
                -2  &0  &0  \\
                 0  &4  &0  \\
                 0  &1  &4
              \end{mat}
            \end{equation*}
            and a suitable basis is this.
            \begin{equation*}
              B=\cat{B_{-2}}{B_4}=
                \sequence{\colvec[r]{1 \\ 1 \\ 1},
                          \colvec[r]{0 \\ -1 \\ 1},
                          \colvec[r]{-1 \\ 1 \\ -1}}
            \end{equation*}
        \partsitem The characteristic polynomial of this
            matrix is \( c(x)=(2-x)^3=-1\cdot (x-2)^3 \).
            This matrix has only a single eigenvalue, $\lambda=2$.
            By finding the powers of $T-2I$ we have  
            \begin{equation*}
              \nullspace{t-2}=\set{\colvec{-y \\ y \\ 0}
                                      \suchthat y\in\C} 
              \qquad
              \nullspace{(t-2)^2}=\set{\colvec{-y-(1/2)z \\ y \\ z}
                                          \suchthat y,z\in\C}
            \end{equation*}
            and
            \begin{equation*} 
              \nullspace{(t-2)^3}=\C^3
            \end{equation*}
            and so 
            the action of $t-2$ on an
            associated string basis is
            $\vec{\beta}_1\mapsto\vec{\beta}_2\mapsto
                   \vec{\beta}_3\mapsto\zero$.
            The Jordan form is this
            \begin{equation*}
                  \begin{mat}[r]
                    2  &0  &0  \\
                    1  &2  &0  \\
                    0  &1  &2
                  \end{mat}
            \end{equation*}
            and one choice of basis is this.
            \begin{equation*}
              B=\sequence{\colvec[r]{0 \\ 1 \\ 0},
                          \colvec[r]{7 \\ -9 \\ 4},
                          \colvec[r]{-2 \\ 2 \\ 0}}
            \end{equation*}
        \partsitem The characteristic polynomial
            \( c(x)=(1-x)^3=-(x-1)^3 \) has only a single root,
            so the matrix has only a single eigenvalue $\lambda=1$.
            Finding the powers of $T-1I$ 
            and calculating the null spaces
            \begin{equation*}
               \nullspace{t-1}=\set{\colvec{-2y+z \\ y \\ z}
                                      \suchthat y,z\in\C} 
               \qquad
               \nullspace{(t-1)^2}=\C^3 
            \end{equation*}
            shows that the action of the nilpotent map $t-1$ on a string
            basis is
            $\vec{\beta}_1\mapsto\vec{\beta}_2\mapsto\zero$ and
            $\vec{\beta}_3\mapsto\zero$.
            Therefore the Jordan form is
            \begin{equation*}
                  J=
                  \begin{mat}[r]
                    1  &0  &0  \\
                    1  &1  &0  \\
                    0  &0  &1
                  \end{mat}
            \end{equation*}
            and an appropriate basis (a string basis associated with
            $t-1$) is this.
            \begin{equation*}
              B=\sequence{\colvec[r]{0 \\ 1 \\ 0},
                          \colvec[r]{2 \\ -2 \\ -2},
                          \colvec[r]{1 \\ 0 \\ 1}}
            \end{equation*}
        \partsitem The characteristic polynomial is a bit large for by-hand
            calculation, but just manageable 
            \( c(x)=x^4-24x^3+216x^2-864x+1296=(x-6)^4 \).
            This is a single-eigenvalue map, so
            the transformation $t-6$ is nilpotent.
            The null spaces
            \begin{equation*}
               \nullspace{t-6}=\set{\colvec{-z-w \\ -z-w \\ z \\ w}
                                      \suchthat z,w\in\C} 
               \qquad
               \nullspace{(t-6)^2}=\set{\colvec{x \\ -z-w \\ z \\ w}
                                      \suchthat x,z,w\in\C} 
            \end{equation*}
            and
            \begin{equation*}
               \nullspace{(t-6)^3}=\C^4 
            \end{equation*}
            and the nullities
            show that the action of $t-6$ on a string basis is
            $\vec{\beta}_1\mapsto\vec{\beta}_2\mapsto
                   \vec{\beta}_3\mapsto\zero$ and
            $\vec{\beta}_4\mapsto\zero$.
            The Jordan form is
            \begin{equation*}
              \begin{mat}[r]
                6  &0  &0  &0  \\
                1  &6  &0  &0  \\
                0  &1  &6  &0  \\
                0  &0  &0  &6  \\
              \end{mat}
            \end{equation*}
            and finding a suitable string basis is routine.
            \begin{equation*}
              B=\sequence{\colvec[r]{0 \\ 0 \\ 0 \\ 1},
                          \colvec[r]{2 \\ -1 \\ -1 \\ 2},
                          \colvec[r]{3 \\ 3 \\ -6 \\ 3},
                          \colvec[r]{-1 \\ -1 \\ 1 \\ 0}}
            \end{equation*}
      \end{exparts}  
    \end{answer}
  \recommended \item
    Find all possible Jordan forms of a transformation with characteristic
    polynomial \( (x-1)^2(x+2)^2  \).
    \begin{answer}
      There are two eigenvalues, $\lambda_1=-2$ and $\lambda_2=1$.
      The restriction of $t+2$ to 
      $\gennullspace{t+2}$ could have either of these actions 
      on an associated string basis.
      \begin{equation*}
        \vec{\beta}_1\mapsto\vec{\beta}_2\mapsto\zero
        \qquad
        \begin{array}[t]{l}
          \vec{\beta}_1\mapsto\zero  \\
          \vec{\beta}_2\mapsto\zero 
        \end{array}
      \end{equation*}
      The restriction of $t-1$ to 
      $\gennullspace{t-1}$ could have either of these actions 
      on an associated string basis.
      \begin{equation*}
        \vec{\beta}_3\mapsto\vec{\beta}_4\mapsto\zero
        \qquad
        \begin{array}[t]{l}
          \vec{\beta}_3\mapsto\zero  \\
          \vec{\beta}_4\mapsto\zero 
        \end{array}
      \end{equation*}
      In combination, that makes four possible Jordan forms,
      the two first actions, the second and first, the first and second, and
      the two second actions.
      \begin{equation*}
        \begin{mat}[r]
          -2  &0  &0  &0  \\
           1  &-2 &0  &0  \\
           0  &0  &1  &0  \\
           0  &0  &1  &1
        \end{mat}
        \quad
        \begin{mat}[r]
          -2  &0  &0  &0  \\
           0  &-2 &0  &0  \\
           0  &0  &1  &0  \\
           0  &0  &1  &1
        \end{mat}
        \quad
        \begin{mat}[r]
          -2  &0  &0  &0  \\
           1  &-2 &0  &0  \\
           0  &0  &1  &0  \\
           0  &0  &0  &1
        \end{mat}
        \quad
        \begin{mat}[r]
          -2  &0  &0  &0  \\
           0  &-2 &0  &0  \\
           0  &0  &1  &0  \\
           0  &0  &0  &1
        \end{mat}
     \end{equation*}  
    \end{answer}
  \item 
    Find all possible Jordan forms of a transformation with characteristic
    polynomial \( (x-1)^3(x+2) \).
    \begin{answer}
     The restriction of $t+2$ to 
     $\gennullspace{t+2}$ can have only the action
     $\vec{\beta}_1\mapsto\zero$.
     The restriction of $t-1$ to $\gennullspace{t-1}$ could have any
     of these three actions on an associated string basis. 
     \begin{equation*}
        \vec{\beta}_2\mapsto\vec{\beta}_3\mapsto\vec{\beta}_4\mapsto\zero
        \qquad
        \begin{array}[t]{l}
          \vec{\beta}_2\mapsto\vec{\beta}_3\mapsto\zero  \\
          \vec{\beta}_4\mapsto\zero 
        \end{array}
        \qquad
        \begin{array}[t]{l}
          \vec{\beta}_2\mapsto\zero  \\
          \vec{\beta}_3\mapsto\zero  \\
          \vec{\beta}_4\mapsto\zero 
        \end{array}
     \end{equation*}
     Taken together there are three possible Jordan forms,
     the one arising from the first action by $t-1$ (along with the only
     action from $t+2$), the one arising from the second action, and
     the one arising from the third action.
     \begin{equation*}
       \begin{mat}[r]
         -2  &0  &0  &0  \\
          0  &1  &0  &0  \\
          0  &1  &1  &0  \\
          0  &0  &1  &1
       \end{mat}
       \quad
       \begin{mat}[r]
         -2  &0  &0  &0  \\
          0  &1  &0  &0  \\
          0  &1  &1  &0  \\
          0  &0  &0  &1
       \end{mat}
       \quad
       \begin{mat}[r]
         -2  &0  &0  &0  \\
          0  &1  &0  &0  \\
          0  &0  &1  &0  \\
          0  &0  &0  &1
       \end{mat}
     \end{equation*}
    \end{answer}
  \recommended \item
    Find all possible Jordan forms of a transformation with characteristic
    polynomial \( (x-2)^3(x+1) \) and minimal polynomial \( (x-2)^2(x+1) \).
    \begin{answer}
      The action of $t+1$ on a string basis for $\gennullspace{t+1}$
      must be $\vec{\beta}_1\mapsto\zero$. 
      Because of the power of \( x-2 \) in the minimal polynomial, a
      string basis for $t-2$ has length two and so
      the action of \( t-2 \) on \( \gennullspace{t-2} \)
      must be of this form.
      \begin{equation*}
        \begin{array}[t]{l}
          \vec{\beta}_2\mapsto\vec{\beta}_3\mapsto\zero  \\
          \vec{\beta}_4\mapsto\zero 
        \end{array}        
      \end{equation*}
      Therefore there is only one Jordan form that is possible.
      \begin{equation*}
          \begin{mat}[r]
            -1  &0  &0  &0  \\
             0  &2  &0  &0  \\
             0  &1  &2  &0  \\
             0  &0  &0  &2
          \end{mat}
       \end{equation*}
     \end{answer}
  \item 
    Find all possible Jordan forms of a transformation with characteristic
    polynomial \( (x-2)^4(x+1) \) and minimal polynomial \( (x-2)^2(x+1) \).
    \begin{answer}
      There are two possible Jordan forms.
      The action of $t+1$ on a string basis for $\gennullspace{t+1}$
      must be $\vec{\beta}_1\mapsto\zero$.
      There are two actions for $t-2$ on a string basis for
      $\gennullspace{t-2}$ that are possible with this characteristic 
      polynomial and minimal polynomial.
      \begin{equation*}
        \begin{array}[t]{l}
          \vec{\beta}_2\mapsto\vec{\beta}_3\mapsto\zero  \\
          \vec{\beta}_4\mapsto\vec{\beta}_5\mapsto\zero  
        \end{array}        
        \qquad
        \begin{array}[t]{l}
          \vec{\beta}_2\mapsto\vec{\beta}_3\mapsto\zero  \\
          \vec{\beta}_4\mapsto\zero                      \\
          \vec{\beta}_5\mapsto\zero                      
        \end{array}        
      \end{equation*}
      The resulting Jordan form matrices are these. 
      \begin{equation*}
        \begin{mat}[r]
          -1  &0  &0  &0  &0  \\
           0  &2  &0  &0  &0  \\
           0  &1  &2  &0  &0  \\
           0  &0  &0  &2  &0  \\
           0  &0  &0  &1  &2
        \end{mat}
        \qquad
        \begin{mat}[r]
          -1  &0  &0  &0  &0  \\
           0  &2  &0  &0  &0  \\
           0  &1  &2  &0  &0  \\
           0  &0  &0  &2  &0  \\
           0  &0  &0  &0  &2
        \end{mat}
     \end{equation*}  
    \end{answer}
  \recommended \item Diagonalize these.
    \begin{exparts*}
       \partsitem \( \begin{mat}[r]
                  1  &1  \\
                  0  &0
                \end{mat}  \)
       \partsitem \( \begin{mat}[r]
                  0  &1  \\
                  1  &0
                \end{mat}  \)
    \end{exparts*}
    \begin{answer}
      \begin{exparts}
        \partsitem The characteristic polynomial is \( c(x)=x(x-1) \).
          For $\lambda_1=0$ we have
          \begin{equation*}
            \nullspace{t-0}=\set{\colvec{-y \\ y}
                                 \suchthat y\in\C }  
          \end{equation*} 
          (of course, the null space of $t^2$ is the same).
          For $\lambda_2=1$,
          \begin{equation*}
            \nullspace{t-1}=\set{\colvec{x \\ 0}
                                     \suchthat x\in\C }  
          \end{equation*}
          (and the null space of $(t-1)^2$ is the same).
          We can take this basis
          \begin{equation*}
            B=\sequence{\colvec[r]{1 \\ -1},\colvec[r]{1 \\ 0}}
          \end{equation*}
          to get the diagonalization.
          \begin{equation*}
            \begin{mat}[r]
              1  &1  \\
             -1  &0
            \end{mat}^{-1}
            \begin{mat}[r]
              1  &1  \\
              0  &0
            \end{mat}
            \begin{mat}[r]
              1  &1  \\
             -1  &0
            \end{mat}
            =
            \begin{mat}[r]
              0  &0  \\
              0  &1
            \end{mat}
          \end{equation*}
        \partsitem The characteristic polynomial is 
          \( c(x)=x^2-1=(x+1)(x-1) \).
          For $\lambda_1=-1$,
          \begin{equation*}
            \nullspace{t+1}=\set{\colvec{-y \\ y}
                                    \suchthat y\in\C } 
          \end{equation*}
          and the null space of $(t+1)^2$ is the same.
          For $\lambda_2=1$ 
          \begin{equation*}
            \nullspace{t-1}=\set{\colvec{y \\ y}
                                    \suchthat y\in\C } 
          \end{equation*}
          and the null space of $(t-1)^2$ is the same.
          We can take this basis
          \begin{equation*}
            B=\sequence{\colvec[r]{1 \\ -1},\colvec[r]{1 \\ 1}}
          \end{equation*}
          to get a diagonalization.
          \begin{equation*}
            \begin{mat}[r]
              1  &1  \\
              1  &-1
            \end{mat}^{-1}
            \begin{mat}[r]
              0  &1  \\
              1  &0
            \end{mat}
            \begin{mat}[r]
              1   &1  \\
              -1  &1
            \end{mat}
            =
            \begin{mat}[r]
              -1  &0  \\
              0   &1
            \end{mat}
          \end{equation*}
      \end{exparts}  
     \end{answer}
  \recommended \item 
    Find the Jordan matrix representing the differentiation
    operator on \( \polyspace_3 \).
    \begin{answer}
      The transformation $\map{d/dx}{\polyspace_3}{\polyspace_3}$ 
      is nilpotent.
      Its action on the basis \( B=\sequence{x^3,3x^2,6x,6} \)
      is $x^3\mapsto 3x^2\mapsto 6x\mapsto 6\mapsto 0$.
      Its Jordan form is its canonical form as a nilpotent matrix.
      \begin{equation*}
         J=
         \begin{mat}[r]
           0  &0  &0  &0  \\
           1  &0  &0  &0  \\
           0  &1  &0  &0  \\
           0  &0  &1  &0
         \end{mat}
      \end{equation*}
    \end{answer}
   \recommended \item 
      Decide if these two are similar.
      \begin{equation*}
         \begin{mat}[r]
            1  &-1 \\
            4  &-3 \\
         \end{mat}
         \qquad
         \begin{mat}[r]
           -1  &0  \\
            1  &-1 \\
         \end{mat}
      \end{equation*}
      \begin{answer}
        Yes.
        Each has the characteristic polynomial $(x+1)^2$.
        Calculations of the powers of $T_1+1\cdot I$ and 
        $T_2+1\cdot I$ gives these two.
        \begin{equation*}
          \nullspace{t_1+1}=\set{\colvec{y/2 \\ y} \suchthat y\in\C}
          \qquad
          \nullspace{t_2+1}=\set{\colvec{0 \\ y} \suchthat y\in\C}
        \end{equation*}
        (Of course, for each the null space of the square is 
        the entire space.)
        The way that the nullities rise shows that each is  
        similar to this Jordan form matrix
        \begin{equation*}
           \begin{mat}[r]
             -1  &0  \\
              1  &-1 \\
           \end{mat}
        \end{equation*}
        and they are therefore similar to each other.  
      \end{answer}
  \item 
     Find the Jordan form of this matrix.
     \begin{equation*}
        \begin{mat}[r]
           0  &-1  \\
           1  &0
        \end{mat}
     \end{equation*}
     Also give a Jordan basis.
     \begin{answer}
       Its characteristic polynomial is
       \( c(x)=x^2+1 \) which has complex roots
       \( x^2+1=(x+i)(x-i) \).
       Because the roots are distinct,
       the matrix is diagonalizable and its Jordan form is that
       diagonal matrix. 
       \begin{equation*}
         \begin{mat}[r]
           -i  &0  \\
            0  &i
         \end{mat}
       \end{equation*}  
       To find an associated basis we compute the null spaces.
       \begin{equation*}
         \nullspace{t+i}=\set{\colvec{-iy \\ y}
                                        \suchthat y\in\C} 
         \qquad
         \nullspace{t-i}=\set{\colvec{iy \\ y}
                                        \suchthat y\in\C} 
       \end{equation*}
       For instance, 
       \begin{equation*}
         T+i\cdot I=
         \begin{mat}[r]
           i  &-1  \\
           1  &i
         \end{mat}
       \end{equation*}
       and so we get a description of the null space of $t+i$ by solving
       this linear system.
       \begin{equation*}
         \begin{linsys}{2}
           ix  &-  &y  &=  &0  \\
            x  &+  &iy &=  &0
         \end{linsys}
         \grstep{i\rho_1+\rho_2}
         \begin{linsys}{2}
           ix  &-  &y  &=  &0  \\
               &   &0  &=  &0
         \end{linsys}
       \end{equation*}
       (To change the relation $ix=y$ so that the leading variable $x$ is
       expressed in terms of the free variable $y$, we can multiply both
       sides by $-i$.)

       As a result, one such basis is this.
       \begin{equation*}
         B=\sequence{\colvec[r]{-i \\ 1},
                     \colvec[r]{i \\ 1}}
       \end{equation*}  
     \end{answer}
  \item 
    How many similarity classes are there for \( \nbyn{3} \) matrices
    whose only eigenvalues are \( -3 \) and \( 4 \)?
    \begin{answer}
     We can count the possible classes by counting the possible
     canonical representatives, that is, the possible Jordan form matrices.
     The characteristic polynomial must be either $c_1(x)=(x+3)^2(x-4)$
     or $c_2(x)=(x+3)(x-4)^2$.
     In the $c_1$ case there are two possible actions 
     of $t+3$ on a string basis for $\gennullspace{t+3}$.
     \begin{equation*}
       \vec{\beta}_1\mapsto\vec{\beta}_2\mapsto \zero
       \qquad
       \begin{array}[t]{l}
         \vec{\beta}_1\mapsto\zero \\
         \vec{\beta}_2\mapsto\zero 
       \end{array}
     \end{equation*}
     There are two associated Jordan form matrices.
     \begin{equation*}
        \begin{mat}[r]
          -3  &0  &0  \\
           1  &-3 &0  \\
           0  &0  &4
        \end{mat}
        \qquad
        \begin{mat}[r]
          -3  &0  &0  \\
           0  &-3 &0  \\
           0  &0  &4
        \end{mat}
      \end{equation*}
      Similarly there are two Jordan form matrices that could arise
      out of $c_2$.
      \begin{equation*}
        \begin{mat}[r]
          -3  &0  &0  \\
           0  &4  &0  \\
           0  &1  &4
        \end{mat}
        \qquad
        \begin{mat}[r]
          -3  &0  &0  \\
           0  &4  &0  \\
           0  &0  &4
        \end{mat}
     \end{equation*}  
     So in total there are four possible Jordan forms.
    \end{answer}
  \recommended \item
    Prove that a matrix is diagonalizable if and only if its minimal
    polynomial has only linear factors.
    \begin{answer}
       Jordan form is unique.
       A diagonal matrix is in Jordan form.
       Thus the Jordan form of a diagonalizable matrix is its diagonalization.
       If the minimal polynomial has factors to some power higher than one
       then the Jordan form has subdiagonal \( 1 \)'s, and so is not
       diagonal.  
     \end{answer}
  \item 
    Give an example of a linear transformation on a vector
    space that has no non-trivial invariant subspaces.
    \begin{answer}
      One example is the transformation of \( \C \) that
       sends \( x \) to \( -x \).  
     \end{answer}
  \item 
    Show that a subspace is \( t-\lambda_1 \) invariant if and only if
    it is \( t-\lambda_2 \) invariant.
    \begin{answer}
      Apply \nearbylemma{le:tInvIfftMinLambdaInv} twice;
      the subspace is $t-\lambda_1$~invariant if and only if it is 
      $t$~invariant, which in turn holds if and only if it is 
      $t-\lambda_2$~invariant.  
    \end{answer}
  \item 
     Prove or disprove: two \( \nbyn{n} \) matrices are
     similar if and only if they have the same characteristic and
     minimal polynomials.
     \begin{answer}
       False; these two $\nbyn{4}$ matrices each have $c(x)=(x-3)^4$
       and $m(x)=(x-3)^2$.
       \begin{equation*}
          \begin{mat}[r]
             3  &0  &0  &0  \\
             1  &3  &0  &0  \\
             0  &0  &3  &0  \\
             0  &0  &1  &3
          \end{mat}
          \quad
          \begin{mat}[r]
             3  &0  &0  &0  \\
             1  &3  &0  &0  \\
             0  &0  &3  &0  \\
             0  &0  &0  &3
          \end{mat}
       \end{equation*} 
     \end{answer}
  \item 
    The \definend{trace}\index{trace}\index{matrix!trace} 
    of a square matrix is the sum of its diagonal entries.
    \begin{exparts}
       \partsitem Find the formula for the characteristic polynomial of
         a $\nbyn{2}$ matrix.
       \partsitem Show that 
         trace is invariant under similarity, and so we can sensibly
         speak of the `trace of a map'.\index{linear map!trace}
         (\textit{Hint:}  see the prior item.)
       \partsitem Is trace invariant under matrix equivalence?
       \partsitem Show that the trace of a map is the sum of its eigenvalues
         (counting multiplicities).
       \partsitem Show that the trace of a nilpotent map is zero.
         Does the converse hold?
    \end{exparts}
    \begin{answer}
      \begin{exparts}
         \partsitem The characteristic polynomial is this. 
           \begin{equation*}
             \begin{vmat}
               a-x  &b  \\
               c  &d-x
             \end{vmat}
             =(a-x)(d-x)-bc=ad-(a+d)x+x^2-bc
             =x^2-(a+d)x+(ad-bc)
           \end{equation*}
           Note that the determinant appears as the constant term.
         \partsitem Recall that the characteristic polynomial
            \( \deter{T-xI} \) is invariant under similarity.
            Use the permutation expansion formula to show that the trace
            is the negative of the coefficient of \( x^{n-1} \).
         \partsitem No, there are matrices $T$ and $S$ that are
            equivalent $S=PTQ$ (for some nonsingular $P$ and $Q$)
            but that have different traces.
            An easy example is this.
            \begin{equation*}
               PTQ=
               \begin{mat}[r]
                  2  &0  \\
                  0  &1
               \end{mat}
               \begin{mat}[r]
                  1  &0  \\
                  0  &1
               \end{mat}
               \begin{mat}[r]
                  1  &0  \\
                  0  &1
               \end{mat}
               =
               \begin{mat}[r]
                  2  &0  \\
                  0  &1
               \end{mat}
            \end{equation*}
            Even easier examples using $\nbyn{1}$ matrices are possible.
         \partsitem Put the matrix in Jordan form.
            By the first item, the trace is unchanged.
         \partsitem The first part is easy; use the third item.
            The converse does not hold:~this matrix
            \begin{equation*}
               \begin{mat}[r]
                  1  &0  \\
                  0  &-1
               \end{mat}
            \end{equation*}
            has a trace of zero but is not nilpotent.
       \end{exparts}  
     \end{answer}
  \item 
    To use \nearbydefinition{def:invariant} to check whether a subspace
    is $t$~invariant, we seemingly have to check all of the infinitely many
    vectors in a (nontrivial) subspace to see if they satisfy the condition.
    Prove that a subspace is \( t \)~invariant if and only if its subbasis
    has the property that for all of its elements, $t(\vec{\beta})$ is in 
    the subspace.
    \begin{answer}
      Suppose that \( B_M \) is a basis for a subspace \( M \) of some vector
      space.
      Implication one way is clear; if \( M \) is \( t \) invariant then
      in particular, if \( \vec{m}\in B_M \) then \( t(\vec{m})\in M \).
      For the other implication, let
      \( B_M=\sequence{\vec{\beta}_1,\dots,\vec{\beta}_q} \) and note that
      \( t(\vec{m})=t(m_1\vec{\beta}_1+\dots+m_q\vec{\beta}_q)
                   =m_1t(\vec{\beta}_1)+\dots+m_qt(\vec{\beta}_q) \)
      is in \( M \) as any subspace is closed under linear
      combinations. 
    \end{answer}
  \recommended \item 
    Is \( t \) invariance preserved under intersection?
    Under union?
    Complementation?
    Sums of subspaces?
    \begin{answer}
      Yes, the intersection 
      of $t$ invariant subspaces is $t$~invariant.
      Assume that \( M \) and \( N \) are \( t \) invariant.
      If \( \vec{v}\in M\intersection N \) then \( t(\vec{v})\in M \)
      by the invariance of \( M \) and \( t(\vec{v})\in N \) by the
      invariance of \( N \).

      Of course, the union of two subspaces need not be a subspace
      (remember that the $x$-\hbox{} and $y$-axes are subspaces of the plane
      $\Re^2$ but the union of the two axes fails to be closed
      under vector addition; for instance it does not contain
      $\vec{e}_1+\vec{e}_2$.)
      However, the union of invariant subsets is an invariant subset; if
      \( \vec{v}\in M\union N \) then \( \vec{v}\in M \) or \( \vec{v}\in N \)
      so \( t(\vec{v})\in M \) or \( t(\vec{v})\in N \).

      No, the complement of an invariant subspace need not be invariant.
      Consider the subspace
      \begin{equation*}
        \set{\colvec{x \\ 0}\suchthat x\in\C}
      \end{equation*}
      of \( \C^2 \) under the zero transformation.

      Yes, the sum of two invariant subspaces is invariant.
      The check is easy.  
    \end{answer}
  \item  \label{exer:OrderC}
     Give a way to order the Jordan blocks if some of the eigenvalues
     are complex numbers.
     That is, suggest a reasonable ordering for the complex numbers.
     \begin{answer}
       One such ordering is the \definend{dictionary ordering}.
       Order by the real component first, then by the coefficient of \( i \).
       For instance, \( 3+2i<4+1i \) but \( 4+1i<4+2i \).  
     \end{answer}
  \item
    Let \( \polyspace_j(\Re) \) be the vector space over
    the reals of degree \( j \) polynomials.
    Show that if \( j\le k \) then \( \polyspace_j(\Re) \) is an invariant
    subspace of \( \polyspace_k(\Re) \) under the differentiation operator.
    In \( \polyspace_7(\Re) \), does any of \( \polyspace_0(\Re) \),
    \ldots, \( \polyspace_6(\Re) \) have an invariant complement?
    \begin{answer}
     The first half is easy\Dash the derivative of any real polynomial is
      a real polynomial of lower degree.
      The answer to the second half is `no'; any complement of
      \( \polyspace_j(\Re) \) must include a polynomial of degree \( j+1 \),
      and the derivative of that polynomial is in \( \polyspace_j(\Re)\).  
     \end{answer}
  \item
    In \( \polyspace_n(\Re) \), the vector space (over the
    reals) of degree \( n \) polynomials,
    \begin{equation*}
      \mathcal{E}=
      \set{p(x)\in\polyspace_n(\Re)\suchthat p(-x)=p(x) \text{\ for all\ }x}
    \end{equation*}
    and
    \begin{equation*}
      \mathcal{O}=
      \set{p(x)\in\polyspace_n(\Re)\suchthat p(-x)=-p(x) \text{\ for all\ }x}
    \end{equation*}
    are the \definend{even}\index{even polynomials}\index{polynomial!even} 
    and the \definend{odd}\index{even polynomials}\index{polynomial!even}
     polynomials; \( p(x)=x^2 \) is
    even while \( p(x)=x^3 \) is odd.
    Show that they are subspaces.
    Are they complementary?
    Are they invariant under the differentiation transformation?
    \begin{answer}
      For the first half, show that each is a subspace and then observe
      that any polynomial can be uniquely
       written as the sum of even-powered and odd-powered terms (the
       zero polynomial is both).
       The answer to the second half is `no': \( x^2 \) is even while
       \( 2x \) is odd.  
     \end{answer}
  % Lemma gone when moved to new JF presentation
  % \item 
  %   \nearbylemma{le:InvCompSubspSplitTrans} says that if \( M \) and
  %   \( N \) are
  %   invariant complements then \( t \) has a representation in the given
  %   block form (with respect to the same ending as starting basis, of course).
  %   Does the implication reverse?
  %   \begin{answer}
  %     Yes.
  %     If \( \rep{t}{B,B} \) has the given block form, take \( B_M \) to
  %     be the first \( j \) vectors of \( B \), where \( J \) is the
  %     \( \nbyn{j} \) upper left submatrix.
  %     Take \( B_N \) to be the remaining \( k \) vectors in \( B \).
  %     Let \( M \) and \( N \) be the spans of \( B_M \) and \( B_N \).
  %     Clearly \( M \) and \( N \) are complementary.
  %     To see \( M \) is invariant (\( N \) works the same way), represent
  %     any \( \vec{m}\in M \) with respect to \( B \), note the last
  %     \( k \) components are zeroes, and multiply by the given block
  %     matrix.
  %     The final \( k \) components of the result are zeroes, so that
  %     result is again in \( M \). 
  %    \end{answer}
   \item 
     A matrix \( S \) is the \definend{square root}\index{square root}
     of another \( T \) if \( S^2=T \).
     Show that any nonsingular matrix has a square root.
     \begin{answer}
         Put the matrix in Jordan form.
         By non-singularity, there are no zero eigenvalues on the diagonal.
         Ape this example:
          \begin{equation*}
             \begin{mat}[r]
                9  &0  &0 \\
                1  &9  &0 \\
                0  &0  &4
             \end{mat}
             =
             \begin{mat}[r]
                3  &0  &0 \\
               1/6 &3  &0 \\
                0  &0  &2
             \end{mat}^2
          \end{equation*}
         to construct a square root.
         Show that it holds up under similarity: if \( S^2=T \) then
         \( (PSP^{-1})(PSP^{-1})=PTP^{-1} \). 
    \end{answer}
\index{Jordan form|)}
\end{exercises}

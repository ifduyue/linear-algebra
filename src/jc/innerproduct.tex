% Chapter 5, Topic _Linear Algebra_ Jim Hefferon
%  https://hefferon.net/linearalgebra
%  2021-Jul-28
\topic{Inner Product}
\index{Inner Product|(}

The first chapter covers the dot product operation. 
It inputs a pair of vectors from some $\R^n$ and outputs a scalar.
\begin{equation*}
  \colvec{-1 \\ 2 \\ 3}\dotprod\colvec{1 \\ 0 \\ 3}
  =-1\cdot 1+2\cdot 0+3\cdot 3
  =8
\end{equation*}
In that chapter we did not yet have the term `linear', but
this operation both respects addition
\begin{equation*}
 (\vec{v}_1+\vec{v}_2)\dotprod\vec{w}=\vec{v}_1\dotprod\vec{w}
                                      +\vec{v}_2\dotprod\vec{w}
\qquad
 \vec{v}\dotprod(\vec{w}_1+\vec{w}_2)=\vec{v}\dotprod\vec{w}_1
                                      +\vec{v}\dotprod\vec{w}_2
\end{equation*}
and scalar multiplication.
\begin{equation*}
 (r\vec{v})\dotprod\vec{w}=r(\vec{v}\dotprod\vec{w})
  \qquad
 \vec{v}\dotprod(r\vec{w})=r(\vec{v}\dotprod\vec{w})
\end{equation*}
Dot product describes the geometry of
$\Re^n$; for instance, two vectors are perpendicular if and only if their
dot product is zero, and the length of a vector is determined by  
$\vec{v}\dotprod\vec{v}=\absval{\vec{v}}^2$.

Here we will extend the dot product and length operations 
to other vector spaces. 
Instead of a dot, the traditional mathematical notation is
$\innerprod{\vec{v}}{\vec{w}}$.
Our goal is to understand the transformations of complex vector spaces
that preserve length, that satisfy $\absval{t(\vec{v})}=\absval{\vec{v}}$.

We need more on complex numbers than we reviewed in this
chapter's opening section.
There, we described~$\C$ as 
the collection of $z=a+bi$ where $a$ and~$b$ are real numbers,
and where $i^2=-1$. 
We also covered addition, scalar multiplication,
subtraction, and multiplication, as well as length.
As for division,  consider $(1+2i)/(3+4i)$.
The difference of squares formula
gives $(a+bi)(a-bi)=a^2-(bi)^2=a^2-(-b^2)=a^2+b^2$,
which is a real number,
so
by multiplying the quotient's numerator and denominator by $3-4i$
we get an expression of the form $a+bi$.
\begin{equation*}
  \frac{1+2i}{3+4i}\cdot\frac{3-4i}{3-4i}
  =
  \frac{(1\cdot 3-2\cdot 4)+(1\cdot 4+2\cdot 3)i}{3^2+4^2}
  =
  \frac{-5+10i}{25}
  =
  \frac{-1}{5}+\frac{2}{5}\,i
\end{equation*}
Consequently, we define the \definend{conjugate} of a complex number~$z$
to be $\compconj{z}=\compconj{a+bi}=a-bi$, so that $z\compconj{z}=\absval{z}^2$.
Then dividing by $\absval{z}^2$ gives $z\compconj{z}/\absval{z}^2=1$,
and so  
every nonzero complex number has a multiplicative inverse,
$z^{-1}=\compconj{z}/\absval{z}^2$.
% , and thus $\absval{z}=1$ if and only if $z^{-1}=\compconj{z}$.
Observe that a complex number is a real number if and only if 
it equals its conjugate, 
that $\compconj{z_1+z_2}=\compconj{z_1}+\compconj{z_2}$,
and that $\compconj{z_1\cdot z_2}=\compconj{z_1}\cdot\compconj{z_2}$.
Plotting $z=a+bi$ on the 
complex plane\index{complex plane} 
illustrates the conjugate.
\begin{center}
  \includegraphics{jc/asy/innerproduct000.pdf}
\end{center}
This picture makes clear that 
$z+\compconj{z}=(a+bi)+(a-bi)$ equals
$2a$, which we write as $2\RE{z}$.
It also shows the \definend{argument}, $\arg(z)$, the angle that
$z$ makes with the real axis.

We have seen many examples of vector spaces that are far removed from
just collections of column vectors.
One example is a vector space of polynomials, where it is not immediately
clear, given a member~$\vec{v}$,
what would be a useful definition of its length, 
$\sqrt{\innerprod{\vec{v}}{\vec{v}}}$.
For some insight into the right direction, consider the complex
vector spaces~$\C^n$.

Based on dot product, the natural first guess is to try defining 
$\innerprod{\vec{v}}{\vec{w}}$
as the linear combination of the components, so that in the two-dimensional 
case
\begin{equation*}
  \vec{v}=\colvec{a_{1,1}+b_{1,1}i \\ a_{2,1}+b_{2,1}i}
  \qquad
  \vec{w}=\colvec{a_{1,2}+b_{1,2}i \\ a_{2,2}+b_{2,2}i}
\end{equation*}
the guess for $\innerprod{\vec{v}}{\vec{w}}$ is 
$(a_{1,1}+b_{1,1}i)\cdot(a_{1,2}+b_{1,2}i)+(a_{2,1}+b_{2,1}i)\cdot(a_{2,2}+b_{2,2}i)$.
But that won't do.
We are investigating lengths,
so we want that $\innerprod{\vec{v}}{\vec{v}}=\absval{\vec{v}}^2\!$.
Thus, we don't want that $\innerprod{\vec{v}}{\vec{v}}$ equals
$(a_1+b_1i)(a_1+b_1i)+(a_2+b_2i)(a_2+b_2i)$,
instead we want something like
$(a_1+b_1i)(a_1-b_1i)+(a_2+b_2i)(a_2-b_2i)$.
Consequently, this our definition in the two-dimensional case.
\begin{equation*}
  \innerprod{\vec{v}}{\vec{w}}
  =(a_{1,1}+b_{1,1}i)\cdot(a_{1,2}-b_{1,2}i)\,+\,(a_{2,1}+b_{2,1}i)\cdot(a_{2,2}-b_{2,2}i)
\end{equation*}
The 
\definend{Hermitian inner product} 
is:\index{inner product!Hermitian}\index{Hermitian inner product}
if two members  $\vec{v},\vec{w}$ of a complex vector space 
have $i$-th components $v_i$ and $w_i$, then
$\innerprod{v}{w}=v_1\compconj{w}_1+\cdots+v_n\compconj{w}_n$.
Observe that this extends the dot product in that
if we fall back to reals, taking the $b$'s be zero, then
we get $\vec{v}\dotprod\vec{w}$.

This operation inputs two vectors but outputs a scalar.
Here is an example.
\begin{align*}
  \innerprod{\colvec{1+2i \\ 3+4i}}{\colvec{5+6i \\ 7+8i}}
  &=(1+2i)(5-6i)+(3+4i)(7-8i)                     \\[-2ex]
  &=(17+4i)+(53+4i)=70+8i
\end{align*}

Hermitian inner product is linear in the first input:~checking that
$\innerprod{\vec{v}_1+\vec{v}_2}{\vec{w}}
 = \innerprod{\vec{v}_1}{\vec{w}}
   +\innerprod{\vec{v}_2}{\vec{w}}$
and that
$\innerprod{z\cdot\vec{v}}{\vec{w}}
     =z\cdot\innerprod{\vec{v}}{\vec{w}}$
is straightforward.
But it is not linear in the second because
while
$\innerprod{\vec{v}}{\vec{w}_1+\vec{w}_2}
 = \innerprod{\vec{v}}{\vec{w}_1}
   +\innerprod{\vec{v}}{\vec{w}_2}$
holds, for scalar multiplication a conjugation appears:
$\innerprod{\vec{v}}{z\cdot\vec{w}}
     =\compconj{z}\cdot\innerprod{\vec{v}}{\vec{w}}$.

Hermitian inner product is one way, in a particular set of vector spaces, 
to extend dot product.
There are lots of others, in lots of spaces.
But we require that our extensions lead to a reasonable notion of length.
For that,
an \definend{inner product} is an operation
that inputs two vectors from a real or complex vector space 
and outputs a scalar, and that
satisfies three conditions.
Condition~(1) is that it is linear in the first input, so that
$\innerprod{\vec{v}_1+\vec{v}_2}{\vec{w}}
 = \innerprod{\vec{v}_1}{\vec{w}}
   +\innerprod{\vec{v}_2}{\vec{w}}$
and $\innerprod{z\vec{v}}{\vec{w}}
     =z\innerprod{\vec{v}}{\vec{w}}$.
(Some presentations take inner product 
to be linear in the second input; more on that
at the end.)
Condition~(2) is that
$\innerprod{\vec{w}}{\vec{v}}=\compconj{\innerprod{\vec{v}}{\vec{w}}}$.
% (this makes the operation  \definend{conjugate symmetric}).
Observe that taking both vectors to be 
$\vec{v}$ gives that $\innerprod{\vec{v}}{\vec{v}}$ equals its conjugate, and
so is a real number.
Condition~(3) says about that real number that
$\innerprod{\vec{v}}{\vec{v}}\geq 0$ for all $\vec{v}$, with 
$\innerprod{\vec{v}}{\vec{v}}= 0$ if and only if $\vec{v}=\zero$.
% (this makes the operation \definend{positive definite}).

A vector space that has an inner product operation is an
\definend{inner product space}.
The dot product operation turns any of the real vector spaces $\R^n$ into
an inner product space. 
Hermitian inner product makes any of the complex vector spaces~$\C^n$ into 
an inner product space (verifications are in the exercises).

An example which looks quite different 
is the real vector space whose members are
real valued functions that are continuous on the 
closed interval $\closedinterval{0}{1}$.
Let the inner product be this.
\begin{equation*}
  \innerprod{f}{g}=\int_{0}^1 f(t)\cdot g(t)\,dt
\end{equation*}
This definition satisfies
condition~(1) because  
$\int_{0}^1 (f_1(t)+f_2(t))\cdot g(t)\,dt$ equals 
$\int_{0}^1 f_1(t)g(t)\,dt+\int_{0}^1 f_2(t)g(t)\,dt$
by a result from elementary Calculus,
and a similar result holds for scalar multiplication. 
It satisfies condition~(2) because for real numbers conjugation has no effect,
and $f(t)g(t)$ equals $g(t)f(t)$. 
For condition~(3), if~$f$ is not the zero function then 
the square $f(t)\cdot f(t)$ is greater than~$0$ for some~$\hat{t}$.
By continuity then, the square is greater than zero for a whole subinterval
around~$\hat{t}$,
and so the area under the curve is greater than~$0$.

% Smash the depth, not the height
\def\smashdp#1{{\setbox0=\hbox{$#1$}\dp0=0pt \box0\relax}}
We generalize length with the operation 
$\norm{\vec{v}}=\sqrt{\smashdp{\innerprod{\vec{v}}{\vec{v}}}}$.
We will show that this operation is a \definend{norm}\index{norm} 
because it satisfies the
three conditions that we expect of a kind of length: 
(1)~the norm of every vector is a real-number, $\norm{\vec{v}}\geq 0$, 
and $\norm{\vec{v}}= 0$ if and only if $\vec{v}=\zero$, 
(2)~rescaling a vector will rescale 
its norm, so that $\norm{z\vec{v}}=\absval{z}\cdot\norm{\vec{v}}$, and 
(3)~the Triangle Inequality,\index{Triangle Inequality} 
$\norm{\vec{v}+\vec{w}}\leq\norm{\vec{v}}+\norm{\vec{w}}$.

We already covered the first condition earlier.
To verify the second, 
$\norm{z\vec{v}}
=\sqrt{\smashdp{\innerprod{z\vec{v}}{z\vec{v}}}}
=\sqrt{\smashdp{z\compconj{z}\cdot\innerprod{\vec{v}}{\vec{v}}}}
=\sqrt{z\compconj{z}}\cdot\sqrt{\smashdp{\innerprod{\vec{v}}{\vec{v}}}}
=\absval{z}\cdot\sqrt{\smashdp{\innerprod{\vec{v}}{\vec{v}}}}$.

To verify the third condition we will again, as in the first
chapter, relate it to the Cauchy-Schwartz inequality,
$\innerprod{\vec{v}}{\vec{w}}\leq \norm{\vec{v}}\cdot\norm{\vec{w}}$.
The proof in this context is an extension of, but more involved than, the 
proof in the first chapter. 
Consider the difference $\vec{v}-z\cdot\vec{w}$ as $z\in\C$ varies.
This vector's norm is a nonnegative real,
$0\leq \norm{\vec{v}-z\cdot\vec{w}}^2
   =\innerprod{\vec{v}-z\vec{w}}{\vec{v}-z\vec{w}}$.
Linearity gives
$\innerprod{\vec{v}}{\vec{v}-z\vec{w}}
  -z\cdot\innerprod{\vec{w}}{\vec{v}-z\vec{w}}$
and then
$\innerprod{\vec{v}}{\vec{v}}
  -\compconj{z}\cdot\innerprod{\vec{v}}{\vec{w}}
  -z\cdot\innerprod{\vec{w}}{\vec{v}}
  +z\compconj{z}\cdot\innerprod{\vec{w}}{\vec{w}}$.
We have this.
\begin{equation*}
 0
  \leq\innerprod{\vec{v}}{\vec{v}}
  -\compconj{z}\cdot\innerprod{\vec{v}}{\vec{w}}
  -z\cdot\compconj{\innerprod{\vec{v}}{\vec{w}}}
  +z\compconj{z}\cdot\innerprod{\vec{w}}{\vec{w}}
\end{equation*}

Take 
$z=\innerprod{\vec{v}}{\vec{w}}/\innerprod{\vec{w}}{\vec{w}}$
(if $\vec{w}=\zero$ then the Cauchy-Schwartz inequality is trivial, so we
ignore this case).
Its denominator is a real number so
$\compconj{z}=\compconj{\innerprod{\vec{v}}{\vec{w}}}/\innerprod{\vec{w}}{\vec{w}}$,
and thus plugging it into the second term gives 
$\compconj{\innerprod{\vec{v}}{\vec{w}}}\innerprod{\vec{v}}{\vec{w}}/\innerprod{\vec{w}}{\vec{w}}$.
Plugging into the third term gives the same,
as does plugging into the last term.
Combining the three like terms gives this.
\begin{equation*}
 0
  \leq\innerprod{\vec{v}}{\vec{v}}
  -\frac{\innerprod{\vec{v}}{\vec{w}}\compconj{\innerprod{\vec{v}}{\vec{w}}}}{\innerprod{\vec{w}}{\vec{w}}}
  =\norm{\vec{v}}^2
  -\frac{\absval{\innerprod{\vec{v}}{\vec{w}}}^2}{\norm{\vec{w}}^2}
\end{equation*}
Rearranging and clearing the denominator gives the desired 
inequality.

With that, we can prove the Triangle Inequality.
Again, this extends the argument in the first chapter.
\begin{align*}
  \norm{\vec{v}+\vec{w}}^2
  =\innerprod{\vec{v}+\vec{w}}{\vec{v}+\vec{w}}  
  &=\innerprod{\vec{v}}{\vec{v}}
    +\innerprod{\vec{v}}{\vec{w}}  
    +\innerprod{\vec{w}}{\vec{v}}  
    +\innerprod{\vec{w}}{\vec{w}}   \\ 
  &=\norm{\vec{v}}^2
    +\innerprod{\vec{v}}{\vec{w}}  
    +\compconj{\innerprod{\vec{v}}{\vec{w}}}  
    +\norm{\vec{w}}^2              \\   
  &=\norm{\vec{v}}^2
    +2\cdot\RE{\innerprod{\vec{v}}{\vec{w}}}  
    +\norm{\vec{w}}^2      \\
  &\leq\norm{\vec{v}}^2
    +2\absval{\innerprod{\vec{v}}{\vec{w}}}  
    +\norm{\vec{w}}^2     \\
% \end{align*}
\intertext{Apply Cauchy-Schwartz.}
  &\leq\norm{\vec{v}}^2
    +2\cdot\norm{\vec{v}}\cdot\norm{\vec{w}}
    +\norm{\vec{w}}^2      
  =(\norm{\vec{v}}+\norm{\vec{w}})^2         
\end{align*}
Taking the square root of both sides gives the result.

A familiar example of a norm in the vector space~$\R^n$ is the ordinary
Euclidean length, $\norm{\vec{v}}=\sqrt{v_1^2+\cdots+v_n^2}$.
Another example, on the real vector space of matrices
$\matspace_{\nbym{n}{m}}$, is that $\norm{M}$ is the largest absolute value, 
over all of the entries in the matrix.  
Condition~(1) is clear, as is condition~(2).
Condition~(3) is the ordinary Triangle Inequality for reals, that
$\absval{a+b}\leq\absval{a}+\absval{b}$.

Still another example of a norm operation 
is on the vector space of continuous real-valued functions 
on the closed interval $\closedinterval{0}{1}$.  
\begin{equation*}
  \norm{f}
  =\int_{x=0}^1 \absval{f(x)}\,dx
  \tag{$*$}
\end{equation*}
This satisfies the conditions to be a norm.

We pause to see one more connection between inner products, norm, and geometry.
Here is the familiar parallelogram diagram for the sum of two vectors,
where besides $\vec{v}+\vec{w}$ we also show $\vec{v}-\vec{w}$. 
\begin{center}
  \includegraphics{jc/asy/innerproduct001.pdf}
\end{center}
% Applied to the two labeled angles, the Law of Cosines gives 
% $\absval{\vec{v}+\vec{w}}^2=\absval{\vec{v}}^2+\absval{\vec{w}}^2+2\absval{\vec{v}}\absval{\vec{w}}\cos(A)$
% and
% $\absval{\vec{v}-\vec{w}}^2=\absval{\vec{v}}^2+\absval{\vec{w}}^2+2\absval{\vec{v}}\absval{\vec{w}}\cos(B)$.
% Recall from geometry that in a parallelogram, adjacent angles are supplementary.
% This means that $\cos(B)=-\cos(A)$.
% Therefore, adding the two equations gives  
We will prove the 
\definend{parallelogram identity}.\index{parallelogram identity}
$\absval{\vec{v}+\vec{w}}^2+\absval{\vec{v}-\vec{w}}^2=2\absval{\vec{v}}^2+2\absval{\vec{w}}^2$
(This has the nice statement that in a parallelogram the sum of the squares 
of the diagonals equals the sum of the squares of the sides.)
Although the drawing above shows the plane, where we usually use dot product 
notation and write length as $\absval{\vec{v}}$, 
here we will use the notation for inner product and norm.

As above, expand
$
  \norm{\vec{v}+\vec{w}}^2=\innerprod{\vec{v}+\vec{w}}{\vec{v}+\vec{w}}
$.
by using additivity of the first input and then the second. 
\begin{align*}
  \norm{\vec{v}+\vec{w}}^2
  &=\innerprod{\vec{v}+\vec{w}}{\vec{v}+\vec{w}}           \\
  &=\innerprod{\vec{v}}{\vec{v}+\vec{w}}+\innerprod{\vec{w}}{\vec{v}+\vec{w}} \\
  &=\innerprod{\vec{v}}{\vec{v}}            
   +\innerprod{\vec{v}}{\vec{w}}
   +\innerprod{\vec{w}}{\vec{v}}
   +\innerprod{\vec{w}}{\vec{w}}
  =\norm{\vec{v}}^2            
   +\innerprod{\vec{v}}{\vec{w}}
   +\innerprod{\vec{w}}{\vec{v}}
   +\norm{\vec{w}}^2
\end{align*}
The same expansion for $\innerprod{\vec{v}-\vec{w}}{\vec{v}-\vec{w}}$
gives this.
\begin{equation*}
  \norm{\vec{v}-\vec{w}}^2
  =\innerprod{\vec{v}-\vec{w}}{\vec{v}-\vec{w}}
  =\norm{\vec{v}}^2
   -\innerprod{\vec{v}}{\vec{w}}
   -\innerprod{\vec{w}}{\vec{v}}
   +\norm{\vec{w}}^2
\end{equation*}
Adding the two gives the desired result. 

Now, with the inner product and norm operations, 
even in spaces where there is no obvious geometry
we can interpret geometric ideas. 
An example is that can generalize distance to a
\definend{metric}\index{metric} 
operation
$d(\vec{v},\vec{w})=\norm{\vec{v}-\vec{w}}$.
Another example is that we can say that
vectors $\vec{v}$ and~$\vec{w}$ are orthogonal when 
$\innerprod{\vec{v}}{\vec{w}}=0$,
and we can extend the Gram-Schmidt process
to any finite dimensional inner product space.

A comment: we have shown how to define a norm operation from the inner product, 
as $\norm{\vec{v}}=\sqrt{\smashdp{\innerprod{\vec{v}}{\vec{v}}}}$,
and then derived the
parallelogram identity.
It is also true that if a norm satisfies this identity then it
arises from an inner product, 
meaning that 
$\innerprod{\vec{v}}{\vec{w}}
  =(1/2)\cdot(\norm{\vec{v}+\vec{w}}^2-\norm{\vec{v}}^2-\norm{\vec{w}}^2)$,
although proving this is beyond our scope.
An example of a norm that does not satisfy this identity is the one
in ($*$) above.
If we take $f(x)=1$ and $g(x)=2x$ then
\begin{equation*}
  \norm{f+g}=\int_0^1\absval{1+2x}\,dx=2
  \qquad
  \norm{f-g}=\int_0^1\absval{1-2x}\,dx=1/2
\end{equation*}
and
\begin{equation*}
  \norm{f}=\int_0^1\absval{1}\,dx=1
  \qquad
  \norm{g}=\int_0^1\absval{2x}\,dx=1
\end{equation*}
so 
$\norm{f+g}^2+\norm{f-g}^2$ does not equal $2\norm{f}^2+2\norm{g}^2$.


% We are now read for our goal, to characterize which transformations
% of the complex vector space~$\C^n$ preserve length.
% Make $\C^n$ an inner product space by using the Hermitian inner product.

% Given a matrix~$M$, its 
% \definend{matrix conjugate}~$\compconj{M}$\index{matrix conjugate}\index{matrix!conjugate} 
% is the matrix whose entries are the conjugate of the entries of~$M$.
% This operation is linear, that is, 
% $\compconj{M+N}=\compconj{M}+\compconj{N}$ and 
% $\compconj{z\cdot M}=\compconj{z}\cdot\compconj{M}$.
% This operation also respects matrix multiplication, 
% $\compconj{M\cdot N}=\compconj{M}\cdot\compconj{N}$.  

% The \definend{conjugate transpose}\index{conjugate transpose}\index{matrix!conjugate transpose}\index{transpose!conjugate} 
% or \definend{Hermitian transpose},\index{Hermitian transpose}\index{transpose!Hermitian}
% $\conjtrans{M}$
% is the transpose of the conjugate.
% We can either conjugate and then transpose, or transpose and 
% then conjugate, so that
% $\conjtrans{M}=\compconj{(\trans{M})}=\trans{(\compconj{M})}$.
% A complex matrix~$U$ is \definend{unitary}\index{unitary}\index{matrix!unitary} 
% if $U\cdot\conjtrans{U}$ equals the identity.
% We will show that a transformation preserves length if and only if its
% representation is unitary.

% We start with some small results.
% First, the conjugate transpose operation is idempotent, 
% $\conjtrans{(\conjtrans{M})}=M$,
% Second, it respects addition, $\conjtrans{M+N}=\conjtrans{M}+\conjtrans{N}$,
% and scalar multiplication, 
% $\conjtrans{(z\cdot M)}=\compconj{z}\cdot\conjtrans{M}$.
% Third, $\conjtrans{(M\cdot N)}=\conjtrans{N}\cdot\conjtrans{M}$,
% and the dot product of complex column vectors is 
% $\innerprod{\vec{v}}{\vec{w}}$ equals the 
% result of treating the two vectors as one-column matrices and 
% multiplying $\conjtrans{\vec{w}\,}\cdot \vec{v}$.

% The verifications are straightforward.
% For the first, recall that we can take a 
% conjugate transpose by doing conjugation and then transposition,
% or we can do them in the other order. 
% That gives this.
% \begin{equation*}
% \conjtrans{(\conjtrans{M})}
%   =\compconj{\trans{\big(\,\compconj{\trans{M}}\,\big)\!}}
%   =\compconj{\compconj{\trans{\trans{M}}}}
%   =\compconj{\compconj{M}}
%   =M
% \end{equation*}
% The second result follows immediately from earlier work.
% For the third, recall how transpose interacts with matrix multipication,
% $\conjtrans{(MN)}
% =\compconj{\trans{(MN)}}
% =\compconj{\trans{N}\cdot\trans{M}}
% =\compconj{\trans{N}}\cdot\compconj{\trans{M}}
% =\conjtrans{N}\cdot\conjtrans{M}$. 
% Item~(4) is a common source of mistakes.
% \begin{equation*}
%   \innerprod{\vec{v}}{\vec{w}}
%   =\innerprod{\colvec{v_1 \\ \vdots \\ v_n}}{\colvec{w_1 \\ \vdots \\ w_n}}
%   =v_1\compconj{w_1}+\cdots +v_n\compconj{w_n}
%   =\rowvec{\compconj{w_1} &\cdots &\compconj{w_n}}\colvec{v_1 \\ \vdots \\ v_n}
% \end{equation*}



\begin{exercises}
\item
Show that an attempted definition of $\innerprod{\vec{v}}{\vec{w}}$
that is linear in both inputs
loses that $\innerprod{\vec{v}}{\vec{v}}$ gives the length.
\begin{answer}
If
$\innerprod{\vec{v}}{\vec{v}}$ is greater than or equal to~$0$ for all
complex vectors,
then 
$0\leq \innerprod{i\vec{v}}{i\vec{v}}
  =i\innerprod{\vec{v}}{i\vec{v}}
  =i^2\innerprod{\vec{v}}{\vec{v}}\leq 0$.
\end{answer}

\item Show that linearity of $\innerprod{\vec{v}}{\vec{w}}$ in the first
input along with conjugate symmetry implies 
\definend{sesquilinearity}:
$\innerprod{\vec{v}}{z\vec{w}}=\compconj{z}\cdot \innerprod{\vec{v}}{\vec{w}}$.
\begin{answer}
$\innerprod{\vec{v}}{z\vec{w}}
=\compconj{\innerprod{z\vec{w}}{\vec{v}}}
=\compconj{z\cdot\innerprod{\vec{w}}{\vec{v}}}
=\compconj{z}\cdot\compconj{\innerprod{\vec{w}}{\vec{v}}}
=
\compconj{z}\cdot \innerprod{\vec{v}}{\vec{w}}$
\end{answer}

\item Prove the Pythagorean Theorem,  
that if $\vec{v}$ is orthogonal to~$\vec{w}$ then 
$\norm{\vec{v}+\vec{w}}^2=\norm{\vec{v}}^2+\norm{\vec{w}}^2$.
\begin{answer}
$
\norm{\vec{v}+\vec{w}}^2
=\innerprod{\vec{v}+\vec{w}}{\vec{v}+\vec{w}}
=\innerprod{\vec{v}}{\vec{v}}
  +\innerprod{\vec{v}}{\vec{w}}
  +\innerprod{\vec{w}}{\vec{v}}
  +\innerprod{\vec{w}}{\vec{w}}
=\norm{\vec{v}}
  +\zero
  +\zero
  +\norm{\vec{w}}
$
\end{answer}

\item
Show that Hermitian inner product on the two-dimensional vector space~$\C^2$ 
satisfies these three conditions in the definition of inner product. 


\end{exercises}
\index{Inner Product|)}
\endinput
% \end{document}
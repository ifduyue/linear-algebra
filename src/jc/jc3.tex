% Chapter 4, Section 1 _Linear Algebra_ Jim Hefferon
%  http://joshua.smcvt.edu/linearalgebra
%  2001-Jun-12
\section{Nilpotence}
This chapter shows that every square matrix is similar to one
that is a sum of two kinds of simple matrices.
The prior section
focused on the first simple kind, diagonal matrices.
We now consider the other kind.









\subsectionoptional{Self-Composition}\index{self composition!of maps}\index{map!self composition}\index{transformation!composed with itself}\index{composition!self}
% \noindent\textit{This subsection is optional, although it is necessary
% for later material in this section and in the next one.}

Because a linear transformation $\map{t}{V}{V}$ has the same domain as
codomain, we can
compose $t$ with itself: 
\( t^2=\composed{t}{t} \), 
and \( t^3=\composed{t}{\composed{t}{t}} \), 
etc.\appendrefs{function iteration}\spacefactor=1000 %
\begin{center}
  \includegraphics{jc/mp/ch5.1}
\end{center}
The superscript
power notation $t^j$ for iterates of the transformations 
fits with 
the notation that we've used for their square matrix representations
because if $\rep{t}{B,B}=T$ then \( \rep{t^j}{B,B}=T^j \).


\begin{example} \label{ex:DerivIter}
For the derivative map \( \map{d/dx}{\polyspace_3}{\polyspace_3} \)
given by 
\begin{equation*}
  a+bx+cx^2+dx^3\xmapsunder{d/dx} b+2cx+3dx^2
\end{equation*}
the second power is the second derivative,
\begin{equation*}
  a+bx+cx^2+dx^3\xmapsunder{d^2/dx^2} 2c+6dx 
\end{equation*}
the third power is the third derivative,
\begin{equation*}
  a+bx+cx^2+dx^3\xmapsunder{d^3/dx^3} 6d 
\end{equation*}
and any higher power is the zero map.
\end{example}

\begin{example}
This transformation of the space $\matspace_{\nbyn{2}}$ of $\nbyn{2}$ matrices
\begin{equation*}
  \begin{mat}
    a  &b  \\
    c  &d
  \end{mat}
  \mapsunder{t}
  \begin{mat}
    b  &a  \\
    d  &0
  \end{mat}
\end{equation*}
has this second power
\begin{equation*}
  \begin{mat}
    a  &b  \\
    c  &d
  \end{mat}
  \mapsunder{t^2}
  \begin{mat}
    a  &b  \\
    0  &0
  \end{mat}
\end{equation*}
and this third power.
\begin{equation*}
  \begin{mat}
    a  &b  \\
    c  &d
  \end{mat}
  \mapsunder{t^3}
  \begin{mat}
    b  &a  \\
    0  &0
  \end{mat}
\end{equation*}
After that, $t^4=t^2$ and $t^5=t^3$, etc.
\end{example}

\begin{example}
Consider the shift transformation $\map{t}{\C^3}{\C^3}$.
\begin{equation*}
  \colvec{x \\ y \\ z} \mapsunder{t} \colvec{0 \\ x \\ y}
\end{equation*}
We have that 
\begin{equation*}
  \colvec{x \\ y \\ z} \mapsunder{t} \colvec{0 \\ x \\ y}
                       \mapsunder{t} \colvec{0 \\ 0 \\ x}
                       \mapsunder{t} \colvec{0 \\ 0 \\ 0}
\end{equation*}
so the range spaces descend to the trivial subspace.
\begin{equation*}
  \rangespace{t}=\set{\colvec{0 \\ a \\ b}\suchthat a,b\in\C}
  \qquad
  \rangespace{t^2}=\set{\colvec{0 \\ 0 \\ c}\suchthat c\in\C}
  \qquad
  \rangespace{t^3}=\set{\colvec{0 \\ 0 \\ 0}}  
\end{equation*}
\end{example}

These examples suggest that after some number of iterations
the map settles down.

\begin{lemma}  \label{le:RangeAndNullChains}
For any transformation \( \map{t}{V}{V} \), the range spaces of the powers
form a descending chain
\begin{equation*}
  V\supseteq \rangespace{t}\supseteq\rangespace{t^2}\supseteq\cdots
\end{equation*}
and the null spaces form an ascending chain.
\begin{equation*}
  \set{\vec{0}}\subseteq\nullspace{t}\subseteq\nullspace{t^2}\subseteq\cdots
\end{equation*}
Further, there is a \( k>0 \) such that
for powers less than $k$ the subsets are proper:~if
$j<k$ then $\rangespace{t^j}\supset\rangespace{t^{j+1}}$
and 
$\nullspace{t^j}\subset\nullspace{t^{j+1}}$,
while 
if $j\geq k$ then $\rangespace{t^j}=\rangespace{t^{j+1}}$
and 
$\nullspace{t^j}=\nullspace{t^{j+1}}$).  
\end{lemma}

\noindent (The $k=1$ case can happen, for instance if $t$ is the identity map,
so that in the chains none of the subsets are proper subsets.)

\begin{proof}
First recall that for any map the dimension of its range space
plus the dimension of its null space equals
the dimension of its domain.
So if the dimensions of the range spaces shrink then the 
dimensions of the null spaces must rise.
We will do the range space half here and leave the rest for
\nearbyexercise{exer:RangeAndNullChains}.

We start by showing 
that the range spaces form a chain.
If $\vec{w}\in\rangespace{t^{j+1}}$, so that
$\vec{w}=t^{j+1}(\vec{v})$ for some $\vec{v}$, 
then $\vec{w}=t^{j}(\,t(\vec{v})\,)$.
Thus $\vec{w}\in\rangespace{t^{j}}$.

Next we verify the ``further'' property:
in the chain the subsets containments are proper 
initially, and then from some power $k$ onward the range spaces are equal. 
We first show that if any pair of adjacent range spaces in the
chain are equal \( \rangespace{t^{k}}=\rangespace{t^{k+1}} \)
then all later ones are also equal:
\( \rangespace{t^{k+1}}=\rangespace{t^{k+2}} \), etc.
This holds because
\( \map{t}{\rangespace{t^{k+1}}}{\rangespace{t^{k+2}}} \)
is the same map, with the same domain, as
\( \map{t}{\rangespace{t^{k}}}{\rangespace{t^{k+1}}} \), and
it therefore has the same range
\( \rangespace{t^{k+1}}=\rangespace{t^{k+2}} \)
(it holds for all higher powers by induction).
So if the chain of range spaces ever stops strictly decreasing then
from that point onward it is stable.

We end by showing that the chain must eventually stop decreasing. 
Each range space is a subspace of the one before it.
For it to be a proper subspace, it must be of strictly lower dimension
(see \nearbyexercise{exer:PropSubspStrictLowerDimen}).
These spaces are finite-dimensional and so the chain can fall for only
finitely many steps.
That is, the power $k$ is at most the dimension of $V$.
\end{proof}

\begin{example}
The derivative map $a+bx+cx^2+dx^3\xmapsunder{d/dx} b+2cx+3dx^2$
on $\polyspace_3$ has this chain of range spaces.
\begin{equation*}
  \rangespace{t^0}=\polyspace_3
    \:\supset\:\rangespace{t^1}=\polyspace_2
    \:\supset\:\rangespace{t^2}=\polyspace_1
    \:\supset\:\rangespace{t^3}=\polyspace_0
    \:\supset\:\rangespace{t^4}=\set{\zero}
    % =\rangespace{t^5}=\set{\zero}=
    % \,=\,\cdots
\end{equation*}
All later elements of the chain are the trivial space. 
It has this chain of null spaces.
\begin{equation*}
  \nullspace{t^0}=\set{\zero}
  \:\subset\: \nullspace{t^1}=\polyspace_0
  \:\subset\:\nullspace{t^2}=\polyspace_1
  \:\subset\:\nullspace{t^3}=\polyspace_2
  \:\subset\:\nullspace{t^4}=\polyspace_3
\end{equation*}
Later elements are the entire space.
\end{example}

\begin{example} \label{exam:PolyRankFalls}
Let \( \map{t}{\polyspace_2}{\polyspace_2} \) be the map
\( d_0+d_1x+d_2x^2 \mapsto 2d_0+d_2x. \)
As the lemma describes, on
iteration the range space shrinks
\begin{equation*}
  \rangespace{t^0}=\polyspace_2
    \quad
  \rangespace{t}=\set{a_0+a_1x\suchthat a_0,a_1\in\C}
    \quad
  \rangespace{t^2}=\set{a_0\suchthat a_0\in\C}
\end{equation*}
and then stabilizes, so that $\rangespace{t^2}=\rangespace{t^3}=\cdots$.
The null space grows
\begin{equation*}
  \nullspace{t^0}=\set{0}
    \quad
  \nullspace{t}=\set{b_1x\suchthat b_1\in\C}
    \quad
  \nullspace{t^2}=\set{b_1x+b_2x^2\suchthat b_1,b_2\in\C}
\end{equation*}
and then stabilizes $\nullspace{t^2}=\nullspace{t^3}=\cdots$.
\end{example}

\begin{example}
The transformation \( \map{\pi}{\C^3}{\C^3} \) projecting onto the
first two coordinates
\begin{equation*}
   \colvec{c_1 \\ c_2 \\ c_3}
     \mapsunder{\pi}
   \colvec{c_1 \\ c_2 \\ 0}
\end{equation*}
has \( \C^3\supset\rangespace{\pi}=\rangespace{\pi^2}=\cdots \)
and \( \set{\zero}\subset\nullspace{\pi}=\nullspace{\pi^2}=\cdots\, \)
where this is the range space and the null space.
\begin{equation*}
  \rangespace{\pi}=\set{\colvec{a \\ b \\ 0}\suchthat a,b\in\C}
  \qquad
  \nullspace{\pi}=\set{\colvec{0 \\ 0 \\ c}\suchthat c\in\C}
\end{equation*}
\end{example}

\begin{definition}
Let \( t \) be a transformation on an \( n \)-dimensional space.
The \definend{generalized range space}\index{generalized range space}%
\index{range space!generalized}
(or \definend{closure of the range space\/}\index{closure!of range space}%
\index{range space!closure of})
is $\genrangespace{t}=\rangespace{t^n}$.
The \definend{generalized null space}\index{generalized null space}%
\index{null space!generalized}
(or \definend{closure of the null space\/}\index{closure!of null space}%
\index{null space!closure of})
is $\gennullspace{t}=\nullspace{t^n}$.
\end{definition}

This graph illustrates.
The horizontal axis gives the power~$j$ of a transformation.
The vertical axis gives
the dimension of the range space of $t^j$
as the distance above zero, and thus also shows the dimension of the 
null space because the two add to the dimension $n$ of the domain.
\begin{center}
  % \includegraphics{jc/mp/ch5.2}
  \includegraphics{jc/mp/ch5.8}
\end{center}
On iteration 
the rank falls and the nullity rises
until there is some $k$ such that 
the map reaches a steady state
$\rangespace{t^k}=\rangespace{t^{k+1}}=\genrangespace{t}$
and $\nullspace{t^k}=\nullspace{t^{k+1}}=\gennullspace{t}$.
This must happen by the $n$-th iterate.
% The steady state's distance above zero is the dimension of the 
% generalized range space
% and its distance below $n$ is the dimension of
% the generalized null space.

\begin{exercises}
  \recommended \item 
    Give the chains of range spaces and null spaces for the zero and
    identity transformations.
    \begin{answer}
      For the zero transformation,
      no matter what the space, the chain of range spaces 
      is \( V\supset\set{\vec{0}}=\set{\vec{0}}=\cdots\, \)
      and the chain of null spaces is \( \set{\vec{0}}\subset V=V=\cdots\, \).
      For the identity transformation the chains are
      \( V=V=V=\cdots \) and
      \( \set{\vec{0}}=\set{\vec{0}}=\cdots\, \). 
    \end{answer}
  \recommended\item 
     For each map, 
     give the chain of range spaces and the chain of null spaces,
     and the generalized range space and the 
     generalized null space.
     \begin{exparts}
       \partsitem $\map{t_0}{\polyspace_2}{\polyspace_2}$, 
         $a+bx+cx^2\mapsto b+cx^2$ 
       \partsitem $\map{t_1}{\Re^2}{\Re^2}$,
         \begin{equation*}
           \colvec{a \\ b}\mapsto\colvec{0 \\ a}
         \end{equation*}
       \partsitem $\map{t_2}{\polyspace_2}{\polyspace_2}$, 
         $a+bx+cx^2\mapsto b+cx+ax^2$
       \partsitem $\map{t_3}{\Re^3}{\Re^3}$,
         \begin{equation*}
           \colvec{a \\ b \\ c}\mapsto\colvec{a \\ a \\ b}
         \end{equation*}
     \end{exparts}
     \begin{answer}
       \begin{exparts}
         \partsitem Iterating $t_0$ twice  
           $a+bx+cx^2\mapsto b+cx^2\mapsto cx^2$
           gives 
           \begin{equation*}
             a+bx+cx^2\mapsunder{t_0^2}cx^2
           \end{equation*}
           and any higher iterate is the same map.
           Thus, while $\rangespace{t_0}$ is the space of 
           quadratic polynomials
           with no linear term $\set{p+rx^2\suchthat p,r\in \C}$,
           and
           $\rangespace{t_0^2}$ is the space of purely-quadratic polynomials
           $\set{rx^2\suchthat r\in \C}$, 
           this is where the chain stabilizes
           $\genrangespace{t_0}=\set{rx^2\suchthat r\in \C}$.
 
           As for null spaces, 
           $\nullspace{t_0}$ is the space of  purely-linear quadratic 
           polynomials $\set{qx\suchthat q\in \C}$, and
           $\nullspace{t_0^2}$ is the space of linear polynomials
           $\set{p+qx\suchthat p,q\in \C}$. 
           This is the end: $\gennullspace{t_0}=\nullspace{t_0^2}$. 
         \partsitem The second power
           \begin{equation*}
             \colvec{a \\ b}
              \mapsunder{t_1}\colvec{0 \\ a}
              \mapsunder{t_1}\colvec{0 \\ 0}
           \end{equation*}
           is the zero map.
           Consequently, the chain of range spaces
           \begin{equation*}
             \Re^2
               \supset\set{\colvec{0 \\ p}\suchthat p\in\C}
               \supset\set{\zero}
               =\cdots
           \end{equation*}
           and the chain of null spaces 
           \begin{equation*}
             \set{\zero}
               \subset\set{\colvec{q \\ 0}\suchthat q\in\C}
               \subset\Re^2
               =\cdots
           \end{equation*}
           each has length two.
           The generalized range space is the trivial subspace and the
           generalized null space is the entire space.
         \partsitem Iterates of this map cycle around
           \begin{equation*}
             a+bx+cx^2
               \mapsunder{t_2} b+cx+ax^2            
               \mapsunder{t_2} c+ax+bx^2            
               \mapsunder{t_2} a+bx+cx^2
               \;\cdots            
           \end{equation*}
           and the chains of range spaces and null spaces are trivial. 
           \begin{equation*}
             \polyspace_2=\polyspace_2=\cdots
              \qquad
              \set{\zero}=\set{\zero}=\cdots  
           \end{equation*}
           Thus, obviously,
           generalized spaces are $\genrangespace{t_2}=\polyspace_2$
           and $\gennullspace{t_2}=\set{\zero}$.
         \partsitem We have 
           \begin{equation*}
             \colvec{a \\ b \\ c}
                \mapsto\colvec{a \\ a \\ b}
                \mapsto\colvec{a \\ a \\ a}
                \mapsto\colvec{a \\ a \\ a}
                \mapsto\cdots
           \end{equation*}
           and so the chain of range spaces 
           \begin{equation*}
             \Re^3
               \supset\set{\colvec{p \\ p \\ r}\suchthat p,r\in\C}
               \supset\set{\colvec{p \\ p \\ p}\suchthat p\in\C}
               =\cdots
           \end{equation*}
           and the chain of null spaces
           \begin{equation*}
             \set{\zero}
                \subset\set{\colvec{0 \\ 0 \\ r}\suchthat r\in \C}
                \subset\set{\colvec{0 \\ q \\ r}\suchthat q,r\in \C}
                =\cdots
           \end{equation*}
           each has length two.
           The generalized spaces are the final ones shown above in each chain.
      \end{exparts}
     \end{answer}
  \item 
    Prove that function composition is associative
    \( \composed{(\composed{t}{t})}{t}=\composed{t}{(\composed{t}{t})} \)
    and so we can write $t^3$ without specifying a grouping.
    \begin{answer}
      Each maps \( x\mapsto t(t(t(x))) \). 
    \end{answer}
  \item \label{exer:PropSubspStrictLowerDimen}
    Check that a subspace must be of dimension less than or equal to the 
    dimension of its superspace.
    Check that if the subspace is proper (the subspace does not equal the
    superspace) then the dimension is strictly less.
    \textit{(This is used in the proof of
             \nearbylemma{le:RangeAndNullChains}.)}
    \begin{answer}
      Recall that if $W$ is a subspace of $V$ then we can enlarge
      any basis $B_W$ for $W$
      to make a basis $B_V$ for $V$.
      From this the first sentence is immediate.
      The second sentence is also not hard:~$W$ is the span of $B_W$ and
      if $W$ is a proper subspace then $V$ is not the span of $B_W$, and
      so $B_V$ must have at least one vector more than does $B_W$.     
    \end{answer}
  \recommended\item 
    Prove that the generalized range space $\genrangespace{t}$ is the
    entire space, and the generalized null space $\gennullspace{t}$ is trivial,
    if the transformation $t$ is nonsingular.
    Is this `only if' also?
    \begin{answer}
      It is both `if' and `only if'.
      A linear map is nonsingular
      if and only if it preserves dimension, that is, if the dimension of 
      its range equals the dimension of its domain.
      With a transformation $\map{t}{V}{V}$ that means that 
      the map is nonsingular if and only if it is onto:
      $\rangespace{t}=V$ (and thus $\rangespace{t^2}=V$, etc).
    \end{answer}
  \item \label{exer:RangeAndNullChains} 
    Verify the null space half of \nearbylemma{le:RangeAndNullChains}.
    \begin{answer}
      The null spaces form chains because
      because if $\vec{v}\in\nullspace{t^j}$ then $t^j(\vec{v})=\zero$
      and $t^{j+1}(\vec{v})=t(\,t^j(\vec{v})\,)=t(\zero)=\zero$ and
      so $\vec{v}\in\nullspace{t^{j+1}}$.

      Now, 
      the ``further'' property for null spaces follows from that fact
      that it holds for range spaces, along with the prior exercise.
      Because the dimension of $\rangespace{t^j}$ plus the dimension of
      $\nullspace{t^j}$ equals the dimension~$n$ of the starting space~$V$,
      when the dimensions of the range spaces stop decreasing, so do the
      dimensions of the null spaces.
      The prior exercise shows that from this point~$k$ on, 
      the containments in the chain are not proper\Dash the null spaces 
      are equal. 
    \end{answer}
  \recommended\item
    Give an example of a transformation on a three
    dimensional space whose range has dimension two.
    What is its null space?
    Iterate your example until the range space and null space stabilize.
    \begin{answer}
      (Many examples are correct but here is one.) 
      An example is the shift operator on triples of reals
      \( (x,y,z)\mapsto (0,x,y) \).
      The null space is all triples that start with two zeros.
      The map stabilizes after three iterations.
     \end{answer}
  \item 
      Show that the range space and null space of a linear transformation
      need not be disjoint.
      Are they ever disjoint?
      \begin{answer}
        The differentiation operator
        \( \map{d/dx}{\polyspace_1}{\polyspace_1} \) has the same
        range space as null space.
        For an example of where they are disjoint\Dash
        except for the zero vector\Dash consider an identity map,
        or any nonsingular map. 
      \end{answer}
\end{exercises}

















\subsectionoptional{Strings}
\index{nilpotent|(}
\noindent\textit{This requires material from the 
    optional Combining Subspaces subsection.}

The prior subsection shows that as \( j \) increases
the dimensions of the $\rangespace{t^j}$'s fall while
the dimensions of the $\nullspace{t^j}$'s rise, 
in such a way that this rank and nullity split between them 
the dimension of $V$.
Can we say more;
do the two split a basis\Dash is
\( V=\rangespace{t^j}\directsum\nullspace{t^j} \)?

The answer is yes for the smallest power $j=0$ since
\( V=\rangespace{t^0}\directsum\nullspace{t^0}=V\directsum\set{\zero} \).
The answer is also yes at the other extreme.

\begin{lemma} \label{lem:RestONeToOne}
For any linear \( \map{t}{V}{V} \) 
the function \( \map{t}{\genrangespace{t}}{\genrangespace{t}} \)
is one-to-one.
\end{lemma}

\begin{proof}
Let the dimension of $V$ be $n$.
Because \( \rangespace{t^n}=\rangespace{t^{n+1}} \),
the map \( \map{t}{\genrangespace{t}}{\genrangespace{t}} \) is
a dimension-preserving homomorphism. 
Therefore, by Theorem~Three.II.\ref{th:OOHomoEquivalence} it is one-to-one.  
\end{proof}

\begin{corollary} \label{GenRngNullDirSumToSp}
Where \( \map{t}{V}{V} \) is a linear transformation, 
the space is the direct sum 
\( V=\genrangespace{t}\directsum\gennullspace{t} \).
That is, both (1)~\( \dim(V)=\dim(\genrangespace{t})+\dim(\gennullspace{t}) \) 
and (2)~\( \genrangespace{t}\intersection\gennullspace{t}=\set{\zero} \).
\end{corollary}

\begin{proof}
Let the dimension of $V$ be $n$.
We will verify the second sentence, which is equivalent to the first.
Clause~(1) is true because any transformation satisfies that
its rank plus its nullity
equals the dimension of the space,  
and in particular this holds for the transformation $t^n$.

For clause~(2), assume that
\( \vec{v}\in\genrangespace{t}\intersection\gennullspace{t} \)
%               =\rangespace{t^n}\intersection\nullspace{t^n} \)
to prove that $\vec{v}=\zero$.
Because \( \vec{v} \) is in the generalized null space, \( t^n(\vec{v})=\zero \).
On the other hand, by the lemma
\( \map{t}{\genrangespace{t}}{\genrangespace{t}} \)
is one-to-one and 
a composition of one-to-one maps is one-to-one, so
\( \map{t^n}{\genrangespace{t}}{\genrangespace{t}} \) is one-to-one.
Only \( \zero \) is sent by a one-to-one linear map to
\( \zero \)  so the fact that \( t^n(\vec{v})=\zero \) implies that
\( \vec{v}=\zero \).
\end{proof}

\begin{remark}
Technically there is a difference between the map $\map{t}{V}{V}$ and
the map on the subspace \( \map{t}{\genrangespace{t}}{\genrangespace{t}} \)
if the generalized range space is not equal to $V$, because the domains are
different.
But the difference is small because
the second is the 
restriction % \index{restriction}\index{map!restriction}\appendrefs{map restrictions}\spacefactor=1000 %
of the first to 
$\genrangespace{t}$. 
\end{remark}

For powers between $j=0$ and~$j=n$, 
the space $V$ might not be the direct sum of
$\rangespace{t^j}$ and $\nullspace{t^j}$.
The next example shows that the two can have a nontrivial intersection.

\begin{example}   \label{FirstNilMap}
Consider the transformation of \( \C^2 \)
defined by this action on the elements of the standard basis.
\begin{equation*}
  \colvec[r]{1 \\ 0}
    \mapsunder{n}
    \colvec[r]{0 \\ 1}
  \quad
  \colvec[r]{0 \\ 1}
    \mapsunder{n}
    \colvec[r]{0 \\ 0}
  \qquad
  N=\rep{n}{\stdbasis_2,\stdbasis_2}=\begin{mat}[r]
    0  &0  \\
    1  &0
  \end{mat}
\end{equation*}
This is a \definend{shift map}\index{shift}\index{transformation!shift}
because it shifts the entries down, with the bottom entry shifting entirely
out of the vector.  
\begin{equation*}
  \colvec{x \\ y}\mapsto \colvec{0 \\ x}
\end{equation*}
On the basis, this map's action gives a 
\definend{string}.\index{string!of basis vectors}
\begin{equation*}
  \begin{strings}{ccccc}
     \colvec{1 \\ 0} &\mapsto &\colvec{0 \\ 1} &\mapsto &\colvec{0 \\ 0}
  \end{strings}
  \qquad\text{that is}\qquad
  \begin{strings}{ccccc}
     \vec{e}_1 &\mapsto &\vec{e}_2 &\mapsto &\zero
  \end{strings}
\end{equation*}
This map is a natural way to have a vector in   
both the range space and null space; the string depiction shows 
that this is one such  vector.
\begin{equation*}
  \vec{e}_2=\colvec[r]{0 \\ 1}
\end{equation*}
Observe also that although $n$ is not the zero map, 
the function $n^2=\composed{n}{n}$
is the zero map.
\end{example}

\begin{example}  \label{NilIndexFourOnCFour}
A linear function \( \map{\hat{n}}{\C^4}{\C^4} \)
whose action on \( \stdbasis_4 \) is given by
the string
\begin{equation*}
  \begin{strings}{ccccccccc}
     \vec{e}_1 &\mapsto &\vec{e}_2
          &\mapsto &\vec{e}_3
          &\mapsto &\vec{e}_4
          &\mapsto &\zero
  \end{strings}
\end{equation*}
has
\( \rangespace{\hat{n}}\intersection\nullspace{\hat{n}} \) equal to the 
span \( \spanof{\set{\vec{e}_4}} \),
has \( \rangespace{\hat{n}^2}\intersection\nullspace{\hat{n}^2}=
  \spanof{\set{\vec{e}_3,\vec{e}_4}} \),
and has \( \rangespace{\hat{n}^3}\intersection\nullspace{\hat{n}^3}=
    \spanof{\set{\vec{e}_4}} \).
The matrix representation  is all zeros except for
some subdiagonal ones.
\begin{equation*}
  \hat{N}=\rep{\hat{n}}{\stdbasis_4,\stdbasis_4}
  =\begin{mat}[r]
    0  &0  &0  &0 \\
    1  &0  &0  &0 \\
    0  &1  &0  &0 \\
    0  &0  &1  &0 \
  \end{mat}
\end{equation*}
Although $\hat{n}$ is not the zero map, 
and neither is $\hat{n}^2$ or~$\hat{n}^3$, the function 
$\hat{n}^4$ is the zero function.
\end{example}

\begin{example} \label{ThirdNilMap}
Transformations can act via more than one string.
The transformation \( t \) acting on a basis
\( B=\sequence{\vec{\beta}_1,\dots,\vec{\beta}_5} \) by
\begin{equation*}
   \begin{strings}{ccccccc}
    \vec{\beta}_1 &\mapsto &\vec{\beta}_2 &\mapsto &\vec{\beta}_3
        &\mapsto &\zero \\
    \vec{\beta}_4 &\mapsto &\vec{\beta}_5 &\mapsto &\zero
  \end{strings}
\end{equation*}
will have, for instance, $\vec{\beta}_3$ in the intersection of
its range space and null space.
The strings make clear that $t^3$ is the zero map.
This map is represented by a matrix that is all zeros except for blocks
of subdiagonal ones
\begin{equation*}
  \rep{t}{B,B}=
  \begin{pmat}{rrr|rr}
     0  &0  &0  &0  &0  \\
     1  &0  &0  &0  &0  \\
     0  &1  &0  &0  &0  \\ \hline
     0  &0  &0  &0  &0  \\
     0  &0  &0  &1  &0
  \end{pmat}
\end{equation*}
(the lines just visually organize the blocks).
\end{example}

In those examples all vectors are eventually transformed to
zero.

\begin{definition} \label{def:nilpotent} \index{nilpotent!definition}
A \definend{nilpotent} transformation\index{transformation!nilpotent}%
\index{nilpotent!transformation}
is one with a power that is the zero map.
A \definend{nilpotent matrix}\index{matrix!nilpotent}%
\index{nilpotent!matrix}
is one with a power that is the zero matrix.
In either case, the least such power is the \definend{index of nilpotency}.%
\index{nilpotency, index of}\index{index of nilpotency}
\end{definition}

\begin{example}
In \nearbyexample{FirstNilMap} the index of nilpotency is two.
In \nearbyexample{NilIndexFourOnCFour} it is four.
In \nearbyexample{ThirdNilMap} it is three.
\end{example}

\begin{example}
The differentiation map \( \map{d/dx}{\polyspace_2}{\polyspace_2} \)
is nilpotent of index three since the third derivative of any quadratic
polynomial is zero.
This map's action is described by the string
$x^2\mapsto 2x\mapsto 2\mapsto 0$
and taking the basis \( B=\sequence{x^2,2x,2} \)
gives this representation.
\begin{equation*}
  \rep{d/dx}{B,B}=
  \begin{mat}[r]
     0  &0  &0  \\
     1  &0  &0  \\
     0  &1  &0
  \end{mat}
\end{equation*}
\end{example}

Not all nilpotent matrices are all zeros except for blocks of
subdiagonal ones.

\begin{example} \label{ex:NilMatNotCanon}
With the matrix $\hat{N}$ from \nearbyexample{NilIndexFourOnCFour},
and this four-vector basis
\begin{equation*}
  D=\sequence{\colvec[r]{1 \\ 0 \\ 1 \\ 0},
              \colvec[r]{0 \\ 2 \\ 1 \\ 0},
              \colvec[r]{1 \\ 1 \\ 1 \\ 0},
              \colvec[r]{0 \\ 0 \\ 0 \\ 1}}
\end{equation*}
a change of basis operation 
produces this representation with respect to \( D,D \).
\begin{equation*}
  \begin{mat}[r]
    1  &0  &1 &0 \\
    0  &2  &1 &0 \\
    1  &1  &1 &0 \\
    0  &0  &0 &1
  \end{mat}
  \begin{mat}[r]
    0  &0  &0 &0 \\
    1  &0  &0 &0 \\
    0  &1  &0 &0 \\
    0  &0  &1 &0
  \end{mat}
  \begin{mat}[r]
    1  &0  &1 &0 \\
    0  &2  &1 &0 \\
    1  &1  &1 &0 \\
    0  &0  &0 &1
  \end{mat}^{-1}\!\!
  =
  \begin{mat}[r]
   -1  &0  &1   &0 \\
   -3  &-2 &5   &0 \\
   -2  &-1  &3  &0 \\
    2  &1   &-2 &0
  \end{mat}
\end{equation*}
The new matrix is nilpotent; its fourth power 
is the zero matrix.
We could verify this with a tedious computation or we can 
instead just observe that
it is nilpotent since its fourth power 
is similar to \( \hat{N}^4 \), the zero matrix,
and the only matrix similar to the zero matrix is itself.
\begin{equation*}
   (P\hat{N}P^{-1})^4
   =P\hat{N}P^{-1}\cdot P\hat{N}P^{-1}\cdot P\hat{N}P^{-1}\cdot P\hat{N}P^{-1}
   =P\hat{N}^4P^{-1}
\end{equation*}
\end{example}

The goal of this subsection is to show
that the prior example is prototypical
in that every nilpotent matrix is similar to one that is all
zeros except for blocks of subdiagonal ones.

\begin{definition}
Let \( t \) be a nilpotent transformation on \( V \).
A  \definend{\( t \)-string generated by 
\( \vec{v}\in V \)}\index{string}
is a sequence
\( \sequence{\vec{v},t(\vec{v}),\ldots,t^{k-1}(\vec{v})} \)
such that $t^k(\vec{v})=\zero$.
A \definend{\( t \)-string basis}\index{basis!string}\index{string!basis}
is a basis that is a concatenation of \( t \)-strings.
\end{definition}

\noindent (The strings cannot form a basis under concatenation
unless they are disjoint because a basis cannot have a repeated vector.)

\begin{example}
This linear map \( \map{t}{\C^3}{\C^3} \)
\begin{equation*}
  \colvec{x \\ y \\ z}\mapsunder{t}\colvec{y \\ z \\ 0}
\end{equation*}
is nilpotent, of index~$3$.
\begin{equation*}
  \colvec{x \\ y \\ z}
  \mapsunder{t}\colvec{y \\ z \\ 0}
  \mapsunder{t}\colvec{z \\ 0 \\ 0}
  \mapsunder{t}\colvec{0 \\ 0 \\ 0}
\end{equation*}
This is a $t$-string.
\begin{equation*}
  \sequence{\colvec{0 \\ 0 \\ 1},
            \colvec{0 \\ 1 \\ 0},
            \colvec{1 \\ 0 \\ 0}}
\end{equation*}
Because that sequence is a basis, 
it is a $t$-string basis for the space~$\C^3$.
\end{example}

Where the sequence of the prior example is  
$\sequence{\vec{\beta}_1,
             \vec{\beta}_2,
             \vec{\beta}_3}$,
another $t$-string is 
$\sequence{\vec{\beta}_2,
             \vec{\beta}_3}$.
But of course the second sequence is not a basis for $\C^3$.
In that sense, the $t$-strings in a $t$-string basis must be maximal.

\begin{example}
The linear map of differentiation \( \map{d/dx}{\polyspace_2}{\polyspace_2} \)
is nilpotent.
The sequence
\( \sequence{x^2, 2x, 2} \)
is a \( d/dx \)-string of length~\( 3 \);
in particular, this string satisfies the requirement that $d/dx(2)=0$.
Because it is a basis, that sequence
is a \( d/dx \)-string basis for \( \polyspace_2\).
\end{example}

\begin{example}
In \nearbyexample{ThirdNilMap}, we can concatenate the $t$-strings
$\sequence{\vec{\beta}_1,\vec{\beta}_2,\vec{\beta}_3}$ and
$\sequence{\vec{\beta}_4,\vec{\beta}_5}$
to make a basis for the domain of $t$.
\end{example}

\begin{lemma}  \label{le:LongestTowerIsIndex}
If a space has a \( t \)-string basis then the index of nilpotency of $t$
equals the length of the longest string in that basis.
\end{lemma}

\begin{proof}
Let the space have a $t$-string basis and let $t$'s index of
nilpotency be~$k$.
Then \( t^k \) sends any vector to \( \zero \). 
That must include 
the vector starting any string,
so each string in the string basis has length at most~$k$.

Now instead suppose that the space has a $t$-string basis~$B$ where 
all of the strings are shorter than length~\( k \).
Because $t$ has the index of nilpotency~$k$, there is a \( \vec{v} \) 
such that \( t^{k-1}(\vec{v})\neq\zero \).
Represent $\vec{v}$ as a linear combination of elements from~$B$ 
and apply \( t^{k-1} \).
We are supposing that \( t^{k-1} \) maps each element of~$B$ to \( \zero \).
It therefore maps each term in the linear combination to $\zero$,
contradicting that it does not map \( \vec{v} \) to \( \zero \).
\end{proof}

We shall show that each
nilpotent map has an associated string basis, a basis of disjoint strings.
% Then our goal theorem, that every
% nilpotent matrix is similar to one that is
% all zeros except for blocks of subdiagonal ones, is immediate,
 %as in \nearbyexample{ThirdNilMap}.

To see the main idea of the argument, imagine that 
we want to construct a counterexample, a map that is nilpotent but 
without an associated basis of disjoint strings. 
We might think to make something like 
the map \( \map{t}{\C^5}{\C^5} \) with this action.
\begin{center}
  \begin{minipage}{1in}
  \setlength{\unitlength}{1pt}
  $\begin{strings}{ccccc}
     \begin{picture}(4,15)
       \put(0,14){$\vec{e}_1$}
       \put(0,-12){$\vec{e}_2$}
     \end{picture}
     &\begin{picture}(8,10)
       \put(0,8){\rotatebox{-30}{$\mapsto$}}%  
       \put(0,-8){\rotatebox{30}{$\mapsto$}}
     \end{picture}
     &\vec{e}_3 &\mapsto &\zero  \\[4ex]
     \vec{e}_4 &\mapsto &\vec{e}_5 &\mapsto &\zero  
  \end{strings}$
  \end{minipage}
  \hspace*{3em}
  $\rep{t}{\stdbasis_5,\stdbasis_5}=
  \begin{mat}[r]
    0  &0  &0  &0  &0  \\
    0  &0  &0  &0  &0  \\
    1  &1  &0  &0  &0  \\
    0  &0  &0  &0  &0  \\
    0  &0  &0  &1  &0
  \end{mat}$
\end{center}
But, the fact that the shown basis isn't disjoint doesn't mean that there 
isn't another basis that consists of disjoint strings.

To produce such a basis for this map 
we will first find the number and lengths of its strings.
Observe that $t$'s index of nilpotency is two.
\nearbylemma{le:LongestTowerIsIndex} says that in a disjoint string basis 
at least one string has length two.
There are five basis elements so if there is a disjoint string basis then
the map must act in one of these ways.
\begin{equation*}
  \begin{strings}{ccccc}
    \vec{\beta}_1 &\mapsto &\vec{\beta}_2 &\mapsto &\zero  \\
    \vec{\beta}_3 &\mapsto &\vec{\beta}_4 &\mapsto &\zero  \\
    \vec{\beta}_5 &\mapsto &\zero
  \end{strings}
  \hspace*{3em}
  \begin{strings}{ccccc}
    \vec{\beta}_1 &\mapsto &\vec{\beta}_2 &\mapsto &\zero  \\
    \vec{\beta}_3 &\mapsto &\zero   \\
    \vec{\beta}_4 &\mapsto &\zero   \\
    \vec{\beta}_5 &\mapsto &\zero
  \end{strings}
\end{equation*}
Now, the key point.
A transformation with the left-hand action has a
null space of dimension three since that's how many basis vectors are
mapped to zero. 
A transformation with the right-hand action has a null space of
dimension four.
With the matrix representation above we can determine which of the 
two possible shapes is right.
\begin{equation*}
  \nullspace{t}=
  \set{\colvec{x \\ -x \\ z \\ 0 \\ r}\suchthat x,z,r\in\C }
\end{equation*}
This is three-dimensional,
meaning that of the two disjoint string basis forms above, \( t \)'s 
basis has the left-hand one.

To produce a string basis for~$t$, first
pick \( \vec{\beta}_2 \) and \( \vec{\beta}_4 \) from
\( \rangespace{t}\intersection\nullspace{t} \).
\begin{equation*}
  \vec{\beta}_2=\colvec[r]{0 \\ 0 \\ 1 \\ 0 \\ 0}\qquad
  \vec{\beta}_4=\colvec[r]{0 \\ 0 \\ 0 \\ 0 \\ 1}
\end{equation*}
(Other choices are possible, just be sure that the set
\( \set{\vec{\beta}_2,\vec{\beta}_4} \) is linearly independent.)
For \( \vec{\beta}_5 \) pick a vector from \( \nullspace{t} \)
that is not in the span of \( \set{ \vec{\beta}_2,\vec{\beta}_4 } \).
\begin{equation*}
  \vec{\beta}_5=\colvec[r]{1 \\ -1 \\ 0 \\ 0 \\ 0}
\end{equation*}
Finally, take \( \vec{\beta}_1 \) and \( \vec{\beta}_3 \) such that
\( t(\vec{\beta}_1)=\vec{\beta}_2 \) and
\( t(\vec{\beta}_3)=\vec{\beta}_4 \).
\begin{equation*}
  \vec{\beta}_1=\colvec[r]{0 \\ 1 \\ 0 \\ 0 \\ 0}\qquad
  \vec{\beta}_3=\colvec[r]{0 \\ 0 \\ 0 \\ 1 \\ 0}
\end{equation*}
Therefore, we have a string basis
\( B=\sequence{\vec{\beta}_1,\ldots,\vec{\beta}_5} \)
and with respect to that basis
the matrix of $t$ has blocks of subdiagonal~$1$'s.
\begin{equation*}
  \rep{t}{B,B}=
  \begin{pmat}{rr|rr|r}
    0  &0  &0  &0  &0  \\
    1  &0  &0  &0  &0  \\  \hline
    0  &0  &0  &0  &0  \\
    0  &0  &1  &0  &0  \\  \hline
    0  &0  &0  &0  &0
  \end{pmat}
\end{equation*}

\begin{theorem}
\label{th:NilMapHasStrBas}
Any nilpotent transformation $t$ is associated with a \( t \)-string basis.
While the basis is not unique, the number
and the length of the strings is determined by \( t \).
\end{theorem}

This illustrates the proof, which describes three kinds of
basis vectors. 
They are in squares if they are members of the 
null space and in circles if they are not.
\begin{equation*}
   \begin{strings}{ccccccccccccccccccc}
     \digitincirc{3}
         &\mapsto &\digitincirc{1} &\mapsto
         &\cdots &  &  &  &  &  &  &  &\cdots
         &\mapsto &\digitincirc{1}
         &\mapsto &\digitinsq{1} &\mapsto &\zero  \\[.75ex]
     \digitincirc{3}
         &\mapsto &\digitincirc{1} &\mapsto
         &\cdots &  &  &  &  &  &  &  &\cdots
         &\mapsto &\digitincirc{1}
         &\mapsto &\digitinsq{1} &\mapsto &\zero  \\[.75ex]
         &\smash{\vdotswithin{\mapsto}}  \\
     \digitincirc{3}
         &\mapsto &\digitincirc{1} &\mapsto
         &\cdots &
         &\mapsto &\digitincirc{1}
         &\mapsto &\digitinsq{1} &\mapsto &\zero  \\[1ex]
     \digitinsq{2} &\mapsto &\zero \\[.75ex]
         &\smash{\vdotswithin{\mapsto}}  \\
     \digitinsq{2} &\mapsto &\zero
   \end{strings}
\end{equation*}

\begin{proof}
Fix a vector space $V$. 
We will argue by induction on the index of nilpotency.
If the map $\map{t}{V}{V}$
has index of nilpotency~\( 1 \) then it is the zero map and any basis 
is a string basis, $\vec{\beta}_1\mapsto\zero$, \ldots, 
$\vec{\beta}_n\mapsto\zero$.

For the inductive step, assume that the theorem holds for any transformation
$\map{t}{V}{V}$
with an index of nilpotency from $1$ up to and including~\( k-1 \) (with $k>1$),
and consider the index~$k$ case.

What gets the induction going is the observaton 
that the restriction of~$t$ to the range space~\( \rangespace{t} \) 
is also nilpotent, of index \( k-1 \).
So apply the inductive hypothesis to get a string basis for
\( \rangespace{t} \),
where the number and length of the strings
is determined by \( t \).
\begin{equation*}
  B=\cat{\cat{\sequence{\vec{\beta}_1,t(\vec{\beta}_1),\dots,
     t^{h_1}(\vec{\beta}_1)}}{
  \cat{\sequence{\vec{\beta}_2,\ldots,t^{h_2}(\vec{\beta}_2)}}}{\cdots}}{
  \sequence{\vec{\beta}_i,\ldots,t^{h_i}(\vec{\beta}_i)} }
\end{equation*}
We write~$i$ for the number of strings.
In the illustration above, these are the vectors of kind~\( 1 \).

Taking the final vector in each string
gives a basis
\( C=\sequence{t^{h_1}(\vec{\beta}_1),\dots,t^{h_i}(\vec{\beta}_i)} \)
for the intersection \( \rangespace{t}\intersection\nullspace{t} \).
This is because a member of \( \rangespace{t} \) maps to zero if and only
if it is a linear combination of basis vectors that map
to zero.
The illustration shows these as \( 1 \)'s in squares.

Now extend \( C \) to a basis for the entire nullspace, \( \nullspace{t} \).
\begin{equation*}
  \hat{C}=\cat{C}{\sequence{\vec{\xi}_1,\dots,\vec{\xi}_p}}
\end{equation*}
While the \( \vec{\xi} \)'s aren't uniquely
determined by $t$, what is uniquely determined is the number
of them:~it is the dimension of
\( \nullspace{t} \) minus the dimension of
\( \rangespace{t}\intersection\nullspace{t} \).
In the illustration, the $\vec{\xi}$'s are the vectors of kind~\( 2 \), 
and so \( \hat{C} \) is the set of vectors in squares.

Finally, \( \cat{B}{\sequence{\vec{\xi}_1,\dots,\vec{\xi}_p}} \)
is a basis for \( \rangespace{t}+\nullspace{t} \).
This is because a sum of 
something in the range space with something in the null space
can be represented using elements of \( B \) for the range space 
part along with $\xi$'s for any part from 
\( \nullspace{t}-\rangespace{t} \).
Note that
\begin{align*}
  \dim\big(\rangespace{t}+\nullspace{t}\big)
  &=
  \dim (\rangespace{t})+\dim (\nullspace{t})
    -\dim\big(\rangespace{t}\intersection\nullspace{t}\big)  \\
  &=
  \rank (t)+\nullity (t)-i          \\
  &=
  \dim (V)-i
\end{align*}
and so we can extend the basis 
\( \cat{B}{\sequence{\vec{\xi}_1,\dots,\vec{\xi}_p}} \) 
to a $t$-string basis for the entirety of 
\( V \) with the addition of~\( i \) more vectors.
To produce those, 
recall that each of \( \vec{\beta}_1,\dots,\vec{\beta}_i \) is
in the range space, \( \rangespace{t} \), 
and so use vectors
\( \vec{v}_1,\dots,\vec{v}_i\in V \) such that
\( t(\vec{v}_1)=\vec{\beta}_1,\dots,t(\vec{v}_i)=\vec{\beta}_i \).
The check that this extension preserves linear independence is
\nearbyexercise{exer:NilMapWillHaveStrBas}.
In the illustration, \( \vec{v}_1,\dots,\vec{v}_i \) are the \( 3 \)'s.
\end{proof}

\begin{corollary} \label{cor:NilpotentMatCanonForm}
\index{nilpotent!canonical form for}
\index{transformation!nilpotent!canonical representative}
\index{canonical form!for nilpotent matrices}
Every nilpotent matrix is similar to a matrix that is all zeros
except for blocks of subdiagonal ones.
That is, every nilpotent map is represented with respect to some basis by
such a matrix.
\end{corollary}

This form is unique in the sense that if a nilpotent matrix is similar to two
such matrices then those two simply have their blocks ordered differently.
Thus this is
a canonical form for the similarity classes of nilpotent matrices
provided that we order the blocks, say, from longest to shortest.

\begin{example}
The matrix
\begin{equation*}
  M=\begin{mat}[r]
      1  &-1  \\
      1  &-1
    \end{mat}
\end{equation*}
has an index of nilpotency of two, as this calculation shows.
\begin{center}
  \begin{tabular}{c|cc}
    \multicolumn{1}{c}{\textit{power} \( p \)}  &\( M^p \)  &\( \nullspace{M^p}  \)   \\  
    \hline
    \( 1 \)
    &\(  
       M=\matrixvenlarge{\begin{mat}[r]
         1  &-1  \\
         1  &-1
       \end{mat}}  \)
    &\( \set{\matrixvenlarge{\colvec{x \\ x}}\suchthat
                               x\in\C}  \)   \\
    \( 2 \)
    &\(  M^2=\matrixvenlarge{\begin{mat}[r]
         0  &0   \\
         0  &0
       \end{mat}}  \)
    &\( \C^2  \)
  \end{tabular}
\end{center}
Because the matrix is $\nbyn{2}$, any transformation that it represents 
is on a space of dimension two.
The nullspace of one application of the map 
$\nullspace{m}$ has dimension~one, and
the nullspace of two applications $\nullspace{m^2}$ has dimension~two.
Thus the action of $m$ on a string basis is
$\vec{\beta}_1\mapsto\vec{\beta}_2\mapsto\zero$ 
and the canonical form of the matrix is this.
\begin{equation*}
  N=\begin{mat}[r]
    0  &0  \\
    1  &0
  \end{mat}
\end{equation*}

We can exhibit such a string basis, 
and also the change of basis matrices witnessing the matrix similarity
between $M$ and~$N$.
Suppose that $\map{m}{\C^2}{\C^2}$ is such that \( M \) represents it 
with respect to the standard bases.
(We could take $M$ to be a representation with respect to some other basis
but the standard one is convenient.)
Pick \( \vec{\beta}_2\in\nullspace{m} \).
Also pick \( \vec{\beta}_1 \) 
so that \( m(\vec{\beta}_1)=\vec{\beta}_2 \).
\begin{equation*}
  \vec{\beta}_2=\colvec[r]{1 \\ 1}
  \qquad
  \vec{\beta}_1=\colvec[r]{1 \\ 0}
\end{equation*}
For the change of basis matrices, recall the similarity diagram.
\begin{equation*}
  \begin{CD}
    \C^2_{\wrt{\stdbasis_2}}      @>m>M>        \C^2_{\wrt{\stdbasis_2}}     \\
    @V\scriptstyle\identity V\scriptstyle PV  @V\scriptstyle\identity V\scriptstyle PV \\
    \C^2_{\wrt{B}}                 @>m>N>         \C^2_{\wrt{B}}
  \end{CD}
\end{equation*}
The canonical form is \( \rep{m}{B,B}=PMP^{-1} \), where
\begin{equation*}
   P^{-1} %=\bigl(\rep{\identity}{\stdbasis_2,B}\bigr)^{-1}
         =\rep{\identity}{B,\stdbasis_2}
         =\begin{mat}[r]
            1  &1  \\
            0  &1
          \end{mat}
   \qquad
   P=(P^{-1})^{-1}
         =\begin{mat}[r]
            1  &-1  \\
            0  &1
          \end{mat}
\end{equation*}
and the verification of the matrix calculation is routine.
\begin{equation*}
  \begin{mat}[r]
    1  &-1  \\
    0  &1
  \end{mat}
  \begin{mat}[r]
    1  &-1  \\
    1  &-1
  \end{mat}
  \begin{mat}[r]
    1  &1  \\
    0  &1
  \end{mat}=
  \begin{mat}[r]
    0  &0  \\
    1  &0
  \end{mat}
\end{equation*}
%which does indeed describe the effect of $m$ on the string basis.
\end{example}

\begin{example} \label{ex:FiveByNilStrBas}
This matrix
\begin{equation*}
  \begin{mat}[r]
     0  &0  &0  &0  &0  \\
     1  &0  &0  &0  &0  \\
     -1 &1  &1  &-1 &1  \\
     0  &1  &0  &0  &0  \\
     1  &0  &-1 &1  &-1
  \end{mat}
\end{equation*}
is nilpotent, of index~$3$.
% \newlength{\extramatrixvspace}
% \setlength{\extramatrixvspace}{.75ex}
% \newcommand{\matrixvenlarge}[1]{\raisebox{-0.5\height}{\vbox{
%        \vspace*{\extramatrixvspace}
%        \hbox{$#1$}
%         \vspace*{\extramatrixvspace}
%         }}}
\begin{center} 
  \begin{tabular}{c|cc}
    \multicolumn{1}{c}{\textit{power} \( p \)}  &\( N^p \)  &\( \nullspace{N^p}  \)   \\  
    \hline
    \( 1 \)
    &\matrixvenlarge{
         \begin{mat}[r]
         0  &0  &0  &0  &0  \\
         1  &0  &0  &0  &0  \\
         -1 &1  &1  &-1 &1  \\
         0  &1  &0  &0  &0  \\
         1  &0  &-1 &1  &-1
         \end{mat}}
    &\( 
        \set{\matrixvenlarge{\colvec{0 \\ 0 \\ u-v \\ u \\ v}} 
           \suchthat u,v\in\C}  \)                      \\
    \( 2 \)
    &\(
       \matrixvenlarge{\begin{mat}[r]
         0  &0  &0  &0  &0  \\
         0  &0  &0  &0  &0  \\
         1  &0  &0  &0  &0  \\
         1  &0  &0  &0  &0  \\
         0  &0  &0  &0  &0
       \end{mat}}  \)
    &\( 
        \set{\matrixvenlarge{\colvec{0 \\ y \\ z \\ u \\ v}}
                       \suchthat y,z,u,v\in\C}  \)  \\
    \( 3 \)
    &\textit{--zero matrix--}
    &\( \C^5 \)
  \end{tabular}
\end{center}
The table tells us this about any string basis:~the 
null space after one map application has
dimension~two so two basis vectors map directly to zero, 
the null space after the second application has dimension four 
so two additional basis vectors map to zero by the second iteration, 
and
the null space after three applications is of dimension~five so the 
remaining one basis vector maps to zero in three hops.
\begin{equation*}
  \begin{strings}{ccccccc}
    \vec{\beta}_1 &\mapsto &\vec{\beta}_2 &\mapsto &\vec{\beta}_3
       &\mapsto &\zero  \\
    \vec{\beta}_4 &\mapsto &\vec{\beta}_5 &\mapsto &\zero
  \end{strings}
\end{equation*}
To produce such a basis, first pick two vectors from
\( \nullspace{n} \) that form a linearly independent set.
\begin{equation*}
   \vec{\beta}_3=\colvec[r]{0 \\ 0 \\ 1 \\ 1 \\ 0} \quad
   \vec{\beta}_5=\colvec[r]{0 \\ 0 \\ 0 \\ 1 \\ 1}
\end{equation*}
Then add \( \vec{\beta}_2,\vec{\beta}_4\in\nullspace{n^2} \)
such that \( n(\vec{\beta}_2)=\vec{\beta}_3 \) and
\( n(\vec{\beta}_4)=\vec{\beta}_5 \).
\begin{equation*}
   \vec{\beta}_2=\colvec[r]{0 \\ 1 \\ 0 \\ 0 \\ 0} \quad
   \vec{\beta}_4=\colvec[r]{0 \\ 1 \\ 0 \\ 1 \\ 0}
\end{equation*}
Finish by adding \( \vec{\beta}_1 \)
such that \( n(\vec{\beta}_1)=\vec{\beta}_2 \).
\begin{equation*}
   \vec{\beta}_1=\colvec[r]{1 \\ 0 \\ 1 \\ 0 \\ 0}
\end{equation*}
\end{example}






\begin{exercises}
   \recommended \item \label{exer:IndNilLftShift}
     What is the index of nilpotency of the \definend{right-shift} operator,
     here acting on the space of triples of reals?
      \begin{equation*}
         (x,y,z)\mapsto(0,x,y)
      \end{equation*}
      \begin{answer}
        Three.  
        It is at least three because $\ell^2(\,(1,1,1)\,)=(0,0,1)\neq \zero$.
        It is at most three because 
        $(x,y,z)\mapsto (0,x,y)\mapsto (0,0,x)\mapsto (0,0,0)$.
      \end{answer}
  \recommended \item 
    For each string basis state the index of nilpotency and
    give the dimension of the range space and
    null space of each iteration of the nilpotent map.
    \begin{exparts}
      \partsitem $
        \begin{strings}{ccccccc}
           \vec{\beta}_1 &\mapsto &\vec{\beta}_2 &\mapsto &\zero  \\
           \vec{\beta}_3 &\mapsto &\vec{\beta}_4 &\mapsto &\zero  
         \end{strings}$
      \partsitem $
        \begin{strings}{ccccccc}
           \vec{\beta}_1 &\mapsto &\vec{\beta}_2 &\mapsto &\vec{\beta}_3
                &\mapsto &\zero  \\
           \vec{\beta}_4 &\mapsto &\zero \\
           \vec{\beta}_5 &\mapsto &\zero \\
           \vec{\beta}_6 &\mapsto &\zero
         \end{strings}$
      \partsitem $
        \begin{strings}{ccccccccc}
           \vec{\beta}_1 &\mapsto &\vec{\beta}_2 &\mapsto &\vec{\beta}_3
                &\mapsto &\zero  
         \end{strings}$
    \end{exparts}
    Also give the canonical form of the matrix.
    \begin{answer}
      \begin{exparts}
        \partsitem The domain has dimension four.
          The map's action is that any vector in the space
          $c_1\cdot \vec{\beta}_1+c_2\cdot \vec{\beta}_2
            +c_3\cdot \vec{\beta}_3+c_4\cdot \vec{\beta}_4$
          goes to
          $c_1\cdot \vec{\beta}_2+c_2\cdot \zero
            +c_3\cdot \vec{\beta}_4+c_4\cdot \zero
           =c_1\cdot \vec{\beta}_3+c_3\cdot\vec{\beta}_4$.
          The first application of the map
          sends two basis vectors $\vec{\beta}_2$ and
          $\vec{\beta}_4$ to zero,
          and therefore the null space has dimension~two and the range space
          has dimension~two.
          With a second application, all four basis vectors go to
          zero and so the null space of the second power has dimension~four
          while the range space of the second power has dimension~zero.
          Thus the index of nilpotency is two.
          This is the canonical form.
          \begin{equation*}
            \begin{mat}[r]
              0  &0  &0  &0  \\
              1  &0  &0  &0  \\
              0  &0  &0  &0  \\
              0  &0  &1  &0
            \end{mat}
          \end{equation*}
        \partsitem The dimension of the domain of this map is six.
          For the first power the dimension of the null space is four
          and the dimension of the range space is two.
          For the second power the dimension of the null space is five
          and the dimension of the range space is one.
          Then the third iteration results in a null space of dimension
          six and a range space of dimension zero.
          The index of nilpotency is three, and this is the
          canonical form.
          \begin{equation*}
            \begin{mat}[r]
              0  &0  &0  &0  &0  &0  \\
              1  &0  &0  &0  &0  &0  \\
              0  &1  &0  &0  &0  &0  \\
              0  &0  &0  &0  &0  &0  \\              
              0  &0  &0  &0  &0  &0  \\              
              0  &0  &0  &0  &0  &0  
            \end{mat}
          \end{equation*}
        \partsitem The dimension of the domain is three, and the index of 
          nilpotency is three.
          The first power's null space has dimension one and its range space
          has dimension two.
          The second power's null space has dimension two and its range space
          has dimension one.
          Finally, the third power's null space has dimension three 
          and its range space
          has dimension zero.
          Here is the canonical form matrix.
          \begin{equation*}
            \begin{mat}[r]
              0  &0  &0  \\
              1  &0  &0  \\
              0  &1  &0 
            \end{mat}
          \end{equation*}
      \end{exparts}
    \end{answer}
  \item 
    Decide which of these matrices are nilpotent.
    \begin{exparts*}
      \partsitem 
        $\begin{mat}[r]
           -2  &4  \\
           -1  &2
        \end{mat}$
      \partsitem 
        $\begin{mat}[r]
          3  &1  \\
          1  &3
        \end{mat}$
      \partsitem 
        $\begin{mat}[r]
          -3  &2  &1  \\
          -3  &2  &1  \\
          -3  &2  &1
        \end{mat}$
      \partsitem 
        $\begin{mat}[r]
           1  &1  &4  \\
           3  &0  &-1 \\
           5  &2  &7
        \end{mat}$
      \partsitem 
        $\begin{mat}[r]
           45  &-22  &-19  \\
           33  &-16  &-14  \\
           69  &-34  &-29
        \end{mat}$
    \end{exparts*}
    \begin{answer}
      By \nearbylemma{le:RangeAndNullChains} the nullity has grown as 
      large as possible by the $n$-th iteration where $n$ is the dimension
      of the domain.
      Thus, for the $\nbyn{2}$ matrices, 
      we need only check whether the square is the zero matrix.
      For the $\nbyn{3}$ matrices, we need only check the cube.
      \begin{exparts}
        \partsitem Yes, this matrix is nilpotent because
          its square is the zero matrix.
        \partsitem No, the square is not the zero matrix.
          \begin{equation*}
            \begin{mat}[r]
               3  &1  \\
               1  &3
            \end{mat}^2
            =\begin{mat}[r]
               10  &6  \\
               6   &10
            \end{mat}
          \end{equation*}
        \partsitem Yes, the cube is the zero matrix.
          In fact, the square is zero.
        \partsitem No, the third power is not the zero matrix.
          \begin{equation*}
            \begin{mat}[r]
              1  &1  &4  \\
              3  &0  &-1 \\
              5  &2  &7
            \end{mat}^3
            =\begin{mat}[r]
              206  &86  &304  \\
               26  &8   &26   \\
              438  &180 &634
            \end{mat}
          \end{equation*}
        \partsitem Yes, the cube of this matrix is the zero matrix.
      \end{exparts}
      Another way to see that the second and fourth matrices are not nilpotent
      is to note that they are nonsingular.
    \end{answer}
  \recommended \item 
    Find the canonical form of this matrix.
    \begin{equation*}
      \begin{mat}[r]
        0  &1  &1  &0  &1  \\
        0  &0  &1  &1  &1  \\
        0  &0  &0  &0  &0  \\
        0  &0  &0  &0  &0  \\
        0  &0  &0  &0  &0
      \end{mat}
    \end{equation*}
    \begin{answer} The table of calculations
      \begin{center}
          \begin{tabular}{c|cc}
             \multicolumn{1}{c}{\( p \)}  &\( N^p \)  &\( \nullspace{N^p}  \) \\
             \hline
             \( 1 \)
               &\matrixvenlarge{\begin{mat}[r]
                   0  &1  &1  &0  &1  \\
                   0  &0  &1  &1  &1  \\
                   0  &0  &0  &0  &0  \\
                   0  &0  &0  &0  &0  \\
                   0  &0  &0  &0  &0
                  \end{mat}}
               &\( \set{\matrixvenlarge{\colvec{r \\ u \\ -u-v \\ u \\ v}} 
                              \suchthat r,u,v\in\C}  \) \\
             \( 2 \)
               &\matrixvenlarge{\begin{mat}[r]
                   0  &0  &1  &1  &1  \\
                   0  &0  &0  &0  &0  \\
                   0  &0  &0  &0  &0  \\
                   0  &0  &0  &0  &0  \\
                   0  &0  &0  &0  &0
                  \end{mat}}
               &\( \set{\matrixvenlarge{\colvec{r \\ s \\ -u-v \\ u \\ v}} 
                              \suchthat r,s,u,v\in\C}  \) \\
            \( 2 \)
               &\textit{--zero matrix--}
               &\( \C^5 \)
          \end{tabular}
        \end{center}
        gives these requirements of the string basis:~three basis vectors
        map directly to zero, one more basis vector maps to zero by a
        second application, and the final basis vector 
        maps to zero by a third application.
        Thus, the string basis has this form.
        \begin{equation*}
          \begin{strings}{ccccccc}
            \vec{\beta}_1 &\mapsto &\vec{\beta}_2 &\mapsto 
              &\vec{\beta}_3 &\mapsto &\zero               \\
            \vec{\beta}_4 &\mapsto &\zero                  \\
            \vec{\beta}_5 &\mapsto &\zero  
          \end{strings}
        \end{equation*}
        From that the canonical form is immediate.
        \begin{equation*}
          \begin{mat}[r]
            0  &0  &0  &0  &0  \\
            1  &0  &0  &0  &0  \\
            0  &1  &0  &0  &0  \\
            0  &0  &0  &0  &0  \\
            0  &0  &0  &0  &0
          \end{mat}
        \end{equation*}
    \end{answer}
  \recommended \item 
    Consider the matrix from \nearbyexample{ex:FiveByNilStrBas}.
    \begin{exparts}
      \partsitem Use the action of the map on the string basis to
        give the canonical form.
      \partsitem Find the change of basis matrices that bring the matrix
        to canonical form.
      \partsitem Use the answer in the prior item to check the answer in the 
        first item.
    \end{exparts}
    \begin{answer}
      \begin{exparts}
        \partsitem The canonical form has a $\nbyn{3}$ block and a 
          $\nbyn{2}$ block
          \begin{equation*}
            \begin{pmat}{rrr|rr}
              0  &0  &0  &0  &0  \\
              1  &0  &0  &0  &0  \\
              0  &1  &0  &0  &0  \\ \hline
              0  &0  &0  &0  &0  \\
              0  &0  &0  &1  &0  \\
            \end{pmat}
          \end{equation*}
          corresponding to the length three string and the length two
          string in the basis.
        \partsitem Assume that $N$ is the representation of the underlying
          map with respect to the standard basis.
          Let $B$ be the basis to which we will change. 
          By the similarity diagram
          \begin{equation*}
            \begin{CD}
              \C^2_{\wrt{\stdbasis_2}}      
                  @>n>N>        
                  \C^2_{\wrt{\stdbasis_2}}     \\
             @V\scriptstyle\identity V\scriptstyle PV  
                 @V\scriptstyle\identity V\scriptstyle PV \\
             C^2_{\wrt{B}}                 
                 @>n>>         
             \C^2_{\wrt{B}}
           \end{CD}
         \end{equation*}
         we have that the canonical form matrix is $PNP^{-1}$ where 
         \begin{equation*}
           P^{-1}
           =\rep{\identity}{B,\stdbasis_5}
           =\begin{mat}[r]
              1 &0 &0 &0 &0 \\              
              0 &1 &0 &1 &0 \\
              1 &0 &1 &0 &0 \\
              0 &0 &1 &1 &1 \\
              0 &0 &0 &0 &1
            \end{mat}
         \end{equation*}
         and $P$ is the inverse of that.
         \begin{equation*}
           P=\rep{\identity}{\stdbasis_5,B}
            =(P^{-1})^{-1}
           =\begin{mat}[r]
              1 &0 &0 &0 &0 \\              
             -1 &1 &1 &-1&1 \\
             -1 &0 &1 &0 &0 \\
              1 &0 &-1&1 &-1\\
              0 &0 &0 &0 &1
            \end{mat}
         \end{equation*}
        \partsitem The calculation to check this is routine.
      \end{exparts}
    \end{answer}
  \recommended \item
    Each of these matrices is nilpotent.
    \begin{exparts*}
      \partsitem \(
        \begin{mat}[r]
          1/2  &-1/2  \\
          1/2  &-1/2
        \end{mat}        \)
      \partsitem \(
        \begin{mat}[r]
          0  &0  &0  \\
          0  &-1 &1  \\
          0  &-1 &1
        \end{mat}        \)
      \partsitem \(
        \begin{mat}[r]
         -1  &1  &-1 \\
          1  &0  &1  \\
          1  &-1 &1
        \end{mat}        \)
    \end{exparts*}
    Put each in canonical form.
    \begin{answer}
      \begin{exparts}
      \partsitem The calculation 
        \begin{center}
          \begin{tabular}{c|cc}
             \multicolumn{1}{c}{\( p \)}  &\( N^p \)  &\( \nullspace{N^p} \) \\
            \hline
             \( 1 \)
               &\matrixvenlarge{\begin{mat}[r]
                    1/2  &-1/2  \\
                    1/2  &-1/2 
                  \end{mat}}
               &\( \set{\matrixvenlarge{\colvec{u \\ u}} 
                              \suchthat u\in\C}  \) \\
            \( 2 \)
               &\textit{--zero matrix--}
               &\( \C^2 \)
          \end{tabular}
        \end{center}
        shows that any map represented by the matrix
        must act on the string basis in this way 
        \begin{equation*}
          \begin{strings}{ccccccc}
            \vec{\beta}_1 &\mapsto &\vec{\beta}_2  &\mapsto &\zero  
          \end{strings}
        \end{equation*}
        because the null space after one application has dimension~one
        and exactly one basis vector, $\vec{\beta}_2$, maps to zero.
        Therefore, this representation with respect to
        $\sequence{\vec{\beta}_1,\vec{\beta}_2}$ is the canonical form.
        \begin{equation*}
          \begin{mat}[r]
            0    &0   \\
            1    &0
          \end{mat}        
        \end{equation*}
      \partsitem The calculation here is similar to the prior one.
        \begin{center}
          \begin{tabular}{c|cc}
             \multicolumn{1}{c}{\( p \)}  &\( N^p \)  &\( \nullspace{N^p} \) \\
             \hline
             \( 1 \)
               &\matrixvenlarge{\begin{mat}[r]
                    0  &0  &0  \\
                    0  &-1 &1  \\
                    0  &-1 &1  
                  \end{mat}}
               &\( \set{\matrixvenlarge{\colvec{u \\ v \\ v}} 
                              \suchthat u,v\in\C}  \) \\
           \( 2 \)
               &\textit{--zero matrix--}
               &\( \C^3 \)
          \end{tabular}
       \end{center}
       The table shows that the string basis is of the form
        \begin{equation*}
          \begin{strings}{ccccccc}
            \vec{\beta}_1 &\mapsto &\vec{\beta}_2 &\mapsto &\zero  \\
            \vec{\beta}_3 &\mapsto &\zero
          \end{strings}
        \end{equation*}
        because the null space after one application of the map has 
        dimension two\Dash
        $\vec{\beta}_2$ and $\vec{\beta}_3$ are both sent to zero\Dash and
        one more iteration results in the additional vector going
        to zero.
      \partsitem The calculation 
        \begin{center}
          \begin{tabular}{c|cc}
             \multicolumn{1}{c}{\( p \)}  &\( N^p \)  &\( \nullspace{N^p} \) \\
             \hline
             \( 1 \)
               &\matrixvenlarge{\begin{mat}[r]
                    -1  &1  &-1  \\
                     1  &0  &1   \\
                     1  &-1 &1 
                  \end{mat}}
               &\( \set{\matrixvenlarge{\colvec{u \\ 0 \\ -u}} 
                              \suchthat u\in\C}  \) \\
             \( 2 \)
               &\matrixvenlarge{\begin{mat}[r]
                     1  &0  &1   \\
                     0  &0  &0   \\
                    -1  &0  &-1
                  \end{mat}}
               &\( \set{\matrixvenlarge{\colvec{u \\ v \\ -u}} 
                              \suchthat u,v\in\C}  \) \\
            \( 3 \)
               &\textit{--zero matrix--}
               &\( \C^3 \)
          \end{tabular}
        \end{center}
        shows that any map represented by this basis must act on 
        a string basis in this way. 
        \begin{equation*}
          \begin{strings}{ccccccc}
            \vec{\beta}_1 &\mapsto &\vec{\beta}_2 &\mapsto 
              &\vec{\beta}_3 &\mapsto &\zero  
          \end{strings}
        \end{equation*}
        Therefore, this is the canonical form.
        \begin{equation*}
          \begin{mat}[r]
            0    &0   &0   \\
            1    &0   &0   \\
            0    &1   &0
          \end{mat}        
        \end{equation*}
      \end{exparts}  
     \end{answer}
  \item 
    Describe the effect of left or right multiplication by a matrix that is
    in the canonical form for nilpotent matrices.
    \begin{answer}
      A couple of examples 
      \begin{equation*}
        \begin{mat}[r]
          0  &0  \\
          1  &0
        \end{mat}
        \begin{mat}
          a  &b  \\
          c  &d
        \end{mat}
        =
        \begin{mat}
          0  &0  \\
          a  &b  
        \end{mat}
        \qquad
        \begin{mat}[r]
          0  &0  &0 \\
          1  &0  &0 \\
          0  &1  &0
        \end{mat}
        \begin{mat}
          a  &b  &c \\
          d  &e  &f \\
          g  &h  &i
        \end{mat}
        =
        \begin{mat}
          0  &0  &0 \\
          a  &b  &c \\
          d  &e  &f
        \end{mat}
      \end{equation*}
      suggest that left multiplication by a block of subdiagonal ones
      shifts the rows of a matrix downward.
      Distinct blocks
      \begin{equation*}
        \begin{mat}[r]
          0  &0  &0  &0  \\
          1  &0  &0  &0  \\
          0  &0  &0  &0  \\
          0  &0  &1  &0
        \end{mat}
        \begin{mat}
          a  &b  &c  &d  \\
          e  &f  &g  &h  \\
          i  &j  &k  &l  \\
          m  &n  &o  &p
        \end{mat}
        =
        \begin{mat}
          0  &0  &0  &0  \\
          a  &b  &c  &d  \\       
          0  &0  &0  &0  \\
          i  &j  &k  &l  
        \end{mat}
      \end{equation*}
      act to shift down distinct parts of the matrix.

      Right multiplication does an analogous thing to columns.
      See \nearbyexercise{exer:IndNilLftShift}.
    \end{answer}
  \item 
    Is nilpotence invariant under similarity?
    That is, must a matrix similar to a nilpotent matrix also be nilpotent?
    If so, with the same index?
    \begin{answer}
      Yes.
      Generalize the last sentence in \nearbyexample{ex:NilMatNotCanon}.
      As to the index, that same last sentence shows that the index of the new 
      matrix is less than or equal to the index of $\hat{N}$, and reversing
      the roles of the two matrices gives inequality in the other direction.

      Another answer to this question is to show that a matrix is 
      nilpotent if and only if any associated map is nilpotent, and 
      with the same index.
      Then, because similar matrices represent the same map, the conclusion
      follows.
      This is \nearbyexercise{exer:MatNilIffMapNil} below.
    \end{answer}  
  \recommended \item
    Show that the only eigenvalue of a nilpotent matrix is zero.
    \begin{answer}
      Observe that a canonical form nilpotent matrix has only
      zero eigenvalues; e.g., the determinant of this lower-triangular matrix
      \begin{equation*}
         \begin{mat}
           -x  &0  &0  \\
            1  &-x &0  \\
            0  &1  &-x
         \end{mat}
      \end{equation*}
      is \( (-x)^3 \), the only root of which is zero.
      But similar matrices have the same eigenvalues and every nilpotent
      matrix is similar to one in canonical form.   

      Another way to see this is to observe that a nilpotent matrix sends all
      vectors to zero after some number of iterations, but that conflicts 
      with an action on an eigenspace $\vec{v}\mapsto \lambda\vec{v}$ unless
      $\lambda$ is zero.
    \end{answer}
  \item 
    Is there a nilpotent transformation of index three on a
    two-dimensional space?
    \begin{answer}
      No, by \nearbylemma{le:RangeAndNullChains} for a map on a 
      two-dimensional space, the nullity has grown
      as large as possible by the second iteration.
    \end{answer}
  \item 
    In the proof of \nearbytheorem{th:NilMapHasStrBas}, why isn't the
    proof's base case that the index of nilpotency is zero?
    \begin{answer}
      The index of nilpotency of a transformation can be zero only when the 
      vector starting the string
      must be $\zero$, that is, only when $V$ is a trivial space.
    \end{answer}
  \recommended \item
    Let \( \map{t}{V}{V} \) be a linear transformation and suppose
    \( \vec{v}\in V \)  is such that \( t^k(\vec{v})=\zero \) but
    \( t^{k-1}(\vec{v})\neq\zero \).
    Consider the $t$-string 
    $\sequence{\vec{v},t(\vec{v}),\dots,t^{k-1}(\vec{v})}$.
    \begin{exparts}
      \partsitem Prove that \( t \) is a transformation on the span of the set
        of vectors in the string,
        that is, prove that \( t \) restricted to the span has a range
        that is a subset of the span.
        We say that the span is a \definend{\( t \)-invariant} 
        subspace.\index{invariant subspace}
      \partsitem Prove that the restriction is nilpotent.
      \partsitem Prove that the $t$-string
        is linearly independent and so is a basis for its span.
      \partsitem Represent the restriction map with respect to the 
        $t$-string basis.
    \end{exparts}
    \begin{answer}
      \begin{exparts}
        \partsitem Any member $\vec{w}$ of the span is
          a linear combination
          $\vec{w}=c_0\cdot \vec{v}+c_1\cdot t(\vec{v})+\dots
           +c_{k-1}\cdot t^{k-1}(\vec{v})$.
          But then, by the linearity of the map, 
          $t(\vec{w})=c_0\cdot t(\vec{v})+c_1\cdot t^2(\vec{v})+\dots
           +c_{k-2}\cdot t^{k-1}(\vec{v})+c_{k-1}\cdot \zero$
          is also in the span.
        \partsitem The operation in the prior item, when iterated $k$ times,
          will result in a linear combination of zeros.    
        \partsitem If \( \vec{v}=\zero \) then the set is empty and so is
          linearly independent by definition.
          Otherwise write \( c_1\vec{v}+\dots+c_{k-1}t^{k-1}(\vec{v})=\zero \)
          and apply \( t^{k-1} \) to both sides.
          The right side gives \( \zero \) while the left side gives
          \( c_1t^{k-1}(\vec{v}) \); conclude that \( c_1=0 \).
          Continue in this way by applying \( t^{k-2} \) to both sides,
          etc.  
        \partsitem Of course, $t$ acts on the span by acting on
          this basis as a single, $k$-long, $t$-string.
          \begin{equation*}
            \begin{mat}
              0 &0 &0 &0 &\ldots &0 &0 \\
              1 &0 &0 &0 &\ldots &0 &0 \\
              0 &1 &0 &0 &\ldots &0 &0 \\
              0 &0 &1 &0 &       &0 &0 \\
                &  &  &\ddots          \\
              0 &0 &0 &0 &       &1 &0 \\
            \end{mat}
          \end{equation*}
      \end{exparts}
    \end{answer}
  \item \label{exer:NilMapWillHaveStrBas}
    Finish the proof of \nearbytheorem{th:NilMapHasStrBas}.
    \begin{answer}
      We must check that
      \( B\union\hat{C}\union\set{\vec{v}_1,\dots,\vec{v}_j} \) is linearly
      independent where \( B \) is a \( t \)-string basis for
      \( \rangespace{t} \), where \( \hat{C} \) is a basis for
      \( \nullspace{t} \), and where
      \( t(\vec{v}_1)=\vec{\beta}_1,\dots,t(\vec{v}_i)=\vec{\beta}_i \).
      Write
      \begin{equation*}
        \zero=c_{1,-1}\vec{v}_1+c_{1,0}\vec{\beta}_1
               +c_{1,1}t(\vec{\beta}_1)+\dots+
               c_{1,{h_1}}t^{h_1}(\vec{\vec{\beta}}_1)
               +c_{2,-1}\vec{v}_2+\dots+c_{j,h_i}t^{h_i}(\vec{\beta_i})
      \end{equation*}
      and apply \( t \).
      \begin{multline*}
        \zero=c_{1,-1}\vec{\beta}_1+c_{1,0}t(\vec{\beta}_1)
               +\dots+
               c_{1,h_1-1}t^{h_1}(\vec{\vec{\beta}}_1)+c_{1,h_1}\zero      \\
            +c_{2,-1}\vec{\beta}_2+\cdots+c_{i,h_i-1}t^{h_i}(\vec{\beta_i})
            +c_{i,h_i}\zero
      \end{multline*}
      Conclude that the coefficients \( c_{1,-1},\dots,c_{1,h_i-1},
      c_{2,-1},\dots,c_{i,h_i-1} \) are all zero as \( B\union\hat{C} \)
      is a basis.
      Substitute back into the first displayed equation to conclude that
      the remaining coefficients are zero also.  
    \end{answer}
  \item \label{exer:MatNilIffMapNil} 
    Show that the terms `nilpotent transformation' and `nilpotent matrix',
    as given in \nearbydefinition{def:nilpotent}, fit with each other:~a
    map is nilpotent if and only if it is represented by a 
    nilpotent matrix.
    (Is it that a transformation is nilpotent if an only if there is a basis
    such that the map's representation with respect to that basis is a
    nilpotent matrix, or that any representation is a nilpotent matrix?)
    \begin{answer}
      For any basis $B$,
      a transformation~$n$ is nilpotent if and only if 
      $N=\rep{n}{B,B}$ is a nilpotent matrix.
      This is because only the zero matrix represents the zero map
      and so \( n^j \) is the zero map if and only if \( N^j \) 
      is the zero matrix.
    \end{answer}
  \item 
    Let \( T \) be nilpotent of index four.
    How big can the range space of \( T^3 \) be?
    \begin{answer}
      It can be of any size greater than or equal to one.
      To have a transformation that is nilpotent of index four,
      whose cube has range space of dimension~$k$, take a vector space,
      a basis for that space, and a transformation that acts on that basis
      in this way.
      \begin{equation*}
         \begin{strings}{ccccccccc}
            \vec{\beta}_1 &\mapsto &\vec{\beta}_2 &\mapsto &\vec{\beta}_3
              &\mapsto &\vec{\beta}_4 &\mapsto &\zero  \\
            \vec{\beta}_5 &\mapsto &\vec{\beta}_6 &\mapsto &\vec{\beta}_7
              &\mapsto &\vec{\beta}_8 &\mapsto &\zero  \\
            &\vdotswithin{\vec{\beta}_{4k-3}}                       \\
            \vec{\beta}_{4k-3} &\mapsto &\vec{\beta}_{4k-2} 
              &\mapsto &\vec{\beta}_{4k-1}
              &\mapsto &\vec{\beta}_{4k} &\mapsto &\zero  \\
            &\vdotswithin{\vec{\beta}_{4k-3}}              \\
            \multicolumn{8}{l}{%
              \text{\textit{--possibly other, shorter, strings--}}}
         \end{strings}
      \end{equation*}
      So the dimension of the range space of $T^3$ can be as large as desired.
      The smallest that it can be is one\Dash there 
      must be at least one string or else the map's index of nilpotency 
      would not be four.  
    \end{answer}
  \item 
    Recall that similar matrices have the same eigenvalues.
    Show that the converse does not hold.
    \begin{answer}
      These two have only zero for eigenvalues
      \begin{equation*}
        \begin{mat}
          0  &0  \\
          0  &0
        \end{mat}
        \qquad
        \begin{mat}
          0  &0  \\
          1  &0
        \end{mat}
      \end{equation*}
      but are not similar (they have different canonical
      representatives, namely, themselves).  
    \end{answer}
  \item \nearbylemma{lem:RestONeToOne} shows that any for any linear 
    transformation \( \map{t}{V}{V} \) 
    the restriction \( \map{t}{\genrangespace{t}}{\genrangespace{t}} \)
    is one-to-one.
    Show that it is also onto, so it is an automorphism.
    Must it be the identity map?
    \begin{answer}
      It is onto by \nearbylemma{le:RangeAndNullChains}.
      It need not be the identity: consider this map $\map{t}{\Re^2}{\Re^2}$.
      \begin{equation*}
        \colvec{x \\ y}\mapsunder{t}\colvec{y \\ x}
      \end{equation*}
      For that map $\genrangespace{t}=\Re^2$, and $t$ is not the identity.
    \end{answer}
  \item 
    Prove that a nilpotent matrix is similar to one that is all zeros
    except for blocks of super-diagonal ones.
    \begin{answer}
      A simple reordering of the string basis will do.
      For instance, a map that is associated with this string basis
      \begin{equation*}
         \begin{strings}{ccccccc}
            \vec{\beta}_1 &\mapsto &\vec{\beta}_2 &\mapsto &\zero  
         \end{strings}
      \end{equation*}
      is represented with respect to 
      $B=\sequence{\vec{\beta}_1,\vec{\beta}_2}$ by this matrix
      \begin{equation*}
        \begin{mat}
          0  &0  \\
          1  &0          
        \end{mat}
      \end{equation*}
      but is represented with respect to 
      $B=\sequence{\vec{\beta}_2,\vec{\beta}_1}$ in this way.
      \begin{equation*}
        \begin{mat}
          0  &1  \\
          0  &0          
        \end{mat}
      \end{equation*}
    \end{answer}
  \recommended \item
    Prove that if a transformation has the same range space as null space.
    then the dimension of its domain is even.
    \begin{answer}
      Let $\map{t}{V}{V}$ be the transformation.
      If \( \rank (t)=\nullity (t) \) then the equation
      \( \rank(t)+\nullity(t)=\dim (V) \) shows that 
      \( \dim (V) \) is even.  
    \end{answer}
  \item 
    Prove that if two nilpotent matrices commute then their product and
    sum are also nilpotent.
    \begin{answer}
      For the matrices to be nilpotent they must be square.
      For them to commute they must be the same size.
      Thus their product and sum are defined.

      Call the matrices \( A \) and \( B \).
      To see that \( AB \) is nilpotent, multiply
      $
         (AB)^2=ABAB=AABB=A^2B^2$,
      and
         $(AB)^3=A^3B^3$, etc.,
      and, as \( A \) is nilpotent, that product is eventually zero.

      The sum is similar; use the Binomial Theorem.  
     \end{answer}
  % 2014-Dec-19 These two questions and answers are fine; I commented them to make the pages fit better.
  % \item 
  %   Consider the transformation of \( \matspace_{\nbyn{n}} \) given
  %   by \( t_S(T)=ST-TS \) where \( S \) is an \( \nbyn{n} \) matrix.
  %   Prove that if \( S \) is nilpotent then so is \( t_S \).
  %   \begin{answer}
  %     Some experimentation gives the idea for the proof.
  %     Expansion of the second power
  %     \begin{equation*}
  %       t^2_S(T)=S(ST-TS)-(ST-TS)S=S^2-2STS+TS^2
  %     \end{equation*}
  %     the third power
  %     \begin{align*}
  %       t^3_S(T)
  %         &=S(S^2-2STS+TS^2)-(S^2-2STS+TS^2)S  \\
  %         &=S^3T-3S^2TS+3STS^2-TS^3
  %     \end{align*}
  %     and the fourth power
  %     \begin{align*}
  %       t^4_S(T)
  %         &=S(S^3T-3S^2TS+3STS^2-TS^3)-(S^3T-3S^2TS+3STS^2-TS^3)S  \\
  %         &=S^4T-4S^3TS+6S^2TS^2-4STS^3+TS^4
  %     \end{align*}
  %     suggest that the expansions follow the Binomial Theorem.
  %     Verifying this by induction on the power of $t_S$ is routine.
  %     This answers the question because, where the index of nilpotency of 
  %     $S$ is $k$, in the expansion of $t^{2k}_S$  
  %     \begin{equation*}
  %       t^{2k}_S(T)=\sum_{0\leq i\leq 2k}(-1)^i\binom{2k}{i} S^iTS^{2k-i}
  %     \end{equation*}
  %     for any $i$ at least one of the $S^i$ and $S^{2k-i}$ 
  %     has a power higher than $k$, and so the term gives the zero matrix. 
  %   \end{answer}
  % \item 
  %   Show that if \( N \) is nilpotent then \( I-N \) is
  %   invertible.
  %   Is that `only if' also?
  %   \begin{answer}
  %     Use the geometric series:
  %     $
  %        I-N^{k+1}=(I-N)(N^k+N^{k-1}+\cdots+I)
  %     $.
  %     If \( N^{k+1} \) is the zero matrix then we have a right inverse for
  %     \( I-N \).
  %     It is also a left inverse.

  %     This statement is not `only if' since
  %     \begin{equation*}
  %        \begin{mat}[r]
  %           1  &0  \\
  %           0  &1
  %        \end{mat}
  %        -\begin{mat}[r]
  %           -1  &0  \\
  %           0  &-1
  %        \end{mat}
  %     \end{equation*}
  %     is invertible.  
  %    \end{answer}
%  \recommended \item 
%    Show that when a nilpotent matrix \( T \) is in canonical form 
%    then the number of
%    blocks of \( \nbyn{(k+1)} \) matrices that are all zeros
%    except for subdiagonal ones is
%    \( 2\,(\nullity (T^k))
%      -\nullity (T^{k+1})-\nullity (T^{k-1}) \).
\index{nilpotent|)}
\end{exercises}
